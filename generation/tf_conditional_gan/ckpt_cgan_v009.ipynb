{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 1000\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v015\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/conditional_gan\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/conditional_gan/cgan_checkpoint-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/numpy_arrays/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/cgan-ckpt-local-v010\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255 # Pixel values need to be scaled between 0 and 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e800a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        6400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 375,681\n",
      "Trainable params: 375,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8832)              1227648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8832)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 138)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       282752    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 2,041,217\n",
      "Trainable params: 2,041,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70546",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8cba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c67",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af608a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b649da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'C:/Users/Max/Documents/GitHub/conditional_gan/cgan-local-v008'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c715c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x17434dcea00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "196/196 [==============================] - 28s 125ms/step - g_loss: 1.1562 - d_loss: 0.5561\n",
      "Epoch 2/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1920 - d_loss: 0.5649\n",
      "Epoch 3/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.1501 - d_loss: 0.5361\n",
      "Epoch 4/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.1510 - d_loss: 0.5891\n",
      "Epoch 5/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0872 - d_loss: 0.5724\n",
      "Epoch 6/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.1715 - d_loss: 0.5567\n",
      "Epoch 7/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.1847 - d_loss: 0.5705\n",
      "Epoch 8/1000\n",
      "196/196 [==============================] - 27s 136ms/step - g_loss: 1.1692 - d_loss: 0.5740\n",
      "Epoch 9/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.1300 - d_loss: 0.5910\n",
      "Epoch 10/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2337 - d_loss: 0.5748\n",
      "Epoch 11/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1451 - d_loss: 0.5650\n",
      "Epoch 12/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1226 - d_loss: 0.6098\n",
      "Epoch 13/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.1719 - d_loss: 0.5979\n",
      "Epoch 14/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0923 - d_loss: 0.5610\n",
      "Epoch 15/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2606 - d_loss: 0.6337\n",
      "Epoch 16/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0919 - d_loss: 0.5842\n",
      "Epoch 17/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1843 - d_loss: 0.5563\n",
      "Epoch 18/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1799 - d_loss: 0.5400\n",
      "Epoch 19/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2201 - d_loss: 0.5345\n",
      "Epoch 20/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1727 - d_loss: 0.5674\n",
      "Epoch 21/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1417 - d_loss: 0.5792\n",
      "Epoch 22/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1427 - d_loss: 0.5760\n",
      "Epoch 23/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1324 - d_loss: 0.5992\n",
      "Epoch 24/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2040 - d_loss: 0.5273\n",
      "Epoch 25/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2305 - d_loss: 0.5732\n",
      "Epoch 26/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2534 - d_loss: 0.5384\n",
      "Epoch 27/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2753 - d_loss: 0.5715\n",
      "Epoch 28/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2454 - d_loss: 0.5537\n",
      "Epoch 29/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1597 - d_loss: 0.5682\n",
      "Epoch 30/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3916 - d_loss: 0.5291\n",
      "Epoch 31/1000\n",
      "196/196 [==============================] - 25s 128ms/step - g_loss: 1.2616 - d_loss: 0.5528\n",
      "Epoch 32/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2243 - d_loss: 0.5358\n",
      "Epoch 33/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1585 - d_loss: 0.5462\n",
      "Epoch 34/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1303 - d_loss: 0.5922\n",
      "Epoch 35/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1341 - d_loss: 0.5653\n",
      "Epoch 36/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3100 - d_loss: 0.6470\n",
      "Epoch 37/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1861 - d_loss: 0.5826\n",
      "Epoch 38/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1701 - d_loss: 0.5471\n",
      "Epoch 39/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1499 - d_loss: 0.5836\n",
      "Epoch 40/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1655 - d_loss: 0.5585\n",
      "Epoch 41/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2341 - d_loss: 0.5474\n",
      "Epoch 42/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2413 - d_loss: 0.6218\n",
      "Epoch 43/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2207 - d_loss: 0.5517\n",
      "Epoch 44/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1452 - d_loss: 0.5735\n",
      "Epoch 45/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0992 - d_loss: 0.5962\n",
      "Epoch 46/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0872 - d_loss: 0.5814\n",
      "Epoch 47/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1475 - d_loss: 0.5649\n",
      "Epoch 48/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.0702 - d_loss: 0.5663\n",
      "Epoch 49/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1580 - d_loss: 0.5862\n",
      "Epoch 50/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1830 - d_loss: 0.5901\n",
      "Epoch 51/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3339 - d_loss: 0.5997\n",
      "Epoch 52/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1028 - d_loss: 0.5846\n",
      "Epoch 53/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0942 - d_loss: 0.6022\n",
      "Epoch 54/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.1086 - d_loss: 0.5610\n",
      "Epoch 55/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1611 - d_loss: 0.5594\n",
      "Epoch 56/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1198 - d_loss: 0.5794\n",
      "Epoch 57/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1732 - d_loss: 0.5558\n",
      "Epoch 58/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.4363 - d_loss: 0.6099\n",
      "Epoch 59/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3487 - d_loss: 0.5402\n",
      "Epoch 60/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.4035 - d_loss: 0.5280\n",
      "Epoch 61/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2404 - d_loss: 0.5814\n",
      "Epoch 62/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.4234 - d_loss: 0.5453\n",
      "Epoch 63/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3343 - d_loss: 0.5920\n",
      "Epoch 64/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2176 - d_loss: 0.5666\n",
      "Epoch 65/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3112 - d_loss: 0.5404\n",
      "Epoch 66/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2137 - d_loss: 0.5820\n",
      "Epoch 67/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1023 - d_loss: 0.5730\n",
      "Epoch 68/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1784 - d_loss: 0.5785\n",
      "Epoch 69/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2727 - d_loss: 0.6091\n",
      "Epoch 70/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1888 - d_loss: 0.5525\n",
      "Epoch 71/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2107 - d_loss: 0.5694\n",
      "Epoch 72/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1850 - d_loss: 0.5788\n",
      "Epoch 73/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1447 - d_loss: 0.5917\n",
      "Epoch 74/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2527 - d_loss: 0.6079\n",
      "Epoch 75/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1892 - d_loss: 0.5455\n",
      "Epoch 76/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1177 - d_loss: 0.5634\n",
      "Epoch 77/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1439 - d_loss: 0.5624\n",
      "Epoch 78/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2174 - d_loss: 0.5601\n",
      "Epoch 79/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1113 - d_loss: 0.5626\n",
      "Epoch 80/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2221 - d_loss: 0.5704\n",
      "Epoch 81/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2003 - d_loss: 0.5290\n",
      "Epoch 82/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1310 - d_loss: 0.5598\n",
      "Epoch 83/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1270 - d_loss: 0.5646\n",
      "Epoch 84/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1497 - d_loss: 0.5603\n",
      "Epoch 85/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1970 - d_loss: 0.5387\n",
      "Epoch 86/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2457 - d_loss: 0.5186\n",
      "Epoch 87/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2293 - d_loss: 0.5282\n",
      "Epoch 88/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2501 - d_loss: 0.5670\n",
      "Epoch 89/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1951 - d_loss: 0.5643\n",
      "Epoch 90/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2793 - d_loss: 0.6155\n",
      "Epoch 91/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.0203 - d_loss: 0.6089\n",
      "Epoch 92/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3655 - d_loss: 0.5599\n",
      "Epoch 93/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3006 - d_loss: 0.5677\n",
      "Epoch 94/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3802 - d_loss: 0.5659\n",
      "Epoch 95/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1702 - d_loss: 0.5853\n",
      "Epoch 96/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3407 - d_loss: 0.5360\n",
      "Epoch 97/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1623 - d_loss: 0.5642\n",
      "Epoch 98/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1448 - d_loss: 0.5860\n",
      "Epoch 99/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1705 - d_loss: 0.6712\n",
      "Epoch 100/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1190 - d_loss: 0.6166\n",
      "Epoch 101/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1345 - d_loss: 0.5584\n",
      "Epoch 102/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1933 - d_loss: 0.5507\n",
      "Epoch 103/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1369 - d_loss: 0.5494\n",
      "Epoch 104/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2417 - d_loss: 0.5936\n",
      "Epoch 105/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1899 - d_loss: 0.5434\n",
      "Epoch 106/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1536 - d_loss: 0.5486\n",
      "Epoch 107/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1706 - d_loss: 0.5968\n",
      "Epoch 108/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.0995 - d_loss: 0.5575\n",
      "Epoch 109/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.4205 - d_loss: 0.5329\n",
      "Epoch 110/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1808 - d_loss: 0.5457\n",
      "Epoch 111/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.0929 - d_loss: 0.6038\n",
      "Epoch 112/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1469 - d_loss: 0.5699\n",
      "Epoch 113/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3666 - d_loss: 0.5719\n",
      "Epoch 114/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3676 - d_loss: 0.5733\n",
      "Epoch 115/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.4206 - d_loss: 0.5473\n",
      "Epoch 116/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2433 - d_loss: 0.5650\n",
      "Epoch 117/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.2133 - d_loss: 0.5637\n",
      "Epoch 118/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1805 - d_loss: 0.5835\n",
      "Epoch 119/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1305 - d_loss: 0.5858\n",
      "Epoch 120/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2990 - d_loss: 0.5450\n",
      "Epoch 121/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.1383 - d_loss: 0.5597\n",
      "Epoch 122/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2452 - d_loss: 0.5673\n",
      "Epoch 123/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.1782 - d_loss: 0.5673\n",
      "Epoch 124/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.1813 - d_loss: 0.5664\n",
      "Epoch 125/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3725 - d_loss: 0.5176\n",
      "Epoch 126/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.2531 - d_loss: 0.5342\n",
      "Epoch 127/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1963 - d_loss: 0.5449\n",
      "Epoch 128/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3294 - d_loss: 0.5629\n",
      "Epoch 129/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1850 - d_loss: 0.5505\n",
      "Epoch 130/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.2583 - d_loss: 0.5887\n",
      "Epoch 131/1000\n",
      "196/196 [==============================] - 25s 127ms/step - g_loss: 1.4336 - d_loss: 0.6690\n",
      "Epoch 132/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.1677 - d_loss: 0.5898\n",
      "Epoch 133/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1676 - d_loss: 0.5639\n",
      "Epoch 134/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1347 - d_loss: 0.5585\n",
      "Epoch 135/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1470 - d_loss: 0.5472\n",
      "Epoch 136/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.2250 - d_loss: 0.5553\n",
      "Epoch 137/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1846 - d_loss: 0.5734\n",
      "Epoch 138/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.2818 - d_loss: 0.5644\n",
      "Epoch 139/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3908 - d_loss: 0.5337\n",
      "Epoch 140/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2253 - d_loss: 0.5642\n",
      "Epoch 141/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1694 - d_loss: 0.5708\n",
      "Epoch 142/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2828 - d_loss: 0.5655\n",
      "Epoch 143/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1656 - d_loss: 0.6137\n",
      "Epoch 144/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1565 - d_loss: 0.5890\n",
      "Epoch 145/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2699 - d_loss: 0.5572\n",
      "Epoch 146/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.3119 - d_loss: 0.5625\n",
      "Epoch 147/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2492 - d_loss: 0.5772\n",
      "Epoch 148/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.6040 - d_loss: 0.6471\n",
      "Epoch 149/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.2700 - d_loss: 0.5788\n",
      "Epoch 150/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2450 - d_loss: 0.6134\n",
      "Epoch 151/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1226 - d_loss: 0.6155\n",
      "Epoch 152/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1509 - d_loss: 0.5726\n",
      "Epoch 153/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.1354 - d_loss: 0.5814\n",
      "Epoch 154/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2598 - d_loss: 0.5680\n",
      "Epoch 155/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1360 - d_loss: 0.5505\n",
      "Epoch 156/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2121 - d_loss: 0.5448\n",
      "Epoch 157/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1770 - d_loss: 0.5622\n",
      "Epoch 158/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.1855 - d_loss: 0.5376\n",
      "Epoch 159/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2263 - d_loss: 0.5629\n",
      "Epoch 160/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1357 - d_loss: 0.5868\n",
      "Epoch 161/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1713 - d_loss: 0.5883\n",
      "Epoch 162/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1935 - d_loss: 0.5427\n",
      "Epoch 163/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4395 - d_loss: 0.5964\n",
      "Epoch 164/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3164 - d_loss: 0.5018\n",
      "Epoch 165/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1934 - d_loss: 0.5723\n",
      "Epoch 166/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.1894 - d_loss: 0.5627\n",
      "Epoch 167/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2097 - d_loss: 0.5732\n",
      "Epoch 168/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1534 - d_loss: 0.5654\n",
      "Epoch 169/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.1588 - d_loss: 0.5805\n",
      "Epoch 170/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1851 - d_loss: 0.5565\n",
      "Epoch 171/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.1415 - d_loss: 0.5448\n",
      "Epoch 172/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1549 - d_loss: 0.5540\n",
      "Epoch 173/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3539 - d_loss: 0.5724\n",
      "Epoch 174/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2432 - d_loss: 0.5396\n",
      "Epoch 175/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1663 - d_loss: 0.5544\n",
      "Epoch 176/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2171 - d_loss: 0.5521\n",
      "Epoch 177/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2454 - d_loss: 0.5437\n",
      "Epoch 178/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1691 - d_loss: 0.5678\n",
      "Epoch 179/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2498 - d_loss: 0.5675\n",
      "Epoch 180/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2260 - d_loss: 0.5567\n",
      "Epoch 181/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.1832 - d_loss: 0.5608\n",
      "Epoch 182/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2599 - d_loss: 0.5641\n",
      "Epoch 183/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2236 - d_loss: 0.5872\n",
      "Epoch 184/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3556 - d_loss: 0.5795\n",
      "Epoch 185/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2823 - d_loss: 0.5499\n",
      "Epoch 186/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3375 - d_loss: 0.6072\n",
      "Epoch 187/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.4413 - d_loss: 0.5487\n",
      "Epoch 188/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3323 - d_loss: 0.5414\n",
      "Epoch 189/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.2607 - d_loss: 0.5942\n",
      "Epoch 190/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1751 - d_loss: 0.5694\n",
      "Epoch 191/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2171 - d_loss: 0.5731\n",
      "Epoch 192/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.2036 - d_loss: 0.5496\n",
      "Epoch 193/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1916 - d_loss: 0.6007\n",
      "Epoch 194/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2180 - d_loss: 0.5486\n",
      "Epoch 195/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4925 - d_loss: 0.6333\n",
      "Epoch 196/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1019 - d_loss: 0.5478\n",
      "Epoch 197/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1062 - d_loss: 0.5953\n",
      "Epoch 198/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1932 - d_loss: 0.5929\n",
      "Epoch 199/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1880 - d_loss: 0.5684\n",
      "Epoch 200/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3388 - d_loss: 0.5554\n",
      "Epoch 201/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1487 - d_loss: 0.5789\n",
      "Epoch 202/1000\n",
      "196/196 [==============================] - 25s 129ms/step - g_loss: 1.2023 - d_loss: 0.5493\n",
      "Epoch 203/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3203 - d_loss: 0.5637\n",
      "Epoch 204/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2638 - d_loss: 0.5391\n",
      "Epoch 205/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2441 - d_loss: 0.5604\n",
      "Epoch 206/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1935 - d_loss: 0.5615\n",
      "Epoch 207/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1806 - d_loss: 0.5732\n",
      "Epoch 208/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1546 - d_loss: 0.5657\n",
      "Epoch 209/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1747 - d_loss: 0.5561\n",
      "Epoch 210/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4971 - d_loss: 0.5716\n",
      "Epoch 211/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3844 - d_loss: 0.5294\n",
      "Epoch 212/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2589 - d_loss: 0.5568\n",
      "Epoch 213/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.2661 - d_loss: 0.5619\n",
      "Epoch 214/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2157 - d_loss: 0.5503\n",
      "Epoch 215/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 1.5657 - d_loss: 0.5644\n",
      "Epoch 216/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.2709 - d_loss: 0.5335\n",
      "Epoch 217/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3202 - d_loss: 0.5282\n",
      "Epoch 218/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2093 - d_loss: 0.5249\n",
      "Epoch 219/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2937 - d_loss: 0.5356\n",
      "Epoch 220/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.2980 - d_loss: 0.5499\n",
      "Epoch 221/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2730 - d_loss: 0.5891\n",
      "Epoch 222/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2516 - d_loss: 0.5750\n",
      "Epoch 223/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2589 - d_loss: 0.5367\n",
      "Epoch 224/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2065 - d_loss: 0.5308\n",
      "Epoch 225/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.2979 - d_loss: 0.5475\n",
      "Epoch 226/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2866 - d_loss: 0.5766\n",
      "Epoch 227/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.3768 - d_loss: 0.6203\n",
      "Epoch 228/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.2060 - d_loss: 0.5647\n",
      "Epoch 229/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2071 - d_loss: 0.5588\n",
      "Epoch 230/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2209 - d_loss: 0.5402\n",
      "Epoch 231/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3400 - d_loss: 0.5513\n",
      "Epoch 232/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.2502 - d_loss: 0.5533\n",
      "Epoch 233/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2682 - d_loss: 0.6022\n",
      "Epoch 234/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2936 - d_loss: 0.5534\n",
      "Epoch 235/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3071 - d_loss: 0.5200\n",
      "Epoch 236/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3085 - d_loss: 0.5366\n",
      "Epoch 237/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2686 - d_loss: 0.5354\n",
      "Epoch 238/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3175 - d_loss: 0.5621\n",
      "Epoch 239/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3542 - d_loss: 0.5350\n",
      "Epoch 240/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4494 - d_loss: 0.5166\n",
      "Epoch 241/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2723 - d_loss: 0.6096\n",
      "Epoch 242/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2698 - d_loss: 0.5536\n",
      "Epoch 243/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3205 - d_loss: 0.5480\n",
      "Epoch 244/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3347 - d_loss: 0.5541\n",
      "Epoch 245/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2069 - d_loss: 0.5454\n",
      "Epoch 246/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1845 - d_loss: 0.5567\n",
      "Epoch 247/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2332 - d_loss: 0.5526\n",
      "Epoch 248/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2582 - d_loss: 0.5796\n",
      "Epoch 249/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2361 - d_loss: 0.5133\n",
      "Epoch 250/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2586 - d_loss: 0.5688\n",
      "Epoch 251/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.2282 - d_loss: 0.5501\n",
      "Epoch 252/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3292 - d_loss: 0.5312\n",
      "Epoch 253/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2986 - d_loss: 0.5290\n",
      "Epoch 254/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3055 - d_loss: 0.5383\n",
      "Epoch 255/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3028 - d_loss: 0.5284\n",
      "Epoch 256/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3991 - d_loss: 0.6388\n",
      "Epoch 257/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.1804 - d_loss: 0.5712\n",
      "Epoch 258/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2302 - d_loss: 0.5482\n",
      "Epoch 259/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1856 - d_loss: 0.5645\n",
      "Epoch 260/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1856 - d_loss: 0.5494\n",
      "Epoch 261/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1816 - d_loss: 0.5836\n",
      "Epoch 262/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2576 - d_loss: 0.5714\n",
      "Epoch 263/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2846 - d_loss: 0.5891\n",
      "Epoch 264/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2948 - d_loss: 0.5288\n",
      "Epoch 265/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2639 - d_loss: 0.5405\n",
      "Epoch 266/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2961 - d_loss: 0.5793\n",
      "Epoch 267/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2141 - d_loss: 0.5669\n",
      "Epoch 268/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3060 - d_loss: 0.5362\n",
      "Epoch 269/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2040 - d_loss: 0.5543\n",
      "Epoch 270/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2730 - d_loss: 0.5294\n",
      "Epoch 271/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2200 - d_loss: 0.5450\n",
      "Epoch 272/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.3253 - d_loss: 0.5186\n",
      "Epoch 273/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.2435 - d_loss: 0.5319\n",
      "Epoch 274/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 1.2493 - d_loss: 0.5252\n",
      "Epoch 275/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2573 - d_loss: 0.5640\n",
      "Epoch 276/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2546 - d_loss: 0.5742\n",
      "Epoch 277/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2921 - d_loss: 0.5212\n",
      "Epoch 278/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4448 - d_loss: 0.5908\n",
      "Epoch 279/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2765 - d_loss: 0.5926\n",
      "Epoch 280/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2711 - d_loss: 0.5396\n",
      "Epoch 281/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2333 - d_loss: 0.5519\n",
      "Epoch 282/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2313 - d_loss: 0.5546\n",
      "Epoch 283/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2443 - d_loss: 0.5629\n",
      "Epoch 284/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2499 - d_loss: 0.5436\n",
      "Epoch 285/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2781 - d_loss: 0.5546\n",
      "Epoch 286/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3896 - d_loss: 0.5694\n",
      "Epoch 287/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2927 - d_loss: 0.5861\n",
      "Epoch 288/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.2740 - d_loss: 0.5285\n",
      "Epoch 289/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3365 - d_loss: 0.5722\n",
      "Epoch 290/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3252 - d_loss: 0.5497\n",
      "Epoch 291/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2963 - d_loss: 0.5372\n",
      "Epoch 292/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2836 - d_loss: 0.5508\n",
      "Epoch 293/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2739 - d_loss: 0.5545\n",
      "Epoch 294/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3046 - d_loss: 0.5564\n",
      "Epoch 295/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2662 - d_loss: 0.5716\n",
      "Epoch 296/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2911 - d_loss: 0.5513\n",
      "Epoch 297/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2720 - d_loss: 0.5715\n",
      "Epoch 298/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.5568 - d_loss: 0.5664\n",
      "Epoch 299/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.6315 - d_loss: 0.4951\n",
      "Epoch 300/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4914 - d_loss: 0.4611\n",
      "Epoch 301/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.4679 - d_loss: 0.4746\n",
      "Epoch 302/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.6593 - d_loss: 0.4916\n",
      "Epoch 303/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.1322 - d_loss: 0.4582\n",
      "Epoch 304/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.7159 - d_loss: 0.4667\n",
      "Epoch 305/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.4297 - d_loss: 0.5119\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.5136 - d_loss: 0.4720\n",
      "Epoch 307/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.4480 - d_loss: 0.4503\n",
      "Epoch 308/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 2.4633 - d_loss: 0.4433\n",
      "Epoch 309/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.3170 - d_loss: 0.4811\n",
      "Epoch 310/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.3467 - d_loss: 0.5508\n",
      "Epoch 311/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.1038 - d_loss: 0.4881\n",
      "Epoch 312/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 2.1695 - d_loss: 0.5363\n",
      "Epoch 313/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.1397 - d_loss: 0.5081\n",
      "Epoch 314/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 2.5019 - d_loss: 0.4983\n",
      "Epoch 315/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.3182 - d_loss: 0.4751\n",
      "Epoch 316/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.0617 - d_loss: 0.4751\n",
      "Epoch 317/1000\n",
      "196/196 [==============================] - 25s 125ms/step - g_loss: 2.1382 - d_loss: 0.4600\n",
      "Epoch 318/1000\n",
      "196/196 [==============================] - 26s 133ms/step - g_loss: 2.4301 - d_loss: 0.4790\n",
      "Epoch 319/1000\n",
      "196/196 [==============================] - 26s 133ms/step - g_loss: 2.2929 - d_loss: 0.4648\n",
      "Epoch 320/1000\n",
      "196/196 [==============================] - 24s 125ms/step - g_loss: 2.2817 - d_loss: 0.4524\n",
      "Epoch 321/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 2.3132 - d_loss: 0.4509\n",
      "Epoch 322/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.3487 - d_loss: 0.4511\n",
      "Epoch 323/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.8678 - d_loss: 0.4710\n",
      "Epoch 324/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.7309 - d_loss: 0.4877\n",
      "Epoch 325/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.6560 - d_loss: 0.5110\n",
      "Epoch 326/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.4373 - d_loss: 0.4712\n",
      "Epoch 327/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.3863 - d_loss: 0.5359\n",
      "Epoch 328/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.6435 - d_loss: 0.4705\n",
      "Epoch 329/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.5177 - d_loss: 0.4573\n",
      "Epoch 330/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 2.5781 - d_loss: 0.4609\n",
      "Epoch 331/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.5075 - d_loss: 0.5037\n",
      "Epoch 332/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 2.3936 - d_loss: 0.4558\n",
      "Epoch 333/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.5028 - d_loss: 0.4396\n",
      "Epoch 334/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.5537 - d_loss: 0.4401\n",
      "Epoch 335/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.2772 - d_loss: 0.4759\n",
      "Epoch 336/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.3386 - d_loss: 0.4504\n",
      "Epoch 337/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.3966 - d_loss: 0.4506\n",
      "Epoch 338/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 3.0344 - d_loss: 0.4310\n",
      "Epoch 339/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.9270 - d_loss: 0.4783\n",
      "Epoch 340/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4735 - d_loss: 0.4687\n",
      "Epoch 341/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 3.7738 - d_loss: 0.4419\n",
      "Epoch 342/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.6547 - d_loss: 0.4551\n",
      "Epoch 343/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.3366 - d_loss: 0.4582\n",
      "Epoch 344/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.4845 - d_loss: 0.4772\n",
      "Epoch 345/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 3.1442 - d_loss: 0.4352\n",
      "Epoch 346/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4550 - d_loss: 0.4411\n",
      "Epoch 347/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4213 - d_loss: 0.4541\n",
      "Epoch 348/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.9953 - d_loss: 0.4498\n",
      "Epoch 349/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.4295 - d_loss: 0.4688\n",
      "Epoch 350/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.2988 - d_loss: 0.4878\n",
      "Epoch 351/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.3982 - d_loss: 0.4616\n",
      "Epoch 352/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4895 - d_loss: 0.4528\n",
      "Epoch 353/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.5153 - d_loss: 0.5082\n",
      "Epoch 354/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.7114 - d_loss: 0.4561\n",
      "Epoch 355/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.3374 - d_loss: 0.4566\n",
      "Epoch 356/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.4230 - d_loss: 0.4590\n",
      "Epoch 357/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.7259 - d_loss: 0.4296\n",
      "Epoch 358/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.4857 - d_loss: 0.4789\n",
      "Epoch 359/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 4.1457 - d_loss: 0.4487\n",
      "Epoch 360/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 2.3589 - d_loss: 0.4523\n",
      "Epoch 361/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 3.1435 - d_loss: 0.4544\n",
      "Epoch 362/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.5010 - d_loss: 0.4450\n",
      "Epoch 363/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.4736 - d_loss: 0.4546\n",
      "Epoch 364/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.5869 - d_loss: 0.4519\n",
      "Epoch 365/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.4170 - d_loss: 0.4754\n",
      "Epoch 366/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.5094 - d_loss: 0.5276\n",
      "Epoch 367/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 2.4825 - d_loss: 0.4587\n",
      "Epoch 368/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.9108 - d_loss: 0.4574\n",
      "Epoch 369/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.4737 - d_loss: 0.4510\n",
      "Epoch 370/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.7744 - d_loss: 0.4404\n",
      "Epoch 371/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.6514 - d_loss: 0.4465\n",
      "Epoch 372/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 4.3186 - d_loss: 0.4493\n",
      "Epoch 373/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 4.0714 - d_loss: 0.4664\n",
      "Epoch 374/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.7515 - d_loss: 0.4564\n",
      "Epoch 375/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 3.0591 - d_loss: 0.4955\n",
      "Epoch 376/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 5.5556 - d_loss: 0.4674\n",
      "Epoch 377/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.7926 - d_loss: 0.4506\n",
      "Epoch 378/1000\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 2.5126 - d_loss: 0.4353\n",
      "Epoch 379/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 4.2055 - d_loss: 0.4421\n",
      "Epoch 380/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.6828 - d_loss: 0.4461\n",
      "Epoch 381/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 3.4636 - d_loss: 0.4356\n",
      "Epoch 382/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 23s 117ms/step - g_loss: 3.0761 - d_loss: 0.4805\n",
      "Epoch 383/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.0686 - d_loss: 0.4581\n",
      "Epoch 384/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 2.8820 - d_loss: 0.4976\n",
      "Epoch 385/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 3.5827 - d_loss: 0.4594\n",
      "Epoch 386/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 2.8160 - d_loss: 0.4505\n",
      "Epoch 387/1000\n",
      "196/196 [==============================] - 24s 124ms/step - g_loss: 3.4999 - d_loss: 0.4462\n",
      "Epoch 388/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.0194 - d_loss: 0.4567\n",
      "Epoch 389/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 3.1896 - d_loss: 0.5066\n",
      "Epoch 390/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 2.8272 - d_loss: 0.4571\n",
      "Epoch 391/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5866 - d_loss: 0.4755\n",
      "Epoch 392/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 2.9237 - d_loss: 0.4886\n",
      "Epoch 393/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.7995 - d_loss: 0.4887\n",
      "Epoch 394/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.0335 - d_loss: 0.4497\n",
      "Epoch 395/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.8138 - d_loss: 0.4472\n",
      "Epoch 396/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.2532 - d_loss: 0.4627\n",
      "Epoch 397/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5531 - d_loss: 0.4776\n",
      "Epoch 398/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5589 - d_loss: 0.4749\n",
      "Epoch 399/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 2.5474 - d_loss: 0.4752\n",
      "Epoch 400/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5180 - d_loss: 0.4563\n",
      "Epoch 401/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.6380 - d_loss: 0.4284\n",
      "Epoch 402/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.7317 - d_loss: 0.5280\n",
      "Epoch 403/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.6074 - d_loss: 0.5073\n",
      "Epoch 404/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 2.5115 - d_loss: 0.4563\n",
      "Epoch 405/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 2.7718 - d_loss: 0.5220\n",
      "Epoch 406/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.1931 - d_loss: 0.5111\n",
      "Epoch 407/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.6885 - d_loss: 0.5112\n",
      "Epoch 408/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5173 - d_loss: 0.4434\n",
      "Epoch 409/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 4.4342 - d_loss: 0.4838\n",
      "Epoch 410/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.9136 - d_loss: 0.4827\n",
      "Epoch 411/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.7139 - d_loss: 0.5081\n",
      "Epoch 412/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.0741 - d_loss: 0.4543\n",
      "Epoch 413/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.4145 - d_loss: 0.4605\n",
      "Epoch 414/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.5656 - d_loss: 0.4820\n",
      "Epoch 415/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 3.0211 - d_loss: 0.5057\n",
      "Epoch 416/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.3795 - d_loss: 0.4720\n",
      "Epoch 417/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.4051 - d_loss: 0.4725\n",
      "Epoch 418/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.4746 - d_loss: 0.4778\n",
      "Epoch 419/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.4361 - d_loss: 0.4581\n",
      "Epoch 420/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.6053 - d_loss: 0.4752\n",
      "Epoch 421/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 2.6859 - d_loss: 0.4719\n",
      "Epoch 422/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 2.3193 - d_loss: 0.4791\n",
      "Epoch 423/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 2.3918 - d_loss: 0.4627\n",
      "Epoch 424/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 2.2465 - d_loss: 0.4901\n",
      "Epoch 425/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 2.6406 - d_loss: 0.4455\n",
      "Epoch 426/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 3.8521 - d_loss: 0.4320\n",
      "Epoch 427/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 2.6063 - d_loss: 0.4895\n",
      "Epoch 428/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 2.6036 - d_loss: 0.5007\n",
      "Epoch 429/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.2929 - d_loss: 0.4529\n",
      "Epoch 430/1000\n",
      "196/196 [==============================] - 24s 124ms/step - g_loss: 3.3117 - d_loss: 0.4599\n",
      "Epoch 431/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 3.9966 - d_loss: 0.4200\n",
      "Epoch 432/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 4.0652 - d_loss: 0.4547\n",
      "Epoch 433/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 2.7938 - d_loss: 0.4484\n",
      "Epoch 434/1000\n",
      "196/196 [==============================] - 24s 124ms/step - g_loss: 4.0120 - d_loss: 0.4691\n",
      "Epoch 435/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 5.4363 - d_loss: 0.4405\n",
      "Epoch 436/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 2.9533 - d_loss: 0.5502\n",
      "Epoch 437/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.4506 - d_loss: 0.4598\n",
      "Epoch 438/1000\n",
      "196/196 [==============================] - 24s 124ms/step - g_loss: 3.6569 - d_loss: 0.4395\n",
      "Epoch 439/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 3.4364 - d_loss: 0.4987\n",
      "Epoch 440/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.4501 - d_loss: 0.4607\n",
      "Epoch 441/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 4.5470 - d_loss: 0.4700\n",
      "Epoch 442/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 6.2515 - d_loss: 0.4558\n",
      "Epoch 443/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 3.1958 - d_loss: 0.4486\n",
      "Epoch 444/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 4.8106 - d_loss: 0.4568\n",
      "Epoch 445/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.4482 - d_loss: 0.4604\n",
      "Epoch 446/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.1873 - d_loss: 0.5164\n",
      "Epoch 447/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.9394 - d_loss: 0.5187\n",
      "Epoch 448/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 5.1741 - d_loss: 0.5098\n",
      "Epoch 449/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 5.2449 - d_loss: 0.5473\n",
      "Epoch 450/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 6.7598 - d_loss: 0.4469\n",
      "Epoch 451/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 3.9581 - d_loss: 0.5006\n",
      "Epoch 452/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.5538 - d_loss: 0.4832\n",
      "Epoch 453/1000\n",
      "196/196 [==============================] - 25s 128ms/step - g_loss: 5.5081 - d_loss: 0.4936\n",
      "Epoch 454/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 5.5071 - d_loss: 0.5102\n",
      "Epoch 455/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.4878 - d_loss: 0.5691\n",
      "Epoch 456/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 7.0314 - d_loss: 0.5366\n",
      "Epoch 457/1000\n",
      "196/196 [==============================] - 25s 127ms/step - g_loss: 6.1800 - d_loss: 0.4646\n",
      "Epoch 458/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.6214 - d_loss: 0.5023\n",
      "Epoch 459/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 3.1048 - d_loss: 0.5313\n",
      "Epoch 460/1000\n",
      "196/196 [==============================] - 24s 124ms/step - g_loss: 3.9087 - d_loss: 0.4540\n",
      "Epoch 461/1000\n",
      "196/196 [==============================] - 24s 125ms/step - g_loss: 4.5001 - d_loss: 0.4693\n",
      "Epoch 462/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 3.3160 - d_loss: 0.5057\n",
      "Epoch 463/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 2.8947 - d_loss: 0.4797\n",
      "Epoch 464/1000\n",
      "196/196 [==============================] - 26s 134ms/step - g_loss: 2.8385 - d_loss: 0.4911\n",
      "Epoch 465/1000\n",
      "196/196 [==============================] - 24s 125ms/step - g_loss: 5.8260 - d_loss: 0.3993\n",
      "Epoch 466/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 5.1484 - d_loss: 0.5503\n",
      "Epoch 467/1000\n",
      "196/196 [==============================] - 26s 131ms/step - g_loss: 3.6699 - d_loss: 0.5031\n",
      "Epoch 468/1000\n",
      "196/196 [==============================] - 24s 125ms/step - g_loss: 6.5426 - d_loss: 0.5085\n",
      "Epoch 469/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 6.2206 - d_loss: 0.5346\n",
      "Epoch 470/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 4.8047 - d_loss: 0.5149\n",
      "Epoch 471/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 4.2056 - d_loss: 0.4930\n",
      "Epoch 472/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 4.8437 - d_loss: 0.5007\n",
      "Epoch 473/1000\n",
      "196/196 [==============================] - 26s 134ms/step - g_loss: 4.6328 - d_loss: 0.5451\n",
      "Epoch 474/1000\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 3.8473 - d_loss: 0.5354\n",
      "Epoch 475/1000\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.2309 - d_loss: 0.4892\n",
      "Epoch 476/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.7029 - d_loss: 0.5242\n",
      "Epoch 477/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 4.4803 - d_loss: 0.4995\n",
      "Epoch 478/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.2604 - d_loss: 0.4804\n",
      "Epoch 479/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 5.0517 - d_loss: 0.4654\n",
      "Epoch 480/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.7812 - d_loss: 0.4523\n",
      "Epoch 481/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 4.0219 - d_loss: 0.5103\n",
      "Epoch 482/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.4250 - d_loss: 0.4437\n",
      "Epoch 483/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.1470 - d_loss: 0.5093\n",
      "Epoch 484/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.1209 - d_loss: 0.4302\n",
      "Epoch 485/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 5.0080 - d_loss: 0.5003\n",
      "Epoch 486/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 4.4195 - d_loss: 0.4607\n",
      "Epoch 487/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 5.5658 - d_loss: 0.4552\n",
      "Epoch 488/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 4.1321 - d_loss: 0.4325\n",
      "Epoch 489/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 3.1938 - d_loss: 0.4129\n",
      "Epoch 490/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 4.1883 - d_loss: 0.4391\n",
      "Epoch 491/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 3.1165 - d_loss: 0.4656\n",
      "Epoch 492/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.6800 - d_loss: 0.4543\n",
      "Epoch 493/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 4.9886 - d_loss: 0.4576\n",
      "Epoch 494/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 3.5387 - d_loss: 0.4680\n",
      "Epoch 495/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.4509 - d_loss: 0.4809\n",
      "Epoch 496/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 4.9312 - d_loss: 0.4638\n",
      "Epoch 497/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 3.2440 - d_loss: 0.4008\n",
      "Epoch 498/1000\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.0944 - d_loss: 0.4769\n",
      "Epoch 499/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 3.8279 - d_loss: 0.4278\n",
      "Epoch 500/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.9049 - d_loss: 0.4217\n",
      "Epoch 501/1000\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 4.6061 - d_loss: 0.4188\n",
      "Epoch 502/1000\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 4.8039 - d_loss: 0.4416\n",
      "Epoch 503/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 4.0994 - d_loss: 0.4410\n",
      "Epoch 504/1000\n",
      "196/196 [==============================] - 24s 125ms/step - g_loss: 4.6143 - d_loss: 0.4895\n",
      "Epoch 505/1000\n",
      "196/196 [==============================] - 23s 120ms/step - g_loss: 2.9137 - d_loss: 0.4330\n",
      "Epoch 506/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 6.4577 - d_loss: 0.4455\n",
      "Epoch 507/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 4.6721 - d_loss: 0.4416\n",
      "Epoch 508/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 3.8261 - d_loss: 0.5120\n",
      "Epoch 509/1000\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 2.7453 - d_loss: 0.5070\n",
      "Epoch 510/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 2.7895 - d_loss: 0.4818\n",
      "Epoch 511/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 3.0231 - d_loss: 0.4746\n",
      "Epoch 512/1000\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 3.5127 - d_loss: 0.4829\n",
      "Epoch 513/1000\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 3.1116 - d_loss: 0.5080s - g_loss: 3.1106 - d_loss: 0.50\n",
      "Epoch 514/1000\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 3.3806 - d_loss: 0.5335\n",
      "Epoch 515/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 3.0879 - d_loss: 0.4427\n",
      "Epoch 516/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 5.6225 - d_loss: 0.4819\n",
      "Epoch 517/1000\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 3.5895 - d_loss: 0.4808\n",
      "Epoch 518/1000\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.5657 - d_loss: 0.5038\n",
      "Epoch 519/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.6322 - d_loss: 0.5121\n",
      "Epoch 520/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.9145 - d_loss: 0.4701\n",
      "Epoch 521/1000\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 3.9544 - d_loss: 0.4778\n",
      "Epoch 522/1000\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 5.1345 - d_loss: 0.4794\n",
      "Epoch 523/1000\n",
      "161/196 [=======================>......] - ETA: 5s - g_loss: 3.1422 - d_loss: 0.4805"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24232/1853602499.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m cond_gan.fit(dataset, epochs=epoch_t, \n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \"\"\"\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=checkpoint.discriminator, generator=checkpoint.generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=checkpoint.discriminator_optimizer,\n",
    "    g_optimizer=checkpoint.generator_optimizer,\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Create new training images using the Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 15000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "780b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0de5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dfad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for class: BetterSurf\n",
      "Generating images for class: Eksor.A\n",
      "Generating images for class: Obfuscator.AFQ\n",
      "Generating images for class: Occamy.C\n",
      "Generating images for class: OnLineGames.CTB\n",
      "Generating images for class: Reveton.A\n",
      "Generating images for class: Sfone\n",
      "Generating images for class: VB.IL\n",
      "Generating images for class: Zbot\n",
      "Generating images for class: Zbot!CI\n"
     ]
    }
   ],
   "source": [
    "# Create images for every class and store in seperate folder\n",
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    print(\"Generating images for class: {name}\".format(name=class_name))\n",
    "    for j in range(num_interpolation):\n",
    "        np_array = np.squeeze(converted_images[j], axis=2)\n",
    "        im = Image.fromarray((np_array))\n",
    "        im.save(f\"{class_dir}/gen_imgs_{class_name}_{j}.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e830ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
