{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DCGAN to generate face images\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2019/04/29<br>\n",
    "**Last modified:** 2021/01/01<br>\n",
    "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import gdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare CelebA data\n",
    "\n",
    "We'll use face images from the CelebA dataset, resized to 64x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/thesis_data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create a dataset from our folder, and rescale the images to the [0-1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26548 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    path_root, label_mode=None, image_size=(64, 64), batch_size=32, interpolation=\"bicubic\", color_mode=\"rgb\"\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's display a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the discriminator\n",
    "\n",
    "It maps a 64x64 image to a binary classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "      \n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\",input_shape=(64, 64, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the generator\n",
    "\n",
    "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "        \n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Override `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        \n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other GAN Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, d_loss):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    d_loss.append(total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, g_loss):\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_loss.append(fake_loss)\n",
    "    return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'C:/Users/Max/Documents/gan_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create a callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start = time.time()\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"C:/Users/Max/Documents/generated_images/generated_img_%03d_%d.png\" % (epoch, i))\n",
    "    \n",
    "        # Save the model every 5 epochs (WAS 15)\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      " 29/830 [>.............................] - ETA: 2:53 - d_loss: 0.6252 - g_loss: 0.9126"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23204/2997942662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m gan.fit(\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \"\"\"\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 90  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Some of the last generated images around epoch 30\n",
    "(results keep improving after that):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan_overriding_train_step",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
