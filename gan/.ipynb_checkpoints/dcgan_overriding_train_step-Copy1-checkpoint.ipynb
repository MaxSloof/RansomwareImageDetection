{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DCGAN to generate face images\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2019/04/29<br>\n",
    "**Last modified:** 2021/01/01<br>\n",
    "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare CelebA data\n",
    "\n",
    "We'll use face images from the CelebA dataset, resized to 64x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/thesis_data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create a dataset from our folder, and rescale the images to the [0-1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26548 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    path_root, label_mode=None, image_size=(64, 64), batch_size=32, interpolation=\"bicubic\"\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's display a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the discriminator\n",
    "\n",
    "It maps a 64x64 image to a binary classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the generator\n",
    "\n",
    "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Override `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create a callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"C:/Users/Max/Documents/generated_images/generated_img_%03d_%d.png\" % (epoch, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/830 [=======>......................] - ETA: 2:11 - d_loss: -196433648.0000 - g_loss: 17001462784.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16172/3052264387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m gan.fit(\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Some of the last generated images around epoch 30\n",
    "(results keep improving after that):\n",
    "\n",
    "![results](https://i.imgur.com/h5MtQZ7l.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan_overriding_train_step",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
