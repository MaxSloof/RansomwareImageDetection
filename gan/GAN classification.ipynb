{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/thesis_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26548 images belonging to 11 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26548, 64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = ImageDataGenerator().flow_from_directory(directory=path_root, color_mode = \"rgb\", \n",
    "                                                   target_size=(64,64), class_mode= \"sparse\", interpolation=\"bicubic\", \n",
    "                                                   batch_size=40000)\n",
    "imgs, labels = next(batches)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9,\n",
       " 'benign': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs, labels, test_size=testsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 18583 | Shape of training data (18583, 64, 64, 3)\n",
      "Size of training data: 7965  | Shape of training data (7965, 64, 64, 3)\n",
      "Shape of training labels (18583,)\n",
      "Shape of training labels (7965,)\n"
     ]
    }
   ],
   "source": [
    "trainsize = len(X_train)\n",
    "testsize = len(X_test)\n",
    "\n",
    "print(f\"Size of training data: {trainsize} | Shape of training data {X_train.shape}\")\n",
    "print(f\"Size of training data: {testsize}  | Shape of training data {X_test.shape}\")\n",
    "print(f\"Shape of training labels {y_train.shape}\")\n",
    "print(f\"Shape of training labels {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\", input_shape=(64, 64, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'C:/Users/Max/Documents/gan_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e208280fa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we transfer the weights of discriminator except the last layer to a new model, add a dense layer with 128 units, and add another dense layer with 10 units and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 1,446,731\n",
      "Trainable params: 1,446,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = checkpoint.discriminator\n",
    "    \n",
    "new_model = Sequential()\n",
    "for i in range(len(disc.layers) - 1):\n",
    "    new_model.add(disc.layers[i])\n",
    "\n",
    "for layer in new_model.layers:\n",
    "    layers.trainable = False\n",
    "\n",
    "new_model.add(Dense(128, activation = \"relu\"))\n",
    "new_model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_6/kernel:0' shape=(8192, 128) dtype=float32, numpy=\n",
       " array([[ 0.01305944,  0.01706236,  0.01150295, ..., -0.01543331,\n",
       "         -0.00646758, -0.01715975],\n",
       "        [-0.00397384, -0.0240164 , -0.00508745, ...,  0.0075558 ,\n",
       "          0.0237201 , -0.0148007 ],\n",
       "        [ 0.01351043,  0.00886332, -0.01109991, ..., -0.02456664,\n",
       "         -0.0115023 , -0.00054914],\n",
       "        ...,\n",
       "        [ 0.00644967,  0.00618113, -0.02109783, ...,  0.02318005,\n",
       "         -0.01724467,  0.01905178],\n",
       "        [-0.0176617 , -0.01398314, -0.00472712, ..., -0.01428101,\n",
       "          0.0241043 , -0.00593137],\n",
       "        [-0.01580074,  0.00928331,  0.02645567, ...,  0.00711003,\n",
       "          0.01465366,  0.01382229]], dtype=float32)>,\n",
       " <tf.Variable 'dense_6/bias:0' shape=(128,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the initialized weights and bias of the second last dense layer; weights are uniformly randomly generated \n",
    "# and biases are all zeroes by default\n",
    "new_model.layers[-2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18583, 64, 64, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_images = X_train.reshape(( 64, 64, 1))\n",
    "# train_images = (train_images.astype('float32') - 127.5) / 127.5\n",
    "\n",
    "# test_images = X_test.reshape(( 64, 64, 1))\n",
    "# test_images = (test_images.astype('float32') - 127.5) / 127.5\n",
    "\n",
    "train_images = X_train.reshape((trainsize, 64,64,3))\n",
    "train_images = train_images.astype('float32') / 255 # Was *255\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "             \n",
    "test_images = X_test.reshape((testsize, 64,64,3))\n",
    "test_images = test_images.astype('float32') / 255 # Was *255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18583, 64, 64, 3) (18583, 11) (7965, 64, 64, 3) (7965, 11)\n"
     ]
    }
   ],
   "source": [
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)\n",
    "\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "186/186 [==============================] - 4s 19ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 2/7\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 3/7\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 4/7\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 5/7\n",
      "186/186 [==============================] - 3s 18ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 6/7\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 7/7\n",
      "186/186 [==============================] - 3s 18ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n"
     ]
    }
   ],
   "source": [
    "new_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history1 = new_model.fit(train_images, train_labels, batch_size=100, epochs=7,\n",
    "                        validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "186/186 [==============================] - 4s 18ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 2/8\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 3/8\n",
      "186/186 [==============================] - 3s 18ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 4/8\n",
      "186/186 [==============================] - 3s 18ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 5/8\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 6/8\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 7/8\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n",
      "Epoch 8/8\n",
      "186/186 [==============================] - 3s 17ms/step - loss: nan - accuracy: 0.0919 - val_loss: nan - val_accuracy: 0.0913\n"
     ]
    }
   ],
   "source": [
    "# unfreeze all layers \n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "#optimizer=new_model.optimizer\n",
    "#optimizer.learning_rate=0.005\n",
    "    \n",
    "new_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history2 = new_model.fit(train_images, train_labels, batch_size=100, epochs=8,\n",
    "                        validation_data=(test_images, test_labels)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history1.history['accuracy'] + history2.history['accuracy']\n",
    "val_accuracy = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(accuracy, label='accuracy')\n",
    "plt.plot(val_accuracy, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.3, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, using the tuned discriminator model, both training accuracy and testing are approximately 0.94 after 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration: predict the ith test digit\n",
    "i = np.random.randint(0, trainsize)\n",
    "\n",
    "# show the actual ith digit\n",
    "print('actual label:', np.argmax(test_labels[i]))\n",
    "plt.figure()\n",
    "plt.imshow(test_images[i,:,:,0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# predict\n",
    "prediction = new_model.predict(test_images[i].reshape(1,28,28,1))\n",
    "\n",
    "# get probability distribution and classification of the test digit\n",
    "print(prediction)\n",
    "print('prediction:', np.argmax(prediction))\n",
    "\n",
    "# draw the barplot\n",
    "plt.figure()\n",
    "plt.bar(np.arange(0,10).astype('str'), prediction[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = new_model.predict(test_images).argmax(axis=1)\n",
    "true_labels = test_labels.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = tf.math.confusion_matrix(labels=true_labels, predictions=pred_labels).numpy()\n",
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_mat_norm = np.around(confusion_mat.astype('float') / np.atleast_2d(confusion_mat.sum(axis=1)).T, decimals=2)\n",
    "\n",
    "classes = np.arange(0,10).astype('str')\n",
    "confusion_mat_df = pd.DataFrame(confusion_mat_norm,\n",
    "                                index = classes, \n",
    "                                columns = classes)\n",
    "\n",
    "figure = plt.figure()\n",
    "sns.heatmap(confusion_mat_df, annot=True, cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate new digits and perform classification on them using our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noi = tf.random.normal([1, 100])\n",
    "sample = checkpoint.generator(noi, training=False)\n",
    "fig = plt.figure()\n",
    "plt.imshow(sample[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = sample[0, :, :, 0].numpy().reshape(1,28,28,1)\n",
    "new_prediction = new_model.predict(new_sample)\n",
    "\n",
    "# get probability distribution and classification of the test digit\n",
    "print(new_prediction)\n",
    "print('prediction:', np.argmax(new_prediction))\n",
    "\n",
    "# draw the barplot\n",
    "plt.figure()\n",
    "plt.bar(np.arange(0,10).astype('str'), new_prediction[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save the numpy array of appropriately generated digits as .npy file,\n",
    "# which could be used for further training\n",
    "np.save('generated_numpy.npy', sample[0, :, :, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the saved numpy array\n",
    "arr = np.load('generated_numpy.npy')\n",
    "arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
