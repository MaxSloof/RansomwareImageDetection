{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DCGAN to generate face images\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2019/04/29<br>\n",
    "**Last modified:** 2021/01/01<br>\n",
    "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import gdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare CelebA data\n",
    "\n",
    "We'll use face images from the CelebA dataset, resized to 64x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/thesis_data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create a dataset from our folder, and rescale the images to the [0-1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22004/851783687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m dataset = keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpath_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bicubic\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rgb\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    path_root, label_mode=None, image_size=(64, 64), batch_size=32, interpolation=\"bicubic\", color_mode=\"rgb\"\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's display a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the discriminator\n",
    "\n",
    "It maps a 64x64 image to a binary classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "      \n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\",input_shape=(64, 64, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the generator\n",
    "\n",
    "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "        \n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Override `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        \n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other GAN Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, d_loss):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    d_loss.append(total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, g_loss):\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_loss.append(fake_loss)\n",
    "    return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'C:/Users/Max/Documents/gan_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create a callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start = time.time()\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"C:/Users/Max/Documents/generated_images/generated_img_%03d_%d.png\" % (epoch, i))\n",
    "    \n",
    "        # Save the model every 5 epochs (WAS 15)\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "830/830 [==============================] - 187s 224ms/step - d_loss: 0.2896 - g_loss: 3.8413\n",
      "Time for epoch 1 is 0.042000770568847656 sec\n",
      "Epoch 2/90\n",
      "830/830 [==============================] - 188s 227ms/step - d_loss: 0.3453 - g_loss: 4.3027\n",
      "Time for epoch 2 is 0.04399895668029785 sec\n",
      "Epoch 3/90\n",
      "830/830 [==============================] - 189s 228ms/step - d_loss: 0.2627 - g_loss: 3.5942\n",
      "Time for epoch 3 is 0.042999982833862305 sec\n",
      "Epoch 4/90\n",
      "830/830 [==============================] - 188s 226ms/step - d_loss: 0.2883 - g_loss: 3.2250\n",
      "Time for epoch 4 is 0.04199981689453125 sec\n",
      "Epoch 5/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.3279 - g_loss: 3.0838\n",
      "Time for epoch 5 is 0.0413362979888916 sec\n",
      "Epoch 6/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.4741 - g_loss: 2.3643\n",
      "Time for epoch 6 is 0.04300045967102051 sec\n",
      "Epoch 7/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.5204 - g_loss: 2.0785\n",
      "Time for epoch 7 is 0.04500126838684082 sec\n",
      "Epoch 8/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.4745 - g_loss: 2.1022\n",
      "Time for epoch 8 is 0.0410008430480957 sec\n",
      "Epoch 9/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5328 - g_loss: 1.8020\n",
      "Time for epoch 9 is 0.03899812698364258 sec\n",
      "Epoch 10/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5111 - g_loss: 1.6915\n",
      "Time for epoch 10 is 0.039662837982177734 sec\n",
      "Epoch 11/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5156 - g_loss: 1.6264\n",
      "Time for epoch 11 is 0.04000139236450195 sec\n",
      "Epoch 12/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5211 - g_loss: 1.4890\n",
      "Time for epoch 12 is 0.039999961853027344 sec\n",
      "Epoch 13/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5650 - g_loss: 1.5223\n",
      "Time for epoch 13 is 0.03867483139038086 sec\n",
      "Epoch 14/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5739 - g_loss: 1.5906\n",
      "Time for epoch 14 is 0.03899955749511719 sec\n",
      "Epoch 15/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5702 - g_loss: 1.4367\n",
      "Time for epoch 15 is 0.09499979019165039 sec\n",
      "Epoch 16/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5296 - g_loss: 1.3694\n",
      "Time for epoch 16 is 0.04099702835083008 sec\n",
      "Epoch 17/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5345 - g_loss: 1.4732\n",
      "Time for epoch 17 is 0.03899836540222168 sec\n",
      "Epoch 18/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5326 - g_loss: 1.3609\n",
      "Time for epoch 18 is 0.038025856018066406 sec\n",
      "Epoch 19/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5348 - g_loss: 1.5663\n",
      "Time for epoch 19 is 0.03866291046142578 sec\n",
      "Epoch 20/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5234 - g_loss: 1.4514\n",
      "Time for epoch 20 is 0.039000511169433594 sec\n",
      "Epoch 21/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5349 - g_loss: 1.5280\n",
      "Time for epoch 21 is 0.04000115394592285 sec\n",
      "Epoch 22/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.5069 - g_loss: 1.3958\n",
      "Time for epoch 22 is 0.04000091552734375 sec\n",
      "Epoch 23/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4935 - g_loss: 1.6114\n",
      "Time for epoch 23 is 0.04199838638305664 sec\n",
      "Epoch 24/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4844 - g_loss: 1.4867\n",
      "Time for epoch 24 is 0.04000115394592285 sec\n",
      "Epoch 25/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4759 - g_loss: 1.4566\n",
      "Time for epoch 25 is 0.04000258445739746 sec\n",
      "Epoch 26/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4661 - g_loss: 1.6822\n",
      "Time for epoch 26 is 0.04000139236450195 sec\n",
      "Epoch 27/90\n",
      "830/830 [==============================] - 186s 225ms/step - d_loss: 0.4373 - g_loss: 1.6369\n",
      "Time for epoch 27 is 0.04000115394592285 sec\n",
      "Epoch 28/90\n",
      "830/830 [==============================] - 188s 226ms/step - d_loss: 0.4455 - g_loss: 1.7075\n",
      "Time for epoch 28 is 0.0390012264251709 sec\n",
      "Epoch 29/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4318 - g_loss: 1.6450\n",
      "Time for epoch 29 is 0.0390019416809082 sec\n",
      "Epoch 30/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4391 - g_loss: 1.7025\n",
      "Time for epoch 30 is 0.0989990234375 sec\n",
      "Epoch 31/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4364 - g_loss: 1.6610\n",
      "Time for epoch 31 is 0.03900003433227539 sec\n",
      "Epoch 32/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4284 - g_loss: 1.6539\n",
      "Time for epoch 32 is 0.04000115394592285 sec\n",
      "Epoch 33/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4285 - g_loss: 1.6833\n",
      "Time for epoch 33 is 0.04000091552734375 sec\n",
      "Epoch 34/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4174 - g_loss: 1.6950\n",
      "Time for epoch 34 is 0.04200124740600586 sec\n",
      "Epoch 35/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.4244 - g_loss: 1.7006\n",
      "Time for epoch 35 is 0.04003620147705078 sec\n",
      "Epoch 36/90\n",
      "830/830 [==============================] - 186s 225ms/step - d_loss: 0.4130 - g_loss: 1.7020\n",
      "Time for epoch 36 is 0.038001060485839844 sec\n",
      "Epoch 37/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4126 - g_loss: 1.7040\n",
      "Time for epoch 37 is 0.0390012264251709 sec\n",
      "Epoch 38/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4341 - g_loss: 1.8116\n",
      "Time for epoch 38 is 0.03999948501586914 sec\n",
      "Epoch 39/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4054 - g_loss: 1.7450\n",
      "Time for epoch 39 is 0.041001319885253906 sec\n",
      "Epoch 40/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4186 - g_loss: 1.6522\n",
      "Time for epoch 40 is 0.0390009880065918 sec\n",
      "Epoch 41/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4159 - g_loss: 1.6439\n",
      "Time for epoch 41 is 0.03915667533874512 sec\n",
      "Epoch 42/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4053 - g_loss: 1.7151\n",
      "Time for epoch 42 is 0.0390017032623291 sec\n",
      "Epoch 43/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4032 - g_loss: 1.7831\n",
      "Time for epoch 43 is 0.040001630783081055 sec\n",
      "Epoch 44/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.4046 - g_loss: 1.7303\n",
      "Time for epoch 44 is 0.04000067710876465 sec\n",
      "Epoch 45/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.4140 - g_loss: 1.7092\n",
      "Time for epoch 45 is 0.09800028800964355 sec\n",
      "Epoch 46/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3959 - g_loss: 1.8133\n",
      "Time for epoch 46 is 0.04099392890930176 sec\n",
      "Epoch 47/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3980 - g_loss: 1.7167\n",
      "Time for epoch 47 is 0.040000200271606445 sec\n",
      "Epoch 48/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3981 - g_loss: 1.7076\n",
      "Time for epoch 48 is 0.039002180099487305 sec\n",
      "Epoch 49/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3902 - g_loss: 1.8871\n",
      "Time for epoch 49 is 0.03811812400817871 sec\n",
      "Epoch 50/90\n",
      "830/830 [==============================] - 189s 228ms/step - d_loss: 0.3831 - g_loss: 1.8254\n",
      "Time for epoch 50 is 0.04300045967102051 sec\n",
      "Epoch 51/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.3858 - g_loss: 1.7380\n",
      "Time for epoch 51 is 0.040547847747802734 sec\n",
      "Epoch 52/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.3817 - g_loss: 1.7874\n",
      "Time for epoch 52 is 0.04000115394592285 sec\n",
      "Epoch 53/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3800 - g_loss: 1.8289\n",
      "Time for epoch 53 is 0.04000115394592285 sec\n",
      "Epoch 54/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3771 - g_loss: 1.8095\n",
      "Time for epoch 54 is 0.0410008430480957 sec\n",
      "Epoch 55/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3734 - g_loss: 1.7996\n",
      "Time for epoch 55 is 0.04000258445739746 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3586 - g_loss: 1.8627\n",
      "Time for epoch 56 is 0.04000067710876465 sec\n",
      "Epoch 57/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3506 - g_loss: 1.9324\n",
      "Time for epoch 57 is 0.03800344467163086 sec\n",
      "Epoch 58/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3523 - g_loss: 1.8924\n",
      "Time for epoch 58 is 0.0390012264251709 sec\n",
      "Epoch 59/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3600 - g_loss: 1.8912\n",
      "Time for epoch 59 is 0.039832115173339844 sec\n",
      "Epoch 60/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3508 - g_loss: 1.9534\n",
      "Time for epoch 60 is 0.10000085830688477 sec\n",
      "Epoch 61/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3494 - g_loss: 1.8535\n",
      "Time for epoch 61 is 0.03899955749511719 sec\n",
      "Epoch 62/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3549 - g_loss: 1.9721\n",
      "Time for epoch 62 is 0.0390012264251709 sec\n",
      "Epoch 63/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3418 - g_loss: 1.9433\n",
      "Time for epoch 63 is 0.03800058364868164 sec\n",
      "Epoch 64/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3344 - g_loss: 2.0001\n",
      "Time for epoch 64 is 0.03900027275085449 sec\n",
      "Epoch 65/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3376 - g_loss: 1.9402\n",
      "Time for epoch 65 is 0.03977036476135254 sec\n",
      "Epoch 66/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3296 - g_loss: 2.0011\n",
      "Time for epoch 66 is 0.03999972343444824 sec\n",
      "Epoch 67/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3377 - g_loss: 2.0161\n",
      "Time for epoch 67 is 0.03900003433227539 sec\n",
      "Epoch 68/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3248 - g_loss: 2.0366\n",
      "Time for epoch 68 is 0.039000511169433594 sec\n",
      "Epoch 69/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3291 - g_loss: 2.0053\n",
      "Time for epoch 69 is 0.0410008430480957 sec\n",
      "Epoch 70/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3300 - g_loss: 2.0049\n",
      "Time for epoch 70 is 0.03899693489074707 sec\n",
      "Epoch 71/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3281 - g_loss: 2.0680\n",
      "Time for epoch 71 is 0.03899860382080078 sec\n",
      "Epoch 72/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3213 - g_loss: 2.0154\n",
      "Time for epoch 72 is 0.04000043869018555 sec\n",
      "Epoch 73/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3246 - g_loss: 2.0147\n",
      "Time for epoch 73 is 0.04000067710876465 sec\n",
      "Epoch 74/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3241 - g_loss: 2.0674\n",
      "Time for epoch 74 is 0.042000770568847656 sec\n",
      "Epoch 75/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3160 - g_loss: 2.1012\n",
      "Time for epoch 75 is 0.09299993515014648 sec\n",
      "Epoch 76/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3248 - g_loss: 2.0581\n",
      "Time for epoch 76 is 0.04000139236450195 sec\n",
      "Epoch 77/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3180 - g_loss: 2.0272\n",
      "Time for epoch 77 is 0.03899860382080078 sec\n",
      "Epoch 78/90\n",
      "830/830 [==============================] - 187s 225ms/step - d_loss: 0.3185 - g_loss: 2.0327\n",
      "Time for epoch 78 is 0.04000115394592285 sec\n",
      "Epoch 79/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3129 - g_loss: 2.0896\n",
      "Time for epoch 79 is 0.03800058364868164 sec\n",
      "Epoch 80/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3137 - g_loss: 2.0820\n",
      "Time for epoch 80 is 0.03999948501586914 sec\n",
      "Epoch 81/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3097 - g_loss: 2.1618\n",
      "Time for epoch 81 is 0.03900027275085449 sec\n",
      "Epoch 82/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3073 - g_loss: 2.0693\n",
      "Time for epoch 82 is 0.0390012264251709 sec\n",
      "Epoch 83/90\n",
      "830/830 [==============================] - 186s 225ms/step - d_loss: 0.3039 - g_loss: 2.1211\n",
      "Time for epoch 83 is 0.04000258445739746 sec\n",
      "Epoch 84/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3070 - g_loss: 2.0786\n",
      "Time for epoch 84 is 0.039000511169433594 sec\n",
      "Epoch 85/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3060 - g_loss: 2.0975\n",
      "Time for epoch 85 is 0.03999948501586914 sec\n",
      "Epoch 86/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3017 - g_loss: 2.1518\n",
      "Time for epoch 86 is 0.04200005531311035 sec\n",
      "Epoch 87/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3113 - g_loss: 2.1238\n",
      "Time for epoch 87 is 0.039999961853027344 sec\n",
      "Epoch 88/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.3046 - g_loss: 2.1225\n",
      "Time for epoch 88 is 0.0410008430480957 sec\n",
      "Epoch 89/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.2974 - g_loss: 2.1277\n",
      "Time for epoch 89 is 0.03999805450439453 sec\n",
      "Epoch 90/90\n",
      "830/830 [==============================] - 186s 224ms/step - d_loss: 0.2981 - g_loss: 2.1667\n",
      "Time for epoch 90 is 0.09400224685668945 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae5c82e910>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 90  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Some of the last generated images around epoch 30\n",
    "(results keep improving after that):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1ae7f2e5130>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan_overriding_train_step",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
