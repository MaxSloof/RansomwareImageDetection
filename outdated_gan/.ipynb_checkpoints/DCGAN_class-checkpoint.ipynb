{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# DCGAN to generate face images\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2019/04/29<br>\n",
    "**Last modified:** 2021/01/01<br>\n",
    "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step` on CelebA images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import gdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare CelebA data\n",
    "\n",
    "We'll use face images from the CelebA dataset, resized to 64x64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/image_data/data_original/Reveton.A/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Create a dataset from our folder, and rescale the images to the [0-1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1445 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    path_root, label_mode=None, image_size=(64, 64), batch_size=32, interpolation=\"bicubic\", color_mode=\"rgb\"\n",
    ")\n",
    "dataset = dataset.map(lambda x: x / 255.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's display a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for x in dataset:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the discriminator\n",
    "\n",
    "It maps a 64x64 image to a binary classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        3136      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "      \n",
    "        \n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\",input_shape=(64, 64, 3)),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create the generator\n",
    "\n",
    "It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              1056768   \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 3)         38403     \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "        \n",
    "        layers.Dense(8 * 8 * 128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Override `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        \n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other GAN Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output, d_loss):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    d_loss.append(total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, g_loss):\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_loss.append(fake_loss)\n",
    "    return fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'C:/Users/Max/Documents/gan_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Create a callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start = time.time()\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"C:/Users/Max/Documents/generated_images/generated_img_%03d_%d.png\" % (epoch, i))\n",
    "    \n",
    "        # Save the model every 5 epochs (WAS 15)\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "46/46 [==============================] - 14s 231ms/step - d_loss: 0.5804 - g_loss: 0.9160\n",
      "Time for epoch 1 is 0.2838144302368164 sec\n",
      "Epoch 2/150\n",
      "46/46 [==============================] - 10s 218ms/step - d_loss: 0.5644 - g_loss: 1.6305\n",
      "Time for epoch 2 is 0.04466509819030762 sec\n",
      "Epoch 3/150\n",
      "46/46 [==============================] - 10s 219ms/step - d_loss: 0.2332 - g_loss: 8.7775\n",
      "Time for epoch 3 is 0.04300069808959961 sec\n",
      "Epoch 4/150\n",
      "46/46 [==============================] - 10s 220ms/step - d_loss: 1.5194 - g_loss: 1.8719\n",
      "Time for epoch 4 is 0.04500436782836914 sec\n",
      "Epoch 5/150\n",
      "46/46 [==============================] - 10s 221ms/step - d_loss: 0.1327 - g_loss: 5.9280\n",
      "Time for epoch 5 is 0.046141862869262695 sec\n",
      "Epoch 6/150\n",
      "46/46 [==============================] - 11s 229ms/step - d_loss: 0.4825 - g_loss: 5.2461\n",
      "Time for epoch 6 is 0.05500078201293945 sec\n",
      "Epoch 7/150\n",
      "46/46 [==============================] - 11s 232ms/step - d_loss: 0.4813 - g_loss: 1.2352\n",
      "Time for epoch 7 is 0.05610394477844238 sec\n",
      "Epoch 8/150\n",
      "46/46 [==============================] - 12s 253ms/step - d_loss: 0.3957 - g_loss: 1.4760\n",
      "Time for epoch 8 is 0.051172733306884766 sec\n",
      "Epoch 9/150\n",
      "46/46 [==============================] - 11s 230ms/step - d_loss: 0.5034 - g_loss: 1.4681\n",
      "Time for epoch 9 is 0.062001705169677734 sec\n",
      "Epoch 10/150\n",
      "46/46 [==============================] - 11s 239ms/step - d_loss: 0.7040 - g_loss: 1.5529\n",
      "Time for epoch 10 is 0.054003000259399414 sec\n",
      "Epoch 11/150\n",
      "46/46 [==============================] - 11s 244ms/step - d_loss: 0.3378 - g_loss: 2.0711\n",
      "Time for epoch 11 is 0.04937934875488281 sec\n",
      "Epoch 12/150\n",
      "46/46 [==============================] - 11s 238ms/step - d_loss: 0.3818 - g_loss: 1.9528\n",
      "Time for epoch 12 is 0.20499825477600098 sec\n",
      "Epoch 13/150\n",
      "46/46 [==============================] - 13s 252ms/step - d_loss: 0.4144 - g_loss: 1.7246\n",
      "Time for epoch 13 is 0.062188148498535156 sec\n",
      "Epoch 14/150\n",
      "46/46 [==============================] - 11s 244ms/step - d_loss: 0.2888 - g_loss: 1.9039\n",
      "Time for epoch 14 is 0.06200361251831055 sec\n",
      "Epoch 15/150\n",
      "46/46 [==============================] - 13s 293ms/step - d_loss: 0.3777 - g_loss: 1.4043\n",
      "Time for epoch 15 is 0.1399233341217041 sec\n",
      "Epoch 16/150\n",
      "46/46 [==============================] - 12s 253ms/step - d_loss: 0.3466 - g_loss: 1.6128\n",
      "Time for epoch 16 is 0.06744146347045898 sec\n",
      "Epoch 17/150\n",
      "46/46 [==============================] - 11s 247ms/step - d_loss: 0.3754 - g_loss: 1.8166\n",
      "Time for epoch 17 is 0.06407332420349121 sec\n",
      "Epoch 18/150\n",
      "46/46 [==============================] - 12s 255ms/step - d_loss: 0.4979 - g_loss: 2.0133\n",
      "Time for epoch 18 is 0.06200122833251953 sec\n",
      "Epoch 19/150\n",
      "46/46 [==============================] - 11s 243ms/step - d_loss: 0.4887 - g_loss: 1.8866\n",
      "Time for epoch 19 is 0.06485819816589355 sec\n",
      "Epoch 20/150\n",
      "46/46 [==============================] - 11s 237ms/step - d_loss: 0.5106 - g_loss: 2.9264\n",
      "Time for epoch 20 is 0.0572206974029541 sec\n",
      "Epoch 21/150\n",
      "46/46 [==============================] - 12s 261ms/step - d_loss: 0.6231 - g_loss: 2.0833\n",
      "Time for epoch 21 is 0.06300044059753418 sec\n",
      "Epoch 22/150\n",
      "46/46 [==============================] - 13s 277ms/step - d_loss: 0.5080 - g_loss: 1.8615\n",
      "Time for epoch 22 is 0.05399966239929199 sec\n",
      "Epoch 23/150\n",
      "46/46 [==============================] - 13s 280ms/step - d_loss: 0.2944 - g_loss: 2.7374\n",
      "Time for epoch 23 is 0.0520017147064209 sec\n",
      "Epoch 24/150\n",
      "46/46 [==============================] - 11s 247ms/step - d_loss: 0.4878 - g_loss: 1.4394\n",
      "Time for epoch 24 is 0.0859994888305664 sec\n",
      "Epoch 25/150\n",
      "46/46 [==============================] - 13s 286ms/step - d_loss: 0.6467 - g_loss: 1.4225\n",
      "Time for epoch 25 is 0.054998159408569336 sec\n",
      "Epoch 26/150\n",
      "46/46 [==============================] - 12s 250ms/step - d_loss: 0.6378 - g_loss: 1.5267\n",
      "Time for epoch 26 is 0.054998159408569336 sec\n",
      "Epoch 27/150\n",
      "46/46 [==============================] - 12s 265ms/step - d_loss: 0.5906 - g_loss: 1.2752\n",
      "Time for epoch 27 is 0.061995744705200195 sec\n",
      "Epoch 28/150\n",
      "46/46 [==============================] - 12s 261ms/step - d_loss: 0.5035 - g_loss: 1.6001\n",
      "Time for epoch 28 is 0.06299781799316406 sec\n",
      "Epoch 29/150\n",
      "46/46 [==============================] - 11s 244ms/step - d_loss: 0.5987 - g_loss: 1.5525\n",
      "Time for epoch 29 is 0.322650671005249 sec\n",
      "Epoch 30/150\n",
      "46/46 [==============================] - 13s 284ms/step - d_loss: 0.4889 - g_loss: 1.5716\n",
      "Time for epoch 30 is 0.11700248718261719 sec\n",
      "Epoch 31/150\n",
      "46/46 [==============================] - 14s 309ms/step - d_loss: 0.4228 - g_loss: 1.9876\n",
      "Time for epoch 31 is 0.06199812889099121 sec\n",
      "Epoch 32/150\n",
      "46/46 [==============================] - 13s 290ms/step - d_loss: 0.4235 - g_loss: 1.6399\n",
      "Time for epoch 32 is 0.06299877166748047 sec\n",
      "Epoch 33/150\n",
      "46/46 [==============================] - 12s 254ms/step - d_loss: 0.5073 - g_loss: 1.4317\n",
      "Time for epoch 33 is 0.06400084495544434 sec\n",
      "Epoch 34/150\n",
      "46/46 [==============================] - 14s 307ms/step - d_loss: 0.6644 - g_loss: 1.3865\n",
      "Time for epoch 34 is 0.07066822052001953 sec\n",
      "Epoch 35/150\n",
      "46/46 [==============================] - 14s 303ms/step - d_loss: 0.2741 - g_loss: 3.2277\n",
      "Time for epoch 35 is 0.06300234794616699 sec\n",
      "Epoch 36/150\n",
      "46/46 [==============================] - 13s 272ms/step - d_loss: 0.5582 - g_loss: 1.3651\n",
      "Time for epoch 36 is 0.05299878120422363 sec\n",
      "Epoch 37/150\n",
      "46/46 [==============================] - 14s 294ms/step - d_loss: 0.4373 - g_loss: 1.5012\n",
      "Time for epoch 37 is 0.04999947547912598 sec\n",
      "Epoch 38/150\n",
      "46/46 [==============================] - 13s 286ms/step - d_loss: 0.5809 - g_loss: 1.6554\n",
      "Time for epoch 38 is 0.06400036811828613 sec\n",
      "Epoch 39/150\n",
      "46/46 [==============================] - 12s 269ms/step - d_loss: 0.8563 - g_loss: 1.2306\n",
      "Time for epoch 39 is 0.05199909210205078 sec\n",
      "Epoch 40/150\n",
      "46/46 [==============================] - 14s 301ms/step - d_loss: 0.4894 - g_loss: 1.8944\n",
      "Time for epoch 40 is 0.06299805641174316 sec\n",
      "Epoch 41/150\n",
      "46/46 [==============================] - 14s 297ms/step - d_loss: 0.5230 - g_loss: 1.6163\n",
      "Time for epoch 41 is 0.059000253677368164 sec\n",
      "Epoch 42/150\n",
      "46/46 [==============================] - 15s 328ms/step - d_loss: 0.6165 - g_loss: 1.3309\n",
      "Time for epoch 42 is 0.06899881362915039 sec\n",
      "Epoch 43/150\n",
      "46/46 [==============================] - 11s 236ms/step - d_loss: 0.4360 - g_loss: 1.6867\n",
      "Time for epoch 43 is 0.05091977119445801 sec\n",
      "Epoch 44/150\n",
      "46/46 [==============================] - 11s 238ms/step - d_loss: 0.4177 - g_loss: 1.7960\n",
      "Time for epoch 44 is 0.057997941970825195 sec\n",
      "Epoch 45/150\n",
      "46/46 [==============================] - 15s 316ms/step - d_loss: 0.5709 - g_loss: 1.6558\n",
      "Time for epoch 45 is 0.1080009937286377 sec\n",
      "Epoch 46/150\n",
      "46/46 [==============================] - 14s 307ms/step - d_loss: 0.3342 - g_loss: 2.1609\n",
      "Time for epoch 46 is 0.06507158279418945 sec\n",
      "Epoch 47/150\n",
      "46/46 [==============================] - 14s 306ms/step - d_loss: 0.3879 - g_loss: 1.7249\n",
      "Time for epoch 47 is 0.06606698036193848 sec\n",
      "Epoch 48/150\n",
      "46/46 [==============================] - 14s 311ms/step - d_loss: 0.4662 - g_loss: 1.5209\n",
      "Time for epoch 48 is 0.039000511169433594 sec\n",
      "Epoch 49/150\n",
      "46/46 [==============================] - 13s 284ms/step - d_loss: 0.3126 - g_loss: 2.1567\n",
      "Time for epoch 49 is 0.041997432708740234 sec\n",
      "Epoch 50/150\n",
      "46/46 [==============================] - 11s 231ms/step - d_loss: 0.3006 - g_loss: 2.1576\n",
      "Time for epoch 50 is 0.11954307556152344 sec\n",
      "Epoch 51/150\n",
      "46/46 [==============================] - 12s 256ms/step - d_loss: 0.2887 - g_loss: 2.2208\n",
      "Time for epoch 51 is 0.04151105880737305 sec\n",
      "Epoch 52/150\n",
      "46/46 [==============================] - 11s 241ms/step - d_loss: 0.3987 - g_loss: 1.9288\n",
      "Time for epoch 52 is 0.03900146484375 sec\n",
      "Epoch 53/150\n",
      "46/46 [==============================] - 11s 229ms/step - d_loss: 0.3074 - g_loss: 2.1926\n",
      "Time for epoch 53 is 0.040999412536621094 sec\n",
      "Epoch 54/150\n",
      "46/46 [==============================] - 12s 266ms/step - d_loss: 0.3599 - g_loss: 2.0473\n",
      "Time for epoch 54 is 0.04007267951965332 sec\n",
      "Epoch 55/150\n",
      "46/46 [==============================] - 11s 231ms/step - d_loss: 0.2871 - g_loss: 2.4945\n",
      "Time for epoch 55 is 0.03900289535522461 sec\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 12s 254ms/step - d_loss: 0.3984 - g_loss: 1.8901\n",
      "Time for epoch 56 is 0.039769649505615234 sec\n",
      "Epoch 57/150\n",
      "46/46 [==============================] - 11s 246ms/step - d_loss: 0.3433 - g_loss: 2.0649\n",
      "Time for epoch 57 is 0.040003299713134766 sec\n",
      "Epoch 58/150\n",
      "46/46 [==============================] - 13s 276ms/step - d_loss: 0.3480 - g_loss: 2.1774\n",
      "Time for epoch 58 is 0.04971432685852051 sec\n",
      "Epoch 59/150\n",
      "46/46 [==============================] - 13s 270ms/step - d_loss: 0.3438 - g_loss: 2.1122\n",
      "Time for epoch 59 is 0.04000234603881836 sec\n",
      "Epoch 60/150\n",
      "46/46 [==============================] - 11s 238ms/step - d_loss: 0.3217 - g_loss: 2.0595\n",
      "Time for epoch 60 is 0.09599757194519043 sec\n",
      "Epoch 61/150\n",
      "46/46 [==============================] - 11s 235ms/step - d_loss: 0.3195 - g_loss: 1.9981\n",
      "Time for epoch 61 is 0.039003610610961914 sec\n",
      "Epoch 62/150\n",
      "46/46 [==============================] - 11s 231ms/step - d_loss: 0.3283 - g_loss: 2.1447\n",
      "Time for epoch 62 is 0.039809226989746094 sec\n",
      "Epoch 63/150\n",
      "46/46 [==============================] - 11s 236ms/step - d_loss: 0.2959 - g_loss: 2.2772\n",
      "Time for epoch 63 is 0.03999924659729004 sec\n",
      "Epoch 64/150\n",
      "46/46 [==============================] - 12s 258ms/step - d_loss: 0.8137 - g_loss: 1.5821\n",
      "Time for epoch 64 is 0.041001319885253906 sec\n",
      "Epoch 65/150\n",
      "46/46 [==============================] - 11s 229ms/step - d_loss: 0.3001 - g_loss: 2.2369\n",
      "Time for epoch 65 is 0.03999948501586914 sec\n",
      "Epoch 66/150\n",
      "46/46 [==============================] - 11s 232ms/step - d_loss: 0.4094 - g_loss: 1.7146\n",
      "Time for epoch 66 is 0.04000377655029297 sec\n",
      "Epoch 67/150\n",
      "46/46 [==============================] - 11s 244ms/step - d_loss: 0.3790 - g_loss: 1.8635\n",
      "Time for epoch 67 is 0.03999948501586914 sec\n",
      "Epoch 68/150\n",
      "46/46 [==============================] - 11s 243ms/step - d_loss: 0.3833 - g_loss: 2.1257\n",
      "Time for epoch 68 is 0.04234600067138672 sec\n",
      "Epoch 69/150\n",
      "46/46 [==============================] - 12s 268ms/step - d_loss: 0.4427 - g_loss: 2.0068\n",
      "Time for epoch 69 is 0.05699968338012695 sec\n",
      "Epoch 70/150\n",
      "46/46 [==============================] - 14s 309ms/step - d_loss: 0.3143 - g_loss: 2.3072\n",
      "Time for epoch 70 is 0.0409998893737793 sec\n",
      "Epoch 71/150\n",
      "46/46 [==============================] - 12s 261ms/step - d_loss: 0.3162 - g_loss: 2.0259\n",
      "Time for epoch 71 is 0.039999961853027344 sec\n",
      "Epoch 72/150\n",
      "46/46 [==============================] - 11s 228ms/step - d_loss: 0.3268 - g_loss: 2.2665\n",
      "Time for epoch 72 is 0.0449976921081543 sec\n",
      "Epoch 73/150\n",
      "46/46 [==============================] - 14s 294ms/step - d_loss: 0.3133 - g_loss: 1.8943\n",
      "Time for epoch 73 is 0.040001630783081055 sec\n",
      "Epoch 74/150\n",
      "46/46 [==============================] - 11s 243ms/step - d_loss: 0.3510 - g_loss: 1.9939\n",
      "Time for epoch 74 is 0.03900003433227539 sec\n",
      "Epoch 75/150\n",
      "46/46 [==============================] - 14s 300ms/step - d_loss: 0.3102 - g_loss: 1.8718\n",
      "Time for epoch 75 is 0.1300208568572998 sec\n",
      "Epoch 76/150\n",
      "46/46 [==============================] - 12s 259ms/step - d_loss: 0.2956 - g_loss: 1.9977\n",
      "Time for epoch 76 is 0.039999961853027344 sec\n",
      "Epoch 77/150\n",
      "46/46 [==============================] - 11s 245ms/step - d_loss: 0.3176 - g_loss: 1.9127\n",
      "Time for epoch 77 is 0.06548857688903809 sec\n",
      "Epoch 78/150\n",
      "46/46 [==============================] - 13s 280ms/step - d_loss: 0.3281 - g_loss: 1.8882\n",
      "Time for epoch 78 is 0.040003299713134766 sec\n",
      "Epoch 79/150\n",
      "46/46 [==============================] - 12s 263ms/step - d_loss: 0.2912 - g_loss: 2.0387\n",
      "Time for epoch 79 is 0.039609670639038086 sec\n",
      "Epoch 80/150\n",
      "46/46 [==============================] - 13s 291ms/step - d_loss: 0.3079 - g_loss: 1.9954\n",
      "Time for epoch 80 is 0.03900265693664551 sec\n",
      "Epoch 81/150\n",
      "46/46 [==============================] - 11s 243ms/step - d_loss: 0.3043 - g_loss: 2.1190\n",
      "Time for epoch 81 is 0.05100107192993164 sec\n",
      "Epoch 82/150\n",
      "46/46 [==============================] - 12s 249ms/step - d_loss: 0.2850 - g_loss: 2.2720\n",
      "Time for epoch 82 is 0.0500028133392334 sec\n",
      "Epoch 83/150\n",
      "46/46 [==============================] - 11s 234ms/step - d_loss: 0.3468 - g_loss: 2.1329\n",
      "Time for epoch 83 is 0.039000749588012695 sec\n",
      "Epoch 84/150\n",
      "46/46 [==============================] - 11s 233ms/step - d_loss: 0.3392 - g_loss: 2.1161\n",
      "Time for epoch 84 is 0.04100441932678223 sec\n",
      "Epoch 85/150\n",
      "46/46 [==============================] - 10s 224ms/step - d_loss: 0.3128 - g_loss: 2.0460\n",
      "Time for epoch 85 is 0.039000511169433594 sec\n",
      "Epoch 86/150\n",
      "46/46 [==============================] - 10s 224ms/step - d_loss: 0.3092 - g_loss: 1.9477\n",
      "Time for epoch 86 is 0.038999319076538086 sec\n",
      "Epoch 87/150\n",
      "46/46 [==============================] - 10s 224ms/step - d_loss: 0.2856 - g_loss: 2.0214\n",
      "Time for epoch 87 is 0.03900003433227539 sec\n",
      "Epoch 88/150\n",
      "46/46 [==============================] - 11s 245ms/step - d_loss: 0.2679 - g_loss: 2.0796\n",
      "Time for epoch 88 is 0.039002180099487305 sec\n",
      "Epoch 89/150\n",
      "46/46 [==============================] - 14s 307ms/step - d_loss: 0.2843 - g_loss: 2.1130\n",
      "Time for epoch 89 is 0.03900003433227539 sec\n",
      "Epoch 90/150\n",
      "46/46 [==============================] - 12s 263ms/step - d_loss: 0.3029 - g_loss: 2.1898\n",
      "Time for epoch 90 is 0.1059105396270752 sec\n",
      "Epoch 91/150\n",
      "46/46 [==============================] - 12s 255ms/step - d_loss: 0.2542 - g_loss: 2.3441\n",
      "Time for epoch 91 is 0.03899717330932617 sec\n",
      "Epoch 92/150\n",
      "46/46 [==============================] - 13s 288ms/step - d_loss: 0.3744 - g_loss: 2.0223\n",
      "Time for epoch 92 is 0.03999972343444824 sec\n",
      "Epoch 93/150\n",
      "46/46 [==============================] - 11s 234ms/step - d_loss: 0.3113 - g_loss: 2.1239\n",
      "Time for epoch 93 is 0.0390164852142334 sec\n",
      "Epoch 94/150\n",
      "46/46 [==============================] - 12s 255ms/step - d_loss: 0.3263 - g_loss: 1.9998\n",
      "Time for epoch 94 is 0.04900026321411133 sec\n",
      "Epoch 95/150\n",
      "46/46 [==============================] - 11s 234ms/step - d_loss: 0.3633 - g_loss: 2.0128\n",
      "Time for epoch 95 is 0.04300045967102051 sec\n",
      "Epoch 96/150\n",
      "46/46 [==============================] - 12s 266ms/step - d_loss: 0.2405 - g_loss: 2.5089\n",
      "Time for epoch 96 is 0.03899669647216797 sec\n",
      "Epoch 97/150\n",
      "46/46 [==============================] - 12s 257ms/step - d_loss: 0.1243 - g_loss: 3.0504\n",
      "Time for epoch 97 is 0.03900337219238281 sec\n",
      "Epoch 98/150\n",
      "46/46 [==============================] - 10s 227ms/step - d_loss: 0.2635 - g_loss: 2.4538\n",
      "Time for epoch 98 is 0.038999319076538086 sec\n",
      "Epoch 99/150\n",
      "46/46 [==============================] - 11s 235ms/step - d_loss: 0.3762 - g_loss: 2.2554\n",
      "Time for epoch 99 is 0.03899884223937988 sec\n",
      "Epoch 100/150\n",
      "46/46 [==============================] - 12s 256ms/step - d_loss: 0.2165 - g_loss: 2.6166\n",
      "Time for epoch 100 is 0.03835940361022949 sec\n",
      "Epoch 101/150\n",
      "46/46 [==============================] - 13s 277ms/step - d_loss: 0.4149 - g_loss: 1.8822\n",
      "Time for epoch 101 is 0.03800320625305176 sec\n",
      "Epoch 102/150\n",
      "46/46 [==============================] - 13s 280ms/step - d_loss: 0.3229 - g_loss: 1.9507\n",
      "Time for epoch 102 is 0.03899788856506348 sec\n",
      "Epoch 103/150\n",
      "46/46 [==============================] - 13s 283ms/step - d_loss: 0.3253 - g_loss: 1.9324\n",
      "Time for epoch 103 is 0.046999216079711914 sec\n",
      "Epoch 104/150\n",
      "46/46 [==============================] - 13s 281ms/step - d_loss: 0.3466 - g_loss: 1.8153\n",
      "Time for epoch 104 is 0.038999319076538086 sec\n",
      "Epoch 105/150\n",
      "46/46 [==============================] - 12s 252ms/step - d_loss: 0.2972 - g_loss: 1.9557\n",
      "Time for epoch 105 is 0.09799885749816895 sec\n",
      "Epoch 106/150\n",
      "46/46 [==============================] - 12s 250ms/step - d_loss: 0.2989 - g_loss: 1.9137\n",
      "Time for epoch 106 is 0.03900027275085449 sec\n",
      "Epoch 107/150\n",
      "46/46 [==============================] - 11s 246ms/step - d_loss: 0.3216 - g_loss: 1.8953\n",
      "Time for epoch 107 is 0.03899836540222168 sec\n",
      "Epoch 108/150\n",
      "46/46 [==============================] - 11s 242ms/step - d_loss: 0.2834 - g_loss: 2.0397\n",
      "Time for epoch 108 is 0.03800082206726074 sec\n",
      "Epoch 109/150\n",
      "46/46 [==============================] - 12s 271ms/step - d_loss: 0.3044 - g_loss: 2.0783\n",
      "Time for epoch 109 is 0.0390012264251709 sec\n",
      "Epoch 110/150\n",
      "46/46 [==============================] - 13s 283ms/step - d_loss: 0.3052 - g_loss: 1.9637\n",
      "Time for epoch 110 is 0.03900027275085449 sec\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 12s 259ms/step - d_loss: 0.2540 - g_loss: 2.1161\n",
      "Time for epoch 111 is 0.03899836540222168 sec\n",
      "Epoch 112/150\n",
      "46/46 [==============================] - 13s 281ms/step - d_loss: 0.2956 - g_loss: 2.0321\n",
      "Time for epoch 112 is 0.04999828338623047 sec\n",
      "Epoch 113/150\n",
      "46/46 [==============================] - 14s 301ms/step - d_loss: 0.3399 - g_loss: 1.9231\n",
      "Time for epoch 113 is 0.04099917411804199 sec\n",
      "Epoch 114/150\n",
      "46/46 [==============================] - 12s 261ms/step - d_loss: 0.2985 - g_loss: 2.0656\n",
      "Time for epoch 114 is 0.038999319076538086 sec\n",
      "Epoch 115/150\n",
      "46/46 [==============================] - 12s 259ms/step - d_loss: 0.2742 - g_loss: 2.0557\n",
      "Time for epoch 115 is 0.039002180099487305 sec\n",
      "Epoch 116/150\n",
      "46/46 [==============================] - 11s 226ms/step - d_loss: 0.2623 - g_loss: 2.1665\n",
      "Time for epoch 116 is 0.0390009880065918 sec\n",
      "Epoch 117/150\n",
      "46/46 [==============================] - 17s 364ms/step - d_loss: 0.2790 - g_loss: 2.1314\n",
      "Time for epoch 117 is 0.05700087547302246 sec\n",
      "Epoch 118/150\n",
      "46/46 [==============================] - 12s 253ms/step - d_loss: 0.3053 - g_loss: 1.9981\n",
      "Time for epoch 118 is 0.0989997386932373 sec\n",
      "Epoch 119/150\n",
      "46/46 [==============================] - 11s 239ms/step - d_loss: 0.2948 - g_loss: 2.1696\n",
      "Time for epoch 119 is 0.04800128936767578 sec\n",
      "Epoch 120/150\n",
      "46/46 [==============================] - 11s 249ms/step - d_loss: 0.2634 - g_loss: 2.2057\n",
      "Time for epoch 120 is 0.10599708557128906 sec\n",
      "Epoch 121/150\n",
      "46/46 [==============================] - 12s 269ms/step - d_loss: 0.3083 - g_loss: 1.9832\n",
      "Time for epoch 121 is 0.056000471115112305 sec\n",
      "Epoch 122/150\n",
      "46/46 [==============================] - 14s 297ms/step - d_loss: 0.3317 - g_loss: 1.9822\n",
      "Time for epoch 122 is 0.0579981803894043 sec\n",
      "Epoch 123/150\n",
      "46/46 [==============================] - 12s 256ms/step - d_loss: 0.2761 - g_loss: 2.1110\n",
      "Time for epoch 123 is 0.04800271987915039 sec\n",
      "Epoch 124/150\n",
      "46/46 [==============================] - 12s 250ms/step - d_loss: 0.2453 - g_loss: 2.2246\n",
      "Time for epoch 124 is 0.08035492897033691 sec\n",
      "Epoch 125/150\n",
      "46/46 [==============================] - 12s 255ms/step - d_loss: 0.3452 - g_loss: 2.0217\n",
      "Time for epoch 125 is 0.04100942611694336 sec\n",
      "Epoch 126/150\n",
      "46/46 [==============================] - 16s 339ms/step - d_loss: 0.2820 - g_loss: 2.0897\n",
      "Time for epoch 126 is 0.04199838638305664 sec\n",
      "Epoch 127/150\n",
      "46/46 [==============================] - 12s 268ms/step - d_loss: 0.3281 - g_loss: 2.0579\n",
      "Time for epoch 127 is 0.0410003662109375 sec\n",
      "Epoch 128/150\n",
      "46/46 [==============================] - 15s 313ms/step - d_loss: 0.2806 - g_loss: 2.0439\n",
      "Time for epoch 128 is 0.039000511169433594 sec\n",
      "Epoch 129/150\n",
      "46/46 [==============================] - 12s 263ms/step - d_loss: 0.2710 - g_loss: 2.0553\n",
      "Time for epoch 129 is 0.037999868392944336 sec\n",
      "Epoch 130/150\n",
      "46/46 [==============================] - 11s 233ms/step - d_loss: 0.2664 - g_loss: 2.1704\n",
      "Time for epoch 130 is 0.0390019416809082 sec\n",
      "Epoch 131/150\n",
      "46/46 [==============================] - 11s 239ms/step - d_loss: 0.3044 - g_loss: 2.2350\n",
      "Time for epoch 131 is 0.0390009880065918 sec\n",
      "Epoch 132/150\n",
      "46/46 [==============================] - 14s 295ms/step - d_loss: 0.2490 - g_loss: 2.2595\n",
      "Time for epoch 132 is 0.0559995174407959 sec\n",
      "Epoch 133/150\n",
      "46/46 [==============================] - 11s 229ms/step - d_loss: 0.3239 - g_loss: 1.9272\n",
      "Time for epoch 133 is 0.048000335693359375 sec\n",
      "Epoch 134/150\n",
      "46/46 [==============================] - 11s 233ms/step - d_loss: 0.3217 - g_loss: 2.0116\n",
      "Time for epoch 134 is 0.04900050163269043 sec\n",
      "Epoch 135/150\n",
      "46/46 [==============================] - 11s 238ms/step - d_loss: 0.2835 - g_loss: 2.1022\n",
      "Time for epoch 135 is 0.1064600944519043 sec\n",
      "Epoch 136/150\n",
      "46/46 [==============================] - 11s 236ms/step - d_loss: 0.2442 - g_loss: 2.2522\n",
      "Time for epoch 136 is 0.05800056457519531 sec\n",
      "Epoch 137/150\n",
      "46/46 [==============================] - 11s 237ms/step - d_loss: 0.1753 - g_loss: 2.7624\n",
      "Time for epoch 137 is 0.04699897766113281 sec\n",
      "Epoch 138/150\n",
      "46/46 [==============================] - 11s 234ms/step - d_loss: 0.2844 - g_loss: 2.7556\n",
      "Time for epoch 138 is 0.04900097846984863 sec\n",
      "Epoch 139/150\n",
      "46/46 [==============================] - 11s 236ms/step - d_loss: 0.3342 - g_loss: 2.0866\n",
      "Time for epoch 139 is 0.05000162124633789 sec\n",
      "Epoch 140/150\n",
      "46/46 [==============================] - 11s 237ms/step - d_loss: 0.3154 - g_loss: 2.1642\n",
      "Time for epoch 140 is 0.05299782752990723 sec\n",
      "Epoch 141/150\n",
      "46/46 [==============================] - 11s 247ms/step - d_loss: 0.2818 - g_loss: 2.2411\n",
      "Time for epoch 141 is 0.048999786376953125 sec\n",
      "Epoch 142/150\n",
      "46/46 [==============================] - 11s 237ms/step - d_loss: 0.2828 - g_loss: 2.1849\n",
      "Time for epoch 142 is 0.05300116539001465 sec\n",
      "Epoch 143/150\n",
      "46/46 [==============================] - 11s 235ms/step - d_loss: 0.3325 - g_loss: 2.1149\n",
      "Time for epoch 143 is 0.04799914360046387 sec\n",
      "Epoch 144/150\n",
      "46/46 [==============================] - 12s 252ms/step - d_loss: 0.2716 - g_loss: 2.2913\n",
      "Time for epoch 144 is 0.0449979305267334 sec\n",
      "Epoch 145/150\n",
      "46/46 [==============================] - 11s 234ms/step - d_loss: 0.2487 - g_loss: 2.4674\n",
      "Time for epoch 145 is 0.06000089645385742 sec\n",
      "Epoch 146/150\n",
      "46/46 [==============================] - 11s 242ms/step - d_loss: 0.1428 - g_loss: 2.9379\n",
      "Time for epoch 146 is 0.05400228500366211 sec\n",
      "Epoch 147/150\n",
      "46/46 [==============================] - 12s 251ms/step - d_loss: 0.3568 - g_loss: 2.2847\n",
      "Time for epoch 147 is 0.07200336456298828 sec\n",
      "Epoch 148/150\n",
      "46/46 [==============================] - 11s 246ms/step - d_loss: 0.2907 - g_loss: 2.1981\n",
      "Time for epoch 148 is 0.04399871826171875 sec\n",
      "Epoch 149/150\n",
      "46/46 [==============================] - 12s 257ms/step - d_loss: 0.3386 - g_loss: 2.0395\n",
      "Time for epoch 149 is 0.04399871826171875 sec\n",
      "Epoch 150/150\n",
      "46/46 [==============================] - 14s 303ms/step - d_loss: 0.2697 - g_loss: 2.1362\n",
      "Time for epoch 150 is 0.10200023651123047 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aa7b8e29d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 150  # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Was 0.0001\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Some of the last generated images around epoch 30\n",
    "(results keep improving after that):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan_overriding_train_step",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
