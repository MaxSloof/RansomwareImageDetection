{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 80\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v005\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/conditional_gan\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/conditional_gan/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/cgan-images/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/cgan-local-v{new_vnum:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 64)        6400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 227,969\n",
      "Trainable params: 227,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 8832)              1227648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 8832)              0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 8, 8, 138)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 16, 16, 128)       282752    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DT (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DT (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 2,041,217\n",
      "Trainable params: 2,041,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d6d35",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "407f8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da998a",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c457b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0652a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'{new_dir}'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "196/196 [==============================] - 23s 114ms/step - g_loss: 1.7453 - d_loss: 0.5438\n",
      "Epoch 2/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.8044 - d_loss: 0.1342\n",
      "Epoch 3/80\n",
      "196/196 [==============================] - 22s 111ms/step - g_loss: 6.6527 - d_loss: 0.0725\n",
      "Epoch 4/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.9598 - d_loss: 0.0586\n",
      "Epoch 5/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 6.4153 - d_loss: 0.1797\n",
      "Epoch 6/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.5808 - d_loss: 0.1608\n",
      "Epoch 7/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.0948 - d_loss: 0.6199\n",
      "Epoch 8/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1938 - d_loss: 0.6105\n",
      "Epoch 9/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.8627 - d_loss: 0.4920\n",
      "Epoch 10/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.3407 - d_loss: 0.4927\n",
      "Epoch 11/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.8124 - d_loss: 0.4392\n",
      "Epoch 12/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3207 - d_loss: 0.5749\n",
      "Epoch 13/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5924 - d_loss: 0.4618\n",
      "Epoch 14/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3458 - d_loss: 0.5326\n",
      "Epoch 15/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.4254 - d_loss: 0.5765\n",
      "Epoch 16/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1743 - d_loss: 0.6840\n",
      "Epoch 17/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1342 - d_loss: 0.5715\n",
      "Epoch 18/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1249 - d_loss: 0.5515\n",
      "Epoch 19/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0712 - d_loss: 0.6255\n",
      "Epoch 20/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0814 - d_loss: 0.6062\n",
      "Epoch 21/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2052 - d_loss: 0.5899\n",
      "Epoch 22/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9620 - d_loss: 0.6144\n",
      "Epoch 23/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 0.9859 - d_loss: 0.6203\n",
      "Epoch 24/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9580 - d_loss: 0.6143\n",
      "Epoch 25/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9834 - d_loss: 0.6269\n",
      "Epoch 26/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0266 - d_loss: 0.6209\n",
      "Epoch 27/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0565 - d_loss: 0.6382\n",
      "Epoch 28/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.0611 - d_loss: 0.6533\n",
      "Epoch 29/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.1161 - d_loss: 0.6153\n",
      "Epoch 30/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.1093 - d_loss: 0.7231\n",
      "Epoch 31/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.0905 - d_loss: 0.6072\n",
      "Epoch 32/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0574 - d_loss: 0.6288\n",
      "Epoch 33/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0529 - d_loss: 0.6146\n",
      "Epoch 34/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.0445 - d_loss: 0.6364\n",
      "Epoch 35/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1737 - d_loss: 0.6411\n",
      "Epoch 36/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1023 - d_loss: 0.6256\n",
      "Epoch 37/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0960 - d_loss: 0.6356\n",
      "Epoch 38/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1291 - d_loss: 0.6085\n",
      "Epoch 39/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1398 - d_loss: 0.6060\n",
      "Epoch 40/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1341 - d_loss: 0.5934\n",
      "Epoch 41/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2083 - d_loss: 0.6024\n",
      "Epoch 42/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1992 - d_loss: 0.6355\n",
      "Epoch 43/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1859 - d_loss: 0.5711\n",
      "Epoch 44/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3013 - d_loss: 0.5736\n",
      "Epoch 45/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3345 - d_loss: 0.6031\n",
      "Epoch 46/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3106 - d_loss: 0.5619\n",
      "Epoch 47/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.2144 - d_loss: 0.5744\n",
      "Epoch 48/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3207 - d_loss: 0.5262\n",
      "Epoch 49/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2961 - d_loss: 0.5405\n",
      "Epoch 50/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5435 - d_loss: 0.5919\n",
      "Epoch 51/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3590 - d_loss: 0.5307\n",
      "Epoch 52/80\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3506 - d_loss: 0.5524\n",
      "Epoch 53/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.2878 - d_loss: 0.5451\n",
      "Epoch 54/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.3527 - d_loss: 0.5293\n",
      "Epoch 55/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.2411 - d_loss: 0.5731\n",
      "Epoch 56/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2683 - d_loss: 0.5547\n",
      "Epoch 57/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4918 - d_loss: 0.5765\n",
      "Epoch 58/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4443 - d_loss: 0.5402\n",
      "Epoch 59/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4084 - d_loss: 0.5348\n",
      "Epoch 60/80\n",
      "196/196 [==============================] - 25s 127ms/step - g_loss: 1.4813 - d_loss: 0.5168\n",
      "Epoch 61/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.2949 - d_loss: 0.5908\n",
      "Epoch 62/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3302 - d_loss: 0.5289\n",
      "Epoch 63/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.4071 - d_loss: 0.5506\n",
      "Epoch 64/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.5196 - d_loss: 0.5104\n",
      "Epoch 65/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.5148 - d_loss: 0.5708\n",
      "Epoch 66/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.4960 - d_loss: 0.5000\n",
      "Epoch 67/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.6066 - d_loss: 0.5112\n",
      "Epoch 68/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.6179 - d_loss: 0.5285\n",
      "Epoch 69/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.6278 - d_loss: 0.5085\n",
      "Epoch 70/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 2.1286 - d_loss: 0.4861\n",
      "Epoch 71/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5398 - d_loss: 0.5399\n",
      "Epoch 72/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.4759 - d_loss: 0.5692\n",
      "Epoch 73/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.3865 - d_loss: 0.5631\n",
      "Epoch 74/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.7165 - d_loss: 0.5434\n",
      "Epoch 75/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.4365 - d_loss: 0.5675\n",
      "Epoch 76/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.4181 - d_loss: 0.5570\n",
      "Epoch 77/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.5077 - d_loss: 0.5556\n",
      "Epoch 78/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.0475 - d_loss: 0.6063\n",
      "Epoch 79/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0995 - d_loss: 0.6647\n",
      "Epoch 80/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0671 - d_loss: 0.6582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x222d5bc4070>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Interpolating between classes with the trained GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 10000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6ba712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ec4aadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ee6b52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    for j in range(num_interpolation):\n",
    "        np.save(file=f\"{class_dir}/gen_imgs_{class_name}_{j}.npy\", arr = converted_images[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f4a4f",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c673f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    test = np.load(\"C:/Users/Max/Documents/image_data/cgan-local-v005/Sfone/gen_imgs_Sfone_25.npy\")\n",
    "    plt.imshow(test, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e169f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
