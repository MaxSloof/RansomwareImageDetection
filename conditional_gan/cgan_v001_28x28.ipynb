{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 28,28\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 20\n",
    "\n",
    "# Training - Validiation split \n",
    "val_size = 0.3\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v002\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/conditional_gan\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"/Users/Max/Documents/GitHub/conditional_gan/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/gan-data\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e7736049",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f5d7add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "351830f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "79b80bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32b949",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d69736c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809c22c",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "6d5e2b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5576c9",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "11b7b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a60f102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(7 * 7 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b71e1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 14, 14, 64)        6400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 80,385\n",
      "Trainable params: 80,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "aa7ccc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 6762)              939918    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)  (None, 6762)              0         \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 7, 7, 138)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_67 (Conv2DT (None, 14, 14, 128)       282752    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_68 (Conv2DT (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_103 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 28, 28, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 1,491,215\n",
      "Trainable params: 1,491,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc36cd2",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4710cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31602e",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "486d06cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "196/196 [==============================] - 6s 27ms/step - g_loss: 0.8593 - d_loss: 0.6556\n",
      "Epoch 2/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 1.0493 - d_loss: 0.5956\n",
      "Epoch 3/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.8534 - d_loss: 0.6594\n",
      "Epoch 4/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 1.2847 - d_loss: 0.5256\n",
      "Epoch 5/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7747 - d_loss: 0.6778\n",
      "Epoch 6/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.8108 - d_loss: 0.6288\n",
      "Epoch 7/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.8247 - d_loss: 0.6291\n",
      "Epoch 8/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.8729 - d_loss: 0.6059\n",
      "Epoch 9/20\n",
      "196/196 [==============================] - 6s 28ms/step - g_loss: 0.9598 - d_loss: 0.6178\n",
      "Epoch 10/20\n",
      "196/196 [==============================] - 6s 30ms/step - g_loss: 0.8384 - d_loss: 0.6223\n",
      "Epoch 11/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.8813 - d_loss: 0.6286\n",
      "Epoch 12/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.7671 - d_loss: 0.7100\n",
      "Epoch 13/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.7184 - d_loss: 0.6900\n",
      "Epoch 14/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7294 - d_loss: 0.6830\n",
      "Epoch 15/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7480 - d_loss: 0.7104\n",
      "Epoch 16/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7380 - d_loss: 0.7057\n",
      "Epoch 17/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7170 - d_loss: 0.6965\n",
      "Epoch 18/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7777 - d_loss: 0.7143\n",
      "Epoch 19/20\n",
      "196/196 [==============================] - 5s 26ms/step - g_loss: 0.7354 - d_loss: 0.6838\n",
      "Epoch 20/20\n",
      "196/196 [==============================] - 5s 27ms/step - g_loss: 0.7214 - d_loss: 0.7235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2f02c500af0>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d514de1",
   "metadata": {},
   "source": [
    "# Interpolating between classes with the trained GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c1d3d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 9  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n",
    "\n",
    "start_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "end_class = 6  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "fake_images = interpolate_class(start_class, end_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "be4dbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images *= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f3db2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_images = fake_images.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c8986d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8875be1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 64, 64, 1)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "afd06705",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2632/1693168181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\master_thesis\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallnums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m         \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# crude validation of num argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[0mmanager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pylab_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGcf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fig_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "plt.figure(converted_images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "55325c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in range(num_interpolation): \n",
    "    plt.imshow((converted_images[2]), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4754af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
