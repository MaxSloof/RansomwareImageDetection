{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 80\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v005\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/conditional_gan\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/conditional_gan/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/cgan-images/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/cgan-local-v{new_vnum:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 64)        6400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 227,969\n",
      "Trainable params: 227,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 8832)              1227648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 8832)              0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 8, 8, 138)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 16, 16, 128)       282752    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DT (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DT (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 2,041,217\n",
      "Trainable params: 2,041,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70546",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf8cba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c67",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af608a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b649da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'{new_dir}'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "196/196 [==============================] - 23s 114ms/step - g_loss: 1.7453 - d_loss: 0.5438\n",
      "Epoch 2/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.8044 - d_loss: 0.1342\n",
      "Epoch 3/80\n",
      "196/196 [==============================] - 22s 111ms/step - g_loss: 6.6527 - d_loss: 0.0725\n",
      "Epoch 4/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 3.9598 - d_loss: 0.0586\n",
      "Epoch 5/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 6.4153 - d_loss: 0.1797\n",
      "Epoch 6/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 4.5808 - d_loss: 0.1608\n",
      "Epoch 7/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.0948 - d_loss: 0.6199\n",
      "Epoch 8/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1938 - d_loss: 0.6105\n",
      "Epoch 9/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.8627 - d_loss: 0.4920\n",
      "Epoch 10/80\n",
      "196/196 [==============================] - 24s 121ms/step - g_loss: 1.3407 - d_loss: 0.4927\n",
      "Epoch 11/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.8124 - d_loss: 0.4392\n",
      "Epoch 12/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3207 - d_loss: 0.5749\n",
      "Epoch 13/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5924 - d_loss: 0.4618\n",
      "Epoch 14/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3458 - d_loss: 0.5326\n",
      "Epoch 15/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.4254 - d_loss: 0.5765\n",
      "Epoch 16/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1743 - d_loss: 0.6840\n",
      "Epoch 17/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1342 - d_loss: 0.5715\n",
      "Epoch 18/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.1249 - d_loss: 0.5515\n",
      "Epoch 19/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0712 - d_loss: 0.6255\n",
      "Epoch 20/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0814 - d_loss: 0.6062\n",
      "Epoch 21/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2052 - d_loss: 0.5899\n",
      "Epoch 22/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9620 - d_loss: 0.6144\n",
      "Epoch 23/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 0.9859 - d_loss: 0.6203\n",
      "Epoch 24/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9580 - d_loss: 0.6143\n",
      "Epoch 25/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9834 - d_loss: 0.6269\n",
      "Epoch 26/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0266 - d_loss: 0.6209\n",
      "Epoch 27/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0565 - d_loss: 0.6382\n",
      "Epoch 28/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.0611 - d_loss: 0.6533\n",
      "Epoch 29/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.1161 - d_loss: 0.6153\n",
      "Epoch 30/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.1093 - d_loss: 0.7231\n",
      "Epoch 31/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.0905 - d_loss: 0.6072\n",
      "Epoch 32/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0574 - d_loss: 0.6288\n",
      "Epoch 33/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0529 - d_loss: 0.6146\n",
      "Epoch 34/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.0445 - d_loss: 0.6364\n",
      "Epoch 35/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1737 - d_loss: 0.6411\n",
      "Epoch 36/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1023 - d_loss: 0.6256\n",
      "Epoch 37/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0960 - d_loss: 0.6356\n",
      "Epoch 38/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1291 - d_loss: 0.6085\n",
      "Epoch 39/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1398 - d_loss: 0.6060\n",
      "Epoch 40/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.1341 - d_loss: 0.5934\n",
      "Epoch 41/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.2083 - d_loss: 0.6024\n",
      "Epoch 42/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1992 - d_loss: 0.6355\n",
      "Epoch 43/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.1859 - d_loss: 0.5711\n",
      "Epoch 44/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3013 - d_loss: 0.5736\n",
      "Epoch 45/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.3345 - d_loss: 0.6031\n",
      "Epoch 46/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3106 - d_loss: 0.5619\n",
      "Epoch 47/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.2144 - d_loss: 0.5744\n",
      "Epoch 48/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3207 - d_loss: 0.5262\n",
      "Epoch 49/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2961 - d_loss: 0.5405\n",
      "Epoch 50/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5435 - d_loss: 0.5919\n",
      "Epoch 51/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3590 - d_loss: 0.5307\n",
      "Epoch 52/80\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 1.3506 - d_loss: 0.5524\n",
      "Epoch 53/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.2878 - d_loss: 0.5451\n",
      "Epoch 54/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.3527 - d_loss: 0.5293\n",
      "Epoch 55/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.2411 - d_loss: 0.5731\n",
      "Epoch 56/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.2683 - d_loss: 0.5547\n",
      "Epoch 57/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4918 - d_loss: 0.5765\n",
      "Epoch 58/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4443 - d_loss: 0.5402\n",
      "Epoch 59/80\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4084 - d_loss: 0.5348\n",
      "Epoch 60/80\n",
      "196/196 [==============================] - 25s 127ms/step - g_loss: 1.4813 - d_loss: 0.5168\n",
      "Epoch 61/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.2949 - d_loss: 0.5908\n",
      "Epoch 62/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.3302 - d_loss: 0.5289\n",
      "Epoch 63/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.4071 - d_loss: 0.5506\n",
      "Epoch 64/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.5196 - d_loss: 0.5104\n",
      "Epoch 65/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.5148 - d_loss: 0.5708\n",
      "Epoch 66/80\n",
      "196/196 [==============================] - 24s 120ms/step - g_loss: 1.4960 - d_loss: 0.5000\n",
      "Epoch 67/80\n",
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.6066 - d_loss: 0.5112\n",
      "Epoch 68/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.6179 - d_loss: 0.5285\n",
      "Epoch 69/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.6278 - d_loss: 0.5085\n",
      "Epoch 70/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 2.1286 - d_loss: 0.4861\n",
      "Epoch 71/80\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.5398 - d_loss: 0.5399\n",
      "Epoch 72/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.4759 - d_loss: 0.5692\n",
      "Epoch 73/80\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.3865 - d_loss: 0.5631\n",
      "Epoch 74/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.7165 - d_loss: 0.5434\n",
      "Epoch 75/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.4365 - d_loss: 0.5675\n",
      "Epoch 76/80\n",
      "196/196 [==============================] - 24s 122ms/step - g_loss: 1.4181 - d_loss: 0.5570\n",
      "Epoch 77/80\n",
      "196/196 [==============================] - 25s 126ms/step - g_loss: 1.5077 - d_loss: 0.5556\n",
      "Epoch 78/80\n",
      "196/196 [==============================] - 24s 123ms/step - g_loss: 1.0475 - d_loss: 0.6063\n",
      "Epoch 79/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 23s 119ms/step - g_loss: 1.0995 - d_loss: 0.6647\n",
      "Epoch 80/80\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 1.0671 - d_loss: 0.6582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x222d5bc4070>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Interpolating between classes with the trained GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 10000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "780b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0de5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79dfad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    for j in range(num_interpolation):\n",
    "        np.save(file=f\"{class_dir}/gen_imgs_{class_name}_{j}.npy\", arr = converted_images[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62104ec",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2c55fe33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5b7bd969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    test = np.load(\"C:/Users/Max/Documents/image_data/cgan-local-v005/Sfone/gen_imgs_Sfone_25.npy\")\n",
    "    plt.imshow(test, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a3276344",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.squeeze(test, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b24ec501",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray((test*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25985421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
