{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KXQu67zVoZAw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size (height x width)\n",
    "ih = 64\n",
    "iw = 64\n",
    "\n",
    "# Grayscale or RGB\n",
    "ch = 'rgb'\n",
    "\n",
    "# Batch size \n",
    "batch_size = 64\n",
    "\n",
    "# Layer adapt\n",
    "ksize = 4 # Kernel size : was '4' for 64x64 image\n",
    "ssize = 2 # Stride size : was '2' for 64x64 image\n",
    "\n",
    "# Size of test set (in %)\n",
    "testsize = 0.3\n",
    "\n",
    "# Number of epochs in model\n",
    "epoch_t = 40\n",
    "\n",
    "# Where computation is performed: Kaggle (0) or Local (1)\n",
    "cenv = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation environment: Local\n"
     ]
    }
   ],
   "source": [
    "if cenv == 0:\n",
    "    print(\"Computation environment: Kaggle\")\n",
    "if cenv == 1:\n",
    "    print(\"Computation environment: Local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new directory for version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 matches(es) found\n",
      "--------------\n",
      "New folder name: ResNet-local-v008\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/ResNet\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"ResNet\" in files: \n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: ResNet-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"/Users/Max/Documents/GitHub/ResNet/ResNet-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T09:05:56.933069Z",
     "iopub.status.busy": "2022-03-25T09:05:56.932806Z",
     "iopub.status.idle": "2022-03-25T09:05:56.940533Z",
     "shell.execute_reply": "2022-03-25T09:05:56.939803Z",
     "shell.execute_reply.started": "2022-03-25T09:05:56.93304Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/thesis-data\"\n",
    "    \n",
    "    # Directory where checkpoints of DCGAN are stored\n",
    "    checkpoint_dir = \"/kaggle/input/checkpoints\" \n",
    "\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/thesis_data\"\n",
    "    path_gen_images = \"C:/Users/Max/Documents/image_data/cgan-local-v005\"\n",
    "    \n",
    "    # Directory where checkpoints of DCGAN are stored\n",
    "    checkpoint_dir = 'C:/Users/Max/Documents/GitHub/dcgan_kaggle_output/dcgan-kaggle-v002/checkpoints'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_si = (ih, iw)\n",
    "\n",
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26548 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = ImageDataGenerator().flow_from_directory(\n",
    "    directory  = path_root, \n",
    "    color_mode = ch, \n",
    "    target_size= (ih,iw), \n",
    "    interpolation=\"bicubic\",\n",
    "    class_mode = 'sparse',\n",
    "    batch_size=40000\n",
    ")\n",
    "imgs, labels = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "batches_gen = ImageDataGenerator().flow_from_directory(\n",
    "    directory  = path_gen_images, \n",
    "    color_mode = ch, \n",
    "    target_size= (ih,iw), \n",
    "    interpolation=\"bicubic\",\n",
    "    class_mode = 'sparse',\n",
    "    batch_size=40000\n",
    ")\n",
    "imgs_gen, labels_gen = next(batches_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32),\n",
       " array([3951, 3956, 3928, 4007, 4036, 4026, 4034, 4133, 3899, 4030],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels_gen, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset1 = dataset1.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices((imgs_gen, labels_gen))\n",
    "dataset2 = dataset2.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset1.concatenate(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_imgs = tf.keras.applications.resnet.preprocess_input(imgs)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_imgs_gen = tf.keras.applications.resnet.preprocess_input(imgs_gen)\n",
    "labels_gen = np.array(labels_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_imgs, labels, test_size=testsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, processed_imgs_gen), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y_train, labels_gen), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 58583 | Shape of training data (58583, 64, 64, 3)\n",
      "Size of training data: 7965  | Shape of training data (7965, 64, 64, 3)\n",
      "Shape of training labels (58583,)\n",
      "Shape of training labels (7965,)\n"
     ]
    }
   ],
   "source": [
    "trainsize = len(X_train)\n",
    "testsize = len(X_test)\n",
    "\n",
    "print(f\"Size of training data: {trainsize} | Shape of training data {X_train.shape}\")\n",
    "print(f\"Size of training data: {testsize}  | Shape of training data {X_test.shape}\")\n",
    "print(f\"Shape of training labels {y_train.shape}\")\n",
    "print(f\"Shape of training labels {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "d2mRBkTBoig8",
    "outputId": "7aeee00a-bdff-41db-8473-6e04850605b5"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (64,64,3))\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "rWSL-tt4qBRa"
   },
   "outputs": [],
   "source": [
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "predictions = layers.Dense(11, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "s1sYr8cIt0nY"
   },
   "outputs": [],
   "source": [
    "head_model = Model(inputs = base_model.input, outputs = predictions)\n",
    "head_model.compile(optimizer='adam', loss=losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "anne = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\n",
    "if cenv == 0:\n",
    "    checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "if cenv == 1:\n",
    "    checkpoint = ModelCheckpoint(f'{new_dir}/model.h5', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "916/916 [==============================] - 78s 74ms/step - loss: 0.1481 - accuracy: 0.9705 - val_loss: 0.2793 - val_accuracy: 0.9238\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27933, saving model to /Users/Max/Documents/GitHub/ResNet/ResNet-local-v008\\model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "916/916 [==============================] - 69s 75ms/step - loss: 0.0659 - accuracy: 0.9812 - val_loss: 0.2551 - val_accuracy: 0.9333\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27933 to 0.25508, saving model to /Users/Max/Documents/GitHub/ResNet/ResNet-local-v008\\model.h5\n",
      "Epoch 3/40\n",
      "916/916 [==============================] - 70s 76ms/step - loss: 0.0478 - accuracy: 0.9855 - val_loss: 0.2412 - val_accuracy: 0.9380\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25508 to 0.24119, saving model to /Users/Max/Documents/GitHub/ResNet/ResNet-local-v008\\model.h5\n",
      "Epoch 4/40\n",
      "916/916 [==============================] - 70s 76ms/step - loss: 0.0445 - accuracy: 0.9870 - val_loss: 0.4645 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.24119\n",
      "Epoch 5/40\n",
      "834/916 [==========================>...] - ETA: 5s - loss: 0.0460 - accuracy: 0.9874"
     ]
    }
   ],
   "source": [
    "history = head_model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    batch_size=64, \n",
    "    epochs=epoch_t, \n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [anne, checkpoint]) # EPOCHS WAS 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b5bRktf4ydg",
    "outputId": "37f5eb06-7caa-4c5f-b6fd-9b5758a1f057"
   },
   "outputs": [],
   "source": [
    "scores = head_model.evaluate(X_test, y_test)\n",
    "print(f\"Overall CNN Accuracy: {scores[1]}\\n(The number of correct predictions divided by the number of total predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_distribution = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = batches.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = (multi_distribution[1]/labels.shape[0])*100\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(classes,perc)\n",
    "if cenv == 0:\n",
    "    plt.savefig(\"multi_data_dist.png\", bbox_inches = 'tight')\n",
    "if cenv == 1:\n",
    "    plt.savefig(f\"{new_dir}/multi_data_dist.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fmt(x):\n",
    "    return '{:.1f}%\\n({:.0f})'.format(x, total*x/100)\n",
    "total = trainsize + testsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.pie(\n",
    "    [trainsize, testsize], \n",
    "    labels = [\"Training\", \"Validation\"], \n",
    "    startangle=90, \n",
    "    counterclock=False, \n",
    "    autopct=my_fmt,\n",
    "    colors = ['gray', 'silver']\n",
    ")\n",
    "\n",
    "plt.title(\"Training and validation data distribution\")\n",
    "\n",
    "if cenv == 0:\n",
    "    plt.savefig(\"train_test_dist.png\", bbox_inches = 'tight')\n",
    "if cenv == 1:\n",
    "    plt.savefig(f\"{new_dir}/train_test_dist.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE3OUgo34v_C",
    "outputId": "fd989d09-d97f-478e-e768-0e2e739a17aa"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])\n",
    "\n",
    "if cenv == 0:\n",
    "    plt.savefig(\"performance_figure.png\", bbox_inches = 'tight')\n",
    "if cenv == 1:\n",
    "    plt.savefig(f\"{new_dir}/performance_figure.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse performance\n",
    "\n",
    "**Multiclass classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass performance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PrettyTable(['Metric', 'Performance'])\n",
    "t.add_row(['Valididation accuracy', round(scores[1],4)])\n",
    "t.add_row(['Validation loss', round(scores[0],4)])\n",
    "t.header = True\n",
    "t.align = \"l\"\n",
    "t.title = \"Perf. of multi-class classification - ResNet\"\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving PrettyTable\n",
    "table = t.get_string()\n",
    "\n",
    "if cenv == 0:\n",
    "    with open('multi_performance_table.txt', 'w') as f:\n",
    "        f.write(table)\n",
    "if cenv == 1:\n",
    "    with open(f'{new_dir}/multi_performance_table.txt', 'w') as f:\n",
    "        f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(head_model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test2 = y_test\n",
    "y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_matrix = metrics.confusion_matrix(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "   \n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    if cenv == 0:\n",
    "        plt.savefig(\"multi_class_cmatrix.png\")\n",
    "    if cenv == 1:\n",
    "        plt.savefig(f\"{new_dir}/multi_class_cmatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_names= batches.class_indices.keys()\n",
    "confusion_matrix(c_matrix, class_names, figsize = (20,7), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary classification**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "from prettytable import PrettyTable, MSWORD_FRIENDLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predbin = [] \n",
    "y_truebin = []\n",
    "for count, value in enumerate(y_test2):\n",
    "    if y_test2[count] in range(10): # range(10) is 0 to 9\n",
    "        y_truebin.append(0)\n",
    "    else: y_truebin.append(1)\n",
    "    \n",
    "    if y_pred[count] in range(10):\n",
    "        y_predbin.append(0)\n",
    "    else: y_predbin.append(1)\n",
    "    \n",
    "    continue\n",
    "if len(y_truebin) == len(y_predbin):\n",
    "    print(f\"Length of the observations in test set: {len(y_truebin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_count = 0\n",
    "bn_count = 0\n",
    "for count, value in enumerate(multi_distribution[1]):\n",
    "    if count in range(10):\n",
    "        rw_count = rw_count + multi_distribution[1][count]\n",
    "    else: \n",
    "        bn_count = bn_count + multi_distribution[1][count]\n",
    "print(f\"Ransomware Occurences: {rw_count}, Benign Occurences: {bn_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x_lab = ['Ransomware', 'Benign']\n",
    "y_lab = [rw_count, bn_count]\n",
    "ax.bar(x_lab, y_lab)\n",
    "if cenv == 0:\n",
    "    plt.savefig(\"data_dist.png.png\", bbox_inches = 'tight')\n",
    "if cenv == 1:\n",
    "    plt.savefig(f\"{new_dir}/bin_data_dist.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix_bin = metrics.confusion_matrix(y_truebin, y_predbin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_bin(confusion_matrix, class_names_bin, figsize = (5,2), fontsize=7):\n",
    "   \n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names_bin, columns=class_names_bin, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(\"ResNet152\")\n",
    "    if cenv == 0:\n",
    "        plt.savefig(\"bin_class_cmatrix.png\", bbox_inches = 'tight')\n",
    "    if cenv == 1:\n",
    "        plt.savefig(f\"{new_dir}/bin_class_cmatrix.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_bin= (\"ransomware\", \"benign\")\n",
    "confusion_matrix_bin(c_matrix_bin, class_names_bin, figsize = (5,2), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positive Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = c_matrix_bin[0,0]/(c_matrix_bin[0,0] + c_matrix_bin[0,1]) #True Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC = accuracy_score(y_truebin, y_predbin) # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = f1_score(y_truebin, y_predbin, labels=0) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matthews Correlation Coefficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCC = matthews_corrcoef(y_truebin, y_predbin) # Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = PrettyTable(['Metric', 'Performance'])\n",
    "t.add_row(['True Positive Rate', round(TPR,4)])\n",
    "t.add_row(['Accuracy', round(ACC,4)])\n",
    "t.add_row(['F1 Score', round(F1,4)])\n",
    "t.add_row(['Matthews Correlation Coefficient', round(MCC,4)])\n",
    "t.header = True\n",
    "t.align = \"l\"\n",
    "t.title = \"Performance of ResNet\"\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving PrettyTable\n",
    "table = t.get_string()\n",
    "\n",
    "if cenv == 0:\n",
    "    with open('bin_performance_table.txt', 'w') as f:\n",
    "        f.write(table)\n",
    "if cenv == 1:\n",
    "    with open(f'{new_dir}/bin_performance_table.txt', 'w') as f:\n",
    "        f.write(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
