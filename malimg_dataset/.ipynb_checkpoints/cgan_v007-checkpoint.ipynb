{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 80\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v007\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/malimg_dataset\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/malimg_dataset/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/numpy_arrays/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/malimg_paper_dataset_imgs\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/malimg-cgan-local-v{new_vnum:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9339 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ded1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ransomware family:  Lolyda.AT\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6O0lEQVR4nO2debRdVZXuvxUwNgRpBSKkQApKQQuBijQGkJ7Qt6FRMCIaCtSCoQ4BrXqFr7QEH1UKJSXF4AHR0BZNSCKSxEDoRCB0AgJGeiQmSKEgWBCS9f6456z81pd7Ti4kOTev9vzGyLjr3rXPPuusvXfOnPOb85sp56xAIPA/H0MGewGBQKA3iIc9EGgI4mEPBBqCeNgDgYYgHvZAoCGIhz0QaAiW6mFPKY1OKT2WUvpNSunUZbWoQCCw7JHeLs+eUlpJ0q8l7SHpOUl3Szoq5/yrZbe8QCCwrLDyUrx2G0m/yTk/IUkppcslHSip48P+rne9Kw8bNkySNGRIbVS8+eabZfzaa69Vc0OHDi3jVVddtYznzp3r5++42NVXX72Mf/e735XxSiutVB23zjrrlHFKqZrj67j+1VZbreP7/vGPf6x+5/v5ennOP/3pTx3Pyf+g3/nOd1Zz3Dueb+WV60vNuddff72a4+duX6/+8Morr3Sc4zXj+bq917vf/e5qjmvm/fHnP/+5Oo6fxe+rN954o4y5b6usskp13Dve8Y4yfvnll9UJvh///d//Xcb+2Qjuh3/Bco2E7wdfx/eVFt1XCxYs0MKFC+sbt4WledjXl/Qsfn9O0rbdXjBs2DDtv//+khb/IC+88EIZP/jgg9Xc+9///jLeddddy/iss86qjttss806vvfBBx9cxt/5znfKmP8JSNIXv/jFMuYFkqTvfve7ZcwHdd99962O4w03efLkao7/MfzVX/1VNccb6Y477ihj3uj++6abblrN3X333WX8nve8p4zXXnvt6jjOPfHEE9UcH7JRo0apE26++eYy9odsww03LGP+B9ftvT760Y9Wc2ussUYZ//73vy/jhx56qDqO+8bPJUlPPvlkGfOh2n777avj+J/8jTfeqE7w182ePbuMf/Ob33R83V/8xV+U8YIFC6q5p59+uoz5n5/fz7zujz76aDXXvq/+67/+q+MalsZn7+9/j8V8gpTSuJTSrJTSLP/fKBAI9A5L47NvL+n0nPNerd9Pk6Sc83c6vWbIkCG5bXa6ibxw4cIynj9/fse5t4tOJmE3uHlOk3yjjTYq46eeemqp1vZWQdPdrx+/Rd3cXVq4FfSHP/yh47G0fLr9J/+Xf/mXZfzss89Wc7xHaDn4/dDNfF7eoFXRzfV6O3A3r9s+ti2pOXPm6PXXX+/XjF+ab/a7JW2aUvpASmmopCMlTVqK8wUCgeWIt+2z55zfTCl9UdJUSStJujDn/PAyW1kgEFimWJoAnXLO10u6fhmtJRAILEe8bZ/97WDVVVfNI0eOlLS430wqxOfoK9Jfc5+aEWyP9s+bN6+MGZV1/+/jH/94GTOSK9XR4WeeeaaMPbq62267dTz/z372szL2uAVjFfQFt9hii+q4Bx54oIydeuP7MQLv0fhbb7213/eS6vjGBz/4wTJ2mpL3ziabbFLNzZw5s4wZZX/++eer4xhX+MhHPlLN0Ufle91///3VcfTZ//qv/7rj+d/73veWsbMHL730Uhm/+uqr1RxpOb8WBPf0V7+qGej11luvjLvFPrbbbrsydnqQ12nOnDnVXDuedMcdd+iPf/zjMvfZA4HA/0eIhz0QaAh6asaPGDEif/nLX+57YzNhSWt5wsDuu+9exqQ33Kzceeedy/jxxx+v5miaXXDBBWW87bZ1HtDee+9dxjTVpdpE/vWvf13Ga665ZnUc3ZC/+Zu/qeZIL3kG4G233VbG48aNK2OamI677rqr4xyTgnbaaadqjolKF198cTVHN2SDDTYoY0+IIeVIM1WS1l133TJ+5JFHyvi5556rjmOyCc1sqU604nqdauOeMplHkh5+eFHMmO4EqVOpTtrxe5PuhJv4I0aMKGMmtHgy1SGHHFLGNPclafjw4f0e5xT09OnTy3iHHXao5s4//3xJ0sSJE/XCCy+EGR8INBnxsAcCDUE87IFAQ7BUPPtbxZAhQwpV1K0gYvTo0dUcKQgWOrz44ovVcVdeeWUZ0w+SpMcee6yMP/GJT5SxV9jRP/OCnI033riMSbfR75RqX3PWrFnqBPf1t9pqqzImvXTPPfdUx5122mllfO+993Y8B31qUo9S7ZfS75Tqwozbb7+9jD/1qU9VxzEm4MUppC1JhzklymKg66+vUzZ43eljT5w4sTrufe97XxmTHvXfee90oz09RsIiGY85MObF9FaPPzDO4HGFzTffvIy5j9x7Sdpzzz3L+L777qvm2u/tn4uIb/ZAoCGIhz0QaAh6asbPnz+/mLikZqQ6M8nNc1JeNDmdeqNp7dQK644/8IEPlLHTOHxvCmVINU1HE5/nlmqXhLSTVFMmXi9P2uXnP/95v+uVagrG18iaamYKuhnP/aYZKUlXXXVVGdMM9uq+adOmlbHXvdMVoOnrGX+kSGmmStI111xTxnTD3EVjNtx//ud/VnP+2dr4/ve/X/1+wAEHlDE1AaTaPP/kJz9Zzf3yl78s4w996ENl7CY41+Gu1y677FLGpHs9044ZdE5Pt7MPJ03qXIsW3+yBQEMQD3sg0BD01IwfNmxYSfSnnptUSwExa0uqzVGavm4ir7XWWmVMySSpjuyyOIKvkWpz12WSaGJRDsqLXVgE4ib4b3/7247vPWXKlDJmpN5NUZrqXvhB858FFu7y7LjjjmX8ve99r5rjOVl046wD1+9FSczY+8lPflLGHo3n53Txii233LKMyUh4ViIz11jIJEkf/vCH+z3HOeecUx3HLEIWvkj15/RCHurw8b5iAZFUu15HHXVUNceiGbI3rhvIbEmXwPL36w/xzR4INATxsAcCDUE87IFAQ9DTqreNNtoof+Mb35C0OK1AasV9Jgov0E93IQFmH7nYIn1Dvjez3aSa1nF/mxQbfXb34xhXcIqRa/zRj35UzZ144ollTErNhRZIBTl1yKpArsuz9Rg7IH0k1RlkrCR06WtWok2dOrWaYybiNttsU8Yu6sDst24Ci7xPmcko1b64xwRI9TGGsccee1THca/oX0vScccdV8YzZsyo5vh+vHdIw0n1NTzzzDOrOe4r4z9+77DizmMk7ZjXzJkz9dJLL0XVWyDQZMTDHgg0BD2l3l577bVC5XjxPTOu3EQhTUc6zLt/MNvLaQuaRMwEcwqDlJQXPTBria6GC2BQlMIFMFh4c8wxx1RzzN4jDeXUG81sd0O45s997nNl7Fl+NB0pxCFJY8aMKeNbbrmljL3IgtmMpLgk6Re/+EUZd6PX6HY4hUnTl4UlLEyRalrVhSEOOuigMuY9RgETPycFTKTa5fF9ZIciio+4K8rr7l1lCNKb7k7QPfRnpN3xyPX5iPhmDwQagnjYA4GGIB72QKAh6Cn1tskmm+R251XSNlKdsrn++utXc6xgY3qit0MmNeHVSfR36FM7rUWfzCvFSK0wJuCVXGz56xVaTKX1XmmklG666aYyZjWcr59CHFKdUrnXXnuVsWvx0++n3r4k7bPPPmU8fvz4MiaFJtWCDx474DVkjMT7obW7+vrapdrXJw3nfjN9dk+XZYyAdOnZZ5/d8RyeJk1NfP+cTG/ldXGqk2B/O6mOPdEX97gTY1eebt6u+LzmmmvevuBkSunClNK8lNJD+NuaKaXpKaXZrZ9rdDtHIBAYfAzEjL9Y0mj726mSZuScN5U0o/V7IBBYgbFE6i3nfEtKaSP784GSdm6Nx0uaKemUgbxh22x2XbUjjzyyjC+66KJqjiY4M+NcO42Zdq7vTfOZIhfuxtB0d1OJQgvUEfOsLZqOXm1Gt8HdFdI11GHvRiO6eU6XgrSZU140Jd0VoNnNdkSsPpRql8o116jXzmwyd9+uvvrqMvaMMV5PZvmxGk6qzW53EzjHe8A/M+k1p1xpkvse0Oxm1qNXbtK183ZbzCokvelaeO3WadLiOv3tjEBec8fbDdCtm3OeI0mtn+ss4fhAIDDIWO7R+JTSuJTSrJTSLP7vFggEeou3m0E3N6U0POc8J6U0XNK8TgfmnM+XdL4kbbjhhrltGnvRA01HL0ChOEG3IhaaPcyWkupI8p133lnGLsnbTZ+ORRuUwnbTlCZntw6p7obQfGTLKzeRmWnnkWmamcwo9HUQfg5qy9EM9pZaNCXdBKfABE31/fbbrzpus802K2PPoKNLxWvrRSbM1nNNPnY75fm8tVK3LD8Wybj0ODPvGFV3qXTet972i24qWQG/ZrwfKRkuLWJN2MbK8Xa/2SdJGtsaj5V03ds8TyAQ6BEGQr1dJukOSR9MKT2XUjpO0hmS9kgpzZa0R+v3QCCwAmMg0fijOkzt1uHvgUBgBURPq95effXVoqftVTv0VVycgH4YfV6veqOf7v4OaRen1Aj6kO7/kc6jH+3vxWo2xhukOjZBIUaprtBidiCFPaSasnN/m4Ie9DWdTuKcZ3Rx/6lp7hmLjB14SyPuD+lHp64o5uGVYvSrGS/xz8zYhAtxMPOOMQH3eZlF6J+Fe+UZeryXOEfRD0k677zzypj3ulRTjKSCnUbk3pE+lhb56k7FEpEbHwg0BPGwBwINQU/NeMLpNeqxOT3DYgPqkXuLJ5qOEyZMqObY3ueGG24o42OPPbY6jsUYruvOQgrSOK7NRlPdM/ko8rD11ltXczTbSNXQBZFq2tLFN9ptgKTaPHQd/bbYgbS4SMIRRxxRxmxj5NmGpMBch4/u0Mc+9rEydjOTpim13iTp3/7t38qY1Olll11WHUe3ZvLkydXcpz/96TKmm0f6Vaq18plNJ9VuA4uEpPqakbL0VlZ0G1wshIU2l1xySRm7e8jr7i5mmzr0bEsivtkDgYYgHvZAoCGIhz0QaAh63rK5nSro9An9QadnSGOwVbKnBh566KFl7FQTj2XlmQs30N/2lEqmXjKddeLEidVxpGM8pZe0lscc6Pd6xRPBvfPqKlJP9F9dp5/HeVowWwoztdNTnEmfeoppuz+AVPvDLthBGootmqWabvvxj39cxk8++WR1HCnMAw88sJpj+2XGS1yYkvvjoo2MM3i1JtfP+IanOLOyzWNBrFTkPec+Oz+Lpye3r2E3MZr4Zg8EGoJ42AOBhqCnGnQppfJmbubQdHczngX9zLLqpjPuWXjMOKIZ6OISbH3rNBFpLs65iUxz112NbhlOrH6iae10Cj+na7lzT3icV2vxnJ6NxXuC2WR+HN0aXwer3pjF5pVz3A93SWjy063xTDu6Q15GzTUSpFGlmiL1rEe+n2e/8XMzo9DvYYpl+D3RSbzC95vmvu93G6+//roWLlwY7Z8CgSYjHvZAoCHoaTR+5ZVXLpk+rttGs9LNtE6mr3dq5Tk9kkkzlgU0bsazqMIj4jwnzSiPrhLMaJPqSL1Ht7kWRmX989M875YxxTlfI81dj27TpOV+0ByX6ixIdwc7mZlsbyTV5q6LefBzuqtE0CX0PeXecY10maT63vEiLcLdBB7L+8NNcO6dr5EuJ9kPlyjnc+FucPv9nBUh4ps9EGgI4mEPBBqCeNgDgYagp9TbWmutldvtcF10r5uvTNAXcj+OlWhOkdBnp7ggtc+lWpPcK8roA9Ov9bZIrFJjtZ0k/exnPytjz6Bj9Rb3oJu2vcc3WA1FCskrCUmBeWUeq8OYxeYinvR7Pa5AQQxmBro4J/fYYwL02ennOo1IUUzSWFJ9j1B8w2M6XL/Hcfg6j32QRqMf7dQbz+97QOHOdktzqd57qb4nPObVrtSbNWuWXn755aDeAoEmIx72QKAh6KkZv+qqq+Z2Cxs32dwsITppmLm5RRPf5wYKCj64acqiFmaWddO0czArzAUISFd5EQ5B091pHHdL2nC9vm46dhSeeOSRRzqul+a/d8NlJh+vi1NydI18jp+T5/cMN7qA7pLw/ub5fb2k/Vy8gveB7yNdCr6X6/rR9fL7ip+Tn81dNK7Zs0fbuP/++/XKK6+EGR8INBnxsAcCDUE87IFAQ9DTdNlhw4Zp22237XtjS/MkZeJ+F/0fpnZ6yiP9aE8bJNUy0Ioyr5iiLjhTdSnOINX+lMcmmF5JkQip7hvGdfk66FM6/ci10OfzGAB9SF8/j2X1mldr0e93kQ6+9+abb17GLkzJa+10Kc/J60Ite6n2m52WI83KPXVNdsaM3B+msKb7+qQOKSpJilWq4wWHH354NTdp0qQyZno1902q+xKyfbO0iB70HnPEQNo/jUgp3ZRSeiSl9HBK6aTW39dMKU1PKc1u/VxjSecKBAKDh4GY8W9K+krOeTNJ20n6Qkppc0mnSpqRc95U0ozW74FAYAXFW6beUkrXSfpB69/OaNs8M+f8wW6vXX/99fMJJ5wgqc4akmqqzOeY7cVKIGYbSTUF8+ijj1Zz1OxiVpvTOKSy/Pyf+tSnypja817h1HZVJOn666+v5mguugnOzCpq43U7zs1Ruhc0aY8//vjquAsvvFCdwCwxnt+zx6hj59QpXSWa+9wbqTZ3vUKQmYhch9Naw4cPL2O/FhSUYMait3GiiezuCt3DG2+8sZpjxiXdLc+go8vJFl1S7TYxS9EFNujCdlrjKaecoscff3zpqbeU0kaStpJ0p6R1c85zJKn1c50uLw0EAoOMAT/sKaVhkq6WdHLO+eUlHY/XjUspzUopzfLAWyAQ6B0G9LCnlN6hvgf9kpxzW+93bst8V+vnvP5em3M+P+c8Muc80qOtgUCgd1gi9Zb6OIP/K+mRnPO/YmqSpLGSzmj9vG5J51q4cGGhWrw6iVVZU6ZMqeboR9N/9T5t3dIa6ReR/vF2yEwVdRqDuuadqDyp7k3nyix8b6bmSrUvyvjDww8/XB3HPm0XX3xxNcfYB+mvW265pTqO53T/9ec//3kZ0790lRb66e5f8pqxWqubXrv7uUzP/dznPlfG/pl32223MvbKPF4b9lgj9SjV13qvvfaq5pgOPWrUqGqOPQrZ88/bW/Ne9VgQYxqMMXh8g/Eqp5bbPQq7Wc8D4dlHSTpG0oMppftbf/u6+h7yK1NKx0l6RtKYAZwrEAgMEpb4sOecb5PUv6CYtFuHvwcCgRUMPa16Gz58eG6bM27C0mwlrSVJX/va18qYrZbc7CO95tQb3QSKAnhmGc/v2VKMOWy33XZlTNNLqltU0cSUam10bzPEzC2azHQLJOnoo48uYzdb+dmY3cXWRFKdGedCDtxHuls0N6WaDvMW3Mx+Y2agZz3SnGa7bEkaM2aRscjKPKci+TrPOmPWHGmyhx56qDqOn9PbLXvLbIJtxngPOxVJSvCwww6r5i666KIy3mOPPcrYaVXSxN4mvN3vYO+999YDDzwQVW+BQJMRD3sg0BD0tBBm1VVXLR1ZXVyCZpWbizR3mS3lGm5XXXVVGbuuNrOPJkyYUMbf+ta3quNoBrpWGE02RrpdT2+TTTYpY2cFmIHlGnd0bdpafdLiJhvNfTf1uHcscPGsLerm/f3f/301R3OXa3K3g2ar7xW7nbIYxbvrMlvyoIMOquYoxEFXZvTo0dVxrjdPsCsv7wlnOFhc48U63FOPstM8p4vigiYnn3xyGXsn2K233rqMqad38803V8dRSMS7uLYzLv1+IOKbPRBoCOJhDwQagnjYA4GGoKc++5/+9KfiW7ifS5rF/dxOQnveC4uZWl5BRZ+S2VhOa5GqcWqPviF9I6e/6Mu6j0rqjT6Y1LnyzwUwSJeS5pPqmAYpNe9fRjrpwx/+cDVHf5Pn8DbY9D29FTMrC6dOndrxHKwKdFEHxll4PV30g1mQhx56aDVHf5t0psd0mOnoNCUz79xXZpYf4zguOML7wCvzeM+dfvrpZewUIGM3t912WzXXpvq8/yER3+yBQEMQD3sg0BD0NINuvfXWy2PHjpUk7b///tUcTSzPfqNoAmkR19+mCePmFs3HbmYZM+3++Z//uZojRcIMKS9KoHvh9MkRRxxRxm7O0RylvryLedBcdK110pG/+MUvytg/J81KL7ig6AWPoxabVGeueQssFnvQ5XEXja6Su010X/i66dOnV8eddNJJZezuCqkyCj44vcb93mmnnao5mvWuy89rw/d2F5PFNczSlGr3kHvAe0WqaVvPemwXHv3t3/6tHnvsscigCwSajHjYA4GGIB72QKAh6Cn1ttpqq2mfffaRtDh1QN/H/SL6g6SFXDSCr/NqM/rO7TVI0rRp06rjzjrrrDJ2+o6pl6TXvAcaxSIdTIf0airSQaykcy10+nje1peVUazoc4HCdtqytDjVdOCBB/a7Dk8Bpa/vVCpFFOhfuq/MdFynq9qVXFJ3cQlW1Xm6KH19xqc++9nPVscxXdupVNK2FJ+UOtOUngrN2IfvI1OZSR9/97vfrY5j3MWrB9vxqm7iFfHNHgg0BPGwBwINQU/N+FdffVV33nmnpMV1uEjrOAXDCiWaOS4aQZOWpp2fkxSJV2t1a0NMuo0ZY06D8L3cBGc2mbsJl112WRlTHMOzokgdOsVDE5R76uYd6Ttqzkn152TGn7srpJBcNIIuCbP8tt9+++o4UofMmPM10tz3ikl+Zq+EJE3H6+KZk3RDdthhh2qO9xXdPKn+PHQj/bpcd90iiUa/FlwL57j3Uk0PelZlu/WUm/dEfLMHAg1BPOyBQEPQUzP+jTfeKBFtCiRIdUGEyy+zIIBmvAsy0Bz1LDwWGzCzjJLHUm3+eydOmt00aV3PjN08XXiCMtkefaZ7wXO6aUqz0rX86HowEk0BCamWxfbMOIpvkDXx7LRDDjmkjGliStK1115bxozu0wWRatbEteXOPffcMmbk390f6t25/DfXzO637rowUu9uGTM1fR+pXUe3w9uK8X78+te/Xs3xWvBas7urVEt+H3XUUdVc+7O5i0PEN3sg0BDEwx4INATxsAcCDUFPffYhQ4YUf4g+tFT7r54Fte+++5YxfX2v2CPtwHbCUu1D0e/6xCc+UR3XpgalxSu0qHFO/8wFD5kh5fRdtywo+r1cr2dccX9cAJH+NqvSPPOLlBGFOqU6jkEK6e67766OIyXo7ZypI8/3ctFKVju6P89WXN2y05h9+cwzz1RzFK9gFtvIkSOr4xib8L1iRqTTj/Sx6b97OyzSmR7f4H1ASs1jV/wsLhZy9tlnS1r8XiGW+M2eUnpXSumulNIDKaWHU0rfbP19zZTS9JTS7NbPNZZ0rkAgMHgYiBn/uqRdc84flbSlpNEppe0knSppRs55U0kzWr8HAoEVFAPp9ZYlte3Gd7T+ZUkHStq59ffxkmZKOqXbuYYOHVpMEzdhaX66SfjjH/+4jKmX5oUwLDJxUQfSYaTefvrTn3ZcrxePsDCDVOE//uM/VsexXRW126XaDbnpppuqOYpjkI6hhry/zl0ZthIaN25cGbveHdfFzyXVmWbUtHPTl/vj2Yw0R//hH/6hjD2zjC6JU3tst8WWYCxC8te5xh0743Zau1TfO14cRXrTzWea9TyHZ4h+4QtfKGPXv6N7xAw6p1xJq7n2YPuZWWrqLaW0UquD6zxJ03POd0paN+c8R5JaP9fpcopAIDDIGNDDnnNekHPeUtIGkrZJKX1kCS8pSCmNSynNSinN6tatIhAILF+8Jeot5/wH9ZnroyXNTSkNl6TWz3kdXnN+znlkznmkZyYFAoHeYYk+e0rpfZLm55z/kFJ6t6TdJZ0paZKksZLOaP28rvNZ+rBgwYKS7uo+NdP/fvKTn1RzTKOkgIRXD9Ff85a5pFbo87kQI4UyrrjiimqOfihbHp9zzjnVcUzvdX+Y8Dn6g6RZXOiDvc6cauJeUbDDj2MLZK8yZCVgt/5l9L9Jofk5SCOecMIJ1XH0xT0VlYKcjLmceOKJ1XHnnXdeGbsvTp+dwhlOr1Hs01O5GddhS2Wp/pzcb6dtZ8yYUcZ+33KPScsxZiHVVKf3qmunLnusgBgIzz5c0viU0krqswSuzDlPSSndIenKlNJxkp6RNKbbSQKBwOBiINH4X0raqp+/vyhpt8VfEQgEVkT0NIPuzTffLBVKrFSSahPOqQlmodG08aqxUaNGlfE3v/nNao5UE81Wp/nGjx9fxp4tRfqKlVwei2DFmmu+M7PPBTZognLu6KOPro6jCc6sLWlxcYg2fL9JqblJywozVv65zj3Nc8/2YvUZq+Oc1qLZ6q26WSnGjEtmBkr1PeGuBjP2eK3pDkp1FqSbz+xx4AIbFKxgRaZXO5Lu5b0jSVdffXUZ855z6o1unwtbtN0Vr+IkIjc+EGgI4mEPBBqCnprxQ4cOLaaJdw6ljphHQ2nO0ZRx3TOew81ZmqqMlHr00otaCIomMLrtkX/KNNO0k+oItotG8JzM2rrkkkuq49ie6JOf/GQ1R7OSxTquY8fjfG7y5MllzIIO123j+d0Ep1lP8Qe6D1J9LTzCzGvBiLVLjXMdLq3Ne4muhWf8cY0ui817x81kmtPdirl473ghDOfoynkxDaXHDzvssGquLcYRXVwDgUA87IFAUxAPeyDQEPTUZ08pFf+KGUtSTWl4Zhkru1gh5BU+zOLyFj70genzeeUc6TXXlCcFSHrQK7lIDbGaT6ozum655ZZqjpl9rORiNZxU+6hOqdFnox997733VseR6nQ/lNmNvE7eIpv+N8UqpPo6UTTCYyKkRF0Ekj42/WY/RzdxTvrfpCldd53Cl6yylOpYiu8Vf6fWv6+R/QJ23XXXao60HK+nxxVYVceMPGmRfx+68YFAIB72QKAp6KkZv8oqq5RCE6e8aBK6eU6znnOuiUYz0ykYmoHU4/ZMJNIdXiDC83P9XmTCDCw342maOYXELDG6F/5ZSN04PXPjjTeWMbXa/Lhu7ZSYGUeNvgkTJlTHMcvPRTRICdLM9iy8f/qnfyrjXXbZpZrj5+b94bp7n/70p8uYn1+qNQVZ5OTZi92KWKiB6J13aTYzW88FNuh6OZVKN61bcRHpauohSot0BJ3qJeKbPRBoCOJhDwQagnjYA4GGoKc++yuvvFLoJtdTp+/j1WwLFy4sY6YhnnpqLWjLdM5nn322mrv88svLmHQPaQ+pFknwlF7SV91SKAlfx4477ljG9OOkmjZitdmYMbVUANNz77nnnmqO+0r/z2kz+p4U25DqPSAdtt9++1XHsR01BTj9dZ3WJNV+umv9jx07toxJFXoPNFa6+flJCbIC0a8LYw4uosH7ylNp58+f3+/62VdOkj7zmc+U8QUXXFDNkaplLMGFVXjdvbqvTbnyWXHEN3sg0BDEwx4INAQ9NeNfe+21YrJ87GMfq+ZIM3hlFCkqar95S2VSNV5BRd22b3/722VMOkaqzSPXGyPlxVZFO+ywQ3UczX0Xr+jWXooZdcygu+yyy6rjSO15phbNSpp0nlFIF8W17bl3zDRzt4MCEF6pSErz1ltvLWOn6FjRyHZPUk1vkm5zM5t0k1N7vHd4f7iOfqc21VJ9Ldz9JO3Kz+yCIMyuc/qOrgHX7y4J+wV45mdbezBaNgcCgXjYA4GmILlZtTzx/ve/Px933HGSFi9YYITVzZfPfvazZcwsM4+o0qz0aCXNG5rP3lqJUVSPDlOqmllmrmNHE8s7k1LO2D8nGQm6K3wvqY7Yuk4ez0nz0LXTaI5SbEOqizEoTe3y33QhvKiHYAGH79XEiRPLePfdd6/maO4yMu0R8YMOOqiMzz333GqO0Xi+7vOf/3x1HHXg+Jn9d/+cLDaiCIW7CXS9XEqa56fbRxdHqt0Ld2XabtQ555yj5557rr5QLcQ3eyDQEMTDHgg0BPGwBwINQU+pt5VXXrn4Xk5hHHHEEWXsRfv0N1mJ1vb/2zj99NPL2KvNOvnbHrOgX+f0Cf0uxgC8Ku266xZ1wvLsOmZSuXY5qTL6cd2q0ty/pP/N1kpODzKm4QIYjDNQi58Zc1JNO/E4qabv6L8yO0/qTjHyGvIze3YaaTmnS6np/4Mf/KCM/bqTVqTYhlRTrn49+d6MMTDmItXxHxcaJTXJe86PI33nVGp7zcskg67Vtvm+lNKU1u9rppSmp5Rmt36usaRzBAKBwcNbMeNPksQQ46mSZuScN5U0o/V7IBBYQTEgMz6ltIGkfSV9W9KXW38+UNLOrfF49bVyPqXbeebPn1+oJ29bxHZNZ555ZjXH4heatE7jUE+dLZKkuoiAbgEz2vz8Tg8yO41Zct6uitlk7gqwyMcFFGiaUUvOKToKUfg5WIDC9bqYAue88IimL81dz0okHeZUEOlNvrd36KUwievYHXnkkWX81a9+tYzdJaFmnPcLYJEJsza7tQ6jWS3VLidNaakWAbnyyivLmBSxVLufTh0ef/zxZXzppZeW8YMPPlgdR6rThU/azxWvq2Og3+zfl/Q1SXQI1s05z5Gk1s91+nldIBBYQbDEhz2ltJ+keTnne5Z0bIfXj0spzUopzfIATyAQ6B0GYsaPknRASmkfSe+S9N6U0gRJc1NKw3POc1JKwyXN6+/FOefzJZ0vScOHD+9dul4gEKgwkP7sp0k6TZJSSjtL+mrO+eiU0v+RNFbSGa2f13U6RxvvfOc7i276yJEjqzn6Vu4DM4XwmGOOKWMXbiC9QX12qfa76Pu4L8teWy7eRx15noO0nlRTdN5emKm0rktPP5e+oMc3WBnlMQeKF5Je87Ra+pD8zFKd9sk97eYPeptjxhI4562MSR365zzllEUhIN4Dro3O/XdfnO/HuI37zfTnvery9ttvL2OPK5Cy4zUjpSjVNKK3t2Z8ife+p1qTSqUIprToOnVLf1+apJozJO2RUpotaY/W74FAYAXFW0qqyTnPVF/UXTnnFyXt1u34QCCw4mDQ2j+5icLCf9c4pzlNXXA391mV5frhPCdNKjfLaKq6qUe9uk56cVJtSnllHt/bTWu2oKZ57q2KKDzh2vak+rheFzug+7LFFltUc1dccUUZU//d9dqZQedUEM1p7qlnv5FidLeG7gRdHKdV+Zl9jvcVWzC5K0B6001hZuVde+211dzHP/7xMqbL5q4ATXx/72233baMqRXo1XGsenNXYCCI3PhAoCGIhz0QaAh6asYPGTKkRGY9K4xmvWeFsSCABQbe/ommkhdmTJ48ud81uVy0m7sEi0zYfoctkqQ6o+tb3/pWx3VQ+ECqTXKODz744Oo4ms/ezZMRbXcTCJqOU6ZMqebY0ZTvRZNbqrPVvAXW448/XsZkRijeIdUCDZ6dxszJbhqF3CsvQKFrR3OfrZqkWu/OGRp2snUBD95zvA/Yekuq94MZilLnlmDu1pDB8uKo9t75/UDEN3sg0BDEwx4INATxsAcCDUFPffaXX35Z06dPl7R4lRQr1jzrjC1/6ee6qCR9e/et6E/RZ3JRSQoPfuUrX6nmKK5w1llnlbHTfKTz7rjjjmpu//33L+P2XrTBLDFShZ4VxuozF2tgtRWpMqd7KAJ53nnnVXOMmXAf3VemWIPHLehjM8uMVJVU+5ieiUjRCFJvF154YXUcBUmdeuM5STHSD5dqcQyvWGM84qSTTqrmKPzBCj6P/fC9fY41I0888UQZe3Uf4wCMq0iLPreLwhDxzR4INATxsAcCDUFPzfihQ4eWjCZSHVJdOOCZcTSxSE141hbNaaf2SDXRLXCBCgpPeEsjUjf/8R//UcbetogZb97+iWarF4/QpaC74ppo3B9vd8Rsw05msCR973vfK2N2lpVq05RmMTPapJp28uvJzEHusWvgk+byjrr83KSaPGuQXVadruLvzHR03T3uGyk/qRY7oWiJVOvIk5rtVrzk1B6zCplB5/c3TXy/d9pddF2/kYhv9kCgIYiHPRBoCOJhDwQagp5XvbV9LxeXII3mIo2HHnpoGZOW81bDTIF0f4e+DOkY988YO6AvKNXpnPStnF4j1eR+6NixY8uYPrVUp7fSt/W2zNSb9+pB+nJMr3SqiTEN97e5/7xO3i+OwhPe+ppa+qQ6PXbA6j4/B4/lObxP27/8y7+UMeMIUk0BMsV5n332qY5jfMDTgnktPF7A+4XncL+c63JRFF4z3mN+zRgz4f0nSWPGjJG0+P1AxDd7INAQxMMeCDQEPW3ZPGLEiHzyySdLWlz3jG11nK5ipdTo0aPL2M0tfhYKFUjSnnvuWcY0R92EZdYczT5p8Yy3Nlz8gaae02Zco5+fFXjMmnNVXtJy3o6IAhg0P72FMKkxz9TiOeiGeLsqF1cgaOKTkvI9JG15+eWXV3PMyqPZSp09qTZd3S3rpOv+d3/3d9VxzOB02owZdW7Gk9oj9esZoqzu86o3gnvqrig/m7eGOvrooyX1Ucz33HNPtGwOBJqMeNgDgYagp9H4BQsWLKZj1gYjlC5ZzKwzRofvu+++6jia/91kj2nG+3HTpk0rY4+WM4rP4gg3s8kK7LLLLtXczTff3HFdjMaz2MULYQ455JB+j5PqTqWHHXZYGXumIDvNusgFI8ncb48A8zjfA2aMUYrZtdNY3OGZiHSxOOfvRZfBBUHIlNB8dpEHypK78AQ7z7qbQE1BZtO5q0FRDRfOYMYli2n8uvM4Z4Da7paLiBDxzR4INATxsAcCDUE87IFAQ9BT6m3jjTfObQFGVvBINb3m2XX0tUgFOWVEH88F+ZilxOwuz1KiuMLChQurOdIijD14BRLXsddee1VzbHPs7atY/cR21MwCk2o/nRl5Uk0TsbKrWxzERSO4/xTicJGOXXfdtYxJa0l1qyV+ZraUluq2zE7tcc2kJakFL9ViEC6eSWqMMQantZhR6NeddKH3GeC1YQspXyOzMX2/eQ6KhZCuk2oBEqfv2lmb5513nn7729/2S70NtD/7U5JekbRA0ps555EppTUlXSFpI0lPSTo85/xSp3MEAoHBxVsx43fJOW+Zc24nXJ8qaUbOeVNJM1q/BwKBFRRLQ70dKGnn1ni8+nrAndLpYKnPLGvTCS4uQfPTqSBme9E8ZMGJVJufTm/QnGNRjHcVpanKrCqp1maj+e+aZcxAc5106tl7BiA/D4UsSMdIdcGIZ+jRPOVeuagDqSyuV6rdBLpNzIRz+DVjhhddKhfAIJ3ktBZf160zLt05dwFpTpMac1N60qRJZez70S4ykRbfb14LuqakA6WainR9RFJsLP5xqpPuhReLtTNSl0UX1yxpWkrpnpTSuNbf1s05z2m9wRxJ63R8dSAQGHQM9Jt9VM75+ZTSOpKmp5QeXeIrWmj95zBOWrzsLxAI9A4D+mbPOT/f+jlP0rWStpE0N6U0XJJaP+d1eO35OeeROeeRHrUOBAK9wxK/2VNKq0gaknN+pTXeU9L/ljRJ0lhJZ7R+Xtf5LH1YuHBh8VP322+/au5HP/pRGXs1GNMXKRbgKYndKJKf/vSnZUw/3auYWEXmvjhb5tJHdd+bKYsem6DePD+LVKfn0uf19FBWVLlYAf10/ufqMQzqiz/44IPVHIVEmObpqZhPP/10GXvsg3t1wAEHlLFbdx536bQOHkfBC6mm3qiHL9UtlrlGFyY54ogjytgpL1Jj7iuTDqPP7qm/vE6e0ssYEj+nxzcomOm0c9u/d8qZGIgZv66ka1tBhZUlXZpzviGldLekK1NKx0l6RtKYLucIBAKDjCU+7DnnJyR9tJ+/vyhpt8VfEQgEVkT0NINunXXWye1KLG+tSzPHta9pKjGzzNs/cY7tk6TaFCPV0dbbbuP4448vY9dco4k1d+7cMnaahRSVi2PQJPcsKJqBbO/jYgqke6iBL9VtlEkPeiUX3RwXtiD1RCrP6TW6L26OkyZiZpnfbxQxcbdpwoQJZczKMz+OFZOuQcd18T7yrEReC9eGpziJV5txr5gB6GvkObwvAuk80m2sqJPqa+gVme39vuSSSzR37twQrwgEmox42AOBhiAe9kCgIeipUs0bb7xRUl+ZAivVPpmnZdI/YcojU0+l2g9zOowxghNPPLGMnQah+o1TTaQL6Qt6/7JuFWX09V1DneD+uH82fvz4MvZ4AT8P98Nb+dJvdBqH1CepQ2/7zAo2/5yk2OhrepUhfVTvrcdUV1JKHh/gZ/E4C+M/J5xwQhk73Uja1vvnsTrRYwL0zbkOp3RvvfXWMuY9INWxJsYwXNWH19YFW9v+vVO9RHyzBwINQTzsgUBD0FPqbZNNNsntVj3eAmf48OFl7O10adLSHHVziBVUTm9QsILms1NjNDk9vZcmEjOk2GZJkm666aYyJu0k1Walm8UzZ87s9739GlE80s9Pl4JZc56Fx3N6Vhur+1i15xQdaT4XR+Re0aSl7r9UZ8O51jp/Jy3Je8XhLhWvO/eU10iqszGdjuV++7XgNWR2nVe2kba89NJLO66R19MzSem+eEVcm4a+5ppr9MILLwT1Fgg0GfGwBwINQU+j8X/+859LFNSjyDRv3Yziscwq8kKY2bNnl7G7AjSr2MHTs9NYEOGRXQpAsGDGzXH+Pnny5Gpuxx13LON///d/r+b233//MuZndhOZLoSzCTRV77777jJ2TTTuD6PBUm0+ct/ImEh1IYy7TZ791QaZBKlu8eQMCoUzuN8uCMKotbekYtSaroW7NXQrXbSErh4LjXxdzOh0BoUdh525oInP9XtxUbf7O6LxgUCgIB72QKAhiIc9EGgIeuqzz58/v/hbXhXEyh/39+hX06fxyjb6q95TjmKG9KOddiJ145lUpP2o8e7ZTKRn2G9NqqvIxo0bV81RUIHv7YIEFGTwyjkXp2zD9/vxxx8vY6eymKXIvnKHH354dRz9b89q4+fkvrlvzyoyFxxhfINUmVc7jhgxooy9YpL3FbMoXc+f6+Xnl+pMTb8WFLpgDGPbbbetjmNMgNSmVNNo9Oc9rsA1OuXafi48U5KIb/ZAoCGIhz0QaAh6mkG39tpr53333VdSne0m1XSP0xs0JWnC+Tlowvgcs+2Y0eVZeDSLXW+Mx9LkdOqNx7nZypbNXvhBk7Bb+ye6Oe6GdMo2dJeHNJdrrvF1pH/czO7mynD9N9xwQxmzjZNU75VnhbFAh3MuIEGtNi9UYUYkaTLP4OTn9HWwDZW3qGKWH81/aghK9T3tWX7cK15bpxj5Xn7N2vft1KlT9eKLL0YGXSDQZMTDHgg0BPGwBwINQU+pt9VXX10HH3ywpMX90GnTppXxdtttV81Rg5y+PakZqfaxp06dWs0xxZT+KtsJ+3Hu97fXLtUiF6RcpNrHY58wqaZT3L9k2iSpMophSLXwggsccA9I5bkgCAU+na7plALqqcV77rlnGXvsh2mbTGt20Uqu3wUlSKMxNuHULP1ov56sSCTF+MMf/rDjej1tt1sPQdKnvJ6kNqV6/13Ygj47e/w51Xn55ZeXscc+2tr5TrES8c0eCDQE8bAHAg1BT834BQsWFPPd9d0+85nPlLFnMNE0ZeWZmzI0CZ3eoGlNqsYFKmgqOQVI/TRmYzmtdfvtt5exm7c0M7kmn2OWlbsrdBtcP47uC81nZpn5mkkHSnX2F/fRz0E3x81b6t6TMvKKL2ZErrfeetUcK/8o7PH5z39eneDrIJXKrDOvRmSlmwuasNrMTWsKoXD8pS99qTqO97u3pma257HHHlvGFOyQajeBghfSIpfNaUNiQN/sKaXVU0pXpZQeTSk9klLaPqW0ZkppekppdutntGgNBFZgDNSMP1vSDTnnD6mvFdQjkk6VNCPnvKmkGa3fA4HACoolZtCllN4r6QFJG2ccnFJ6TNLOOec5rZbNM3PO/VdhtLDaaqvldlGBR7oZAfZoJTPUOOeRXR7nn4smM+dc94wmm5+DxSnUJWOxhVSbc65FRrPSBQ4YSWV02F0NrsvNNp6Te+qfk0UVXvDDvWKBixe70AT3LELOMYvQWRia6m7e0mzl+fi5pHqP/b7i67iPfu9wP9y9YradXwu+H9flDAdZGJcQ5zXkHvs6eJzPtd/7rrvu0ssvv/y2M+g2lvSCpItSSvellC5otW5eN+c8R5JaP9fpdpJAIDC4GMjDvrKkrSX9MOe8laRX9RZM9pTSuJTSrJTSLP/GDgQCvcNAHvbnJD2Xc25r/l6lvod/bst8V+vnvP5enHM+P+c8Muc80k29QCDQOwykP/vvUkrPppQ+mHN+TH092X/V+jdW0hmtn9cN4FzF53bRBdJc7o+QkqF/3M4a6u93Fw0kvcT/dFwLneIPLpJA35P+pftxzMDyqjSu0efoX3IP3LeneKS3YnZ/tr+1S3VFlb+GsQOu1+lS+tt+LXh9GRNwf5WfzeMbrKSjVejvxfgJWzVJnffDs/D42Vy0kTSlC6WSpuN7+T1MwUkXiyS4Vx5XIEXnwprtvfM9JAbKs39J0iUppaGSnpB0rPqsgitTSsdJekbSmC6vDwQCg4wBPew55/sljexnard+/hYIBFZA9DSDbsiQIcW88+w0mmZuAtGcpqnk5hapCad4aGqT3vDCCboMrmdG85nCDQ6ua+HChdUc39vpMK6f9JrvVTczkO9HU9oLM3jObrpldFfcJOY5PB5DE5S0ln9mXnd3h7gu7o1fF94vfj25Rr7OMyx5zXyNpNdcvILXkya07xU/i9/fvFe5B76nXL9r/ff3PovNdZwJBAL/oxAPeyDQEMTDHgg0BD2vemvTB+5TE56m2o1O6AQX63O9705gC2F/XwpWcK5bspCnohIecyDVxAozpwDpl7nvxn2ln+v+MI/zc9A3pHCDV2GRovPryRRT+uUu8Ek4ndRt7wjGFVxIkj47/euHH364Oo6Ck92qzQYq0OpxFcZSPObAa00q0veUPryLVLTjIh4jIuKbPRBoCOJhDwQagp7qxqeUXpD0tKS1Jf1+CYf3ArGOGrGOGivCOt7qGjbMOb+vv4mePuzlTVOalXPuL0kn1hHriHUspzWEGR8INATxsAcCDcFgPeznD9L7OmIdNWIdNVaEdSyzNQyKzx4IBHqPMOMDgYagpw97Sml0SumxlNJvUko9U6NNKV2YUpqXUnoIf+u5FHZKaURK6aaWHPfDKaWTBmMtKaV3pZTuSik90FrHNwdjHVjPSi19wymDtY6U0lMppQdTSvenlGYN4jqWm2x7zx72lNJKks6VtLekzSUdlVLavEdvf7Gk0fa3wZDCflPSV3LOm0naTtIXWnvQ67W8LmnXnPNHJW0paXRKabtBWEcbJ6lPnryNwVrHLjnnLUF1DcY6lp9se865J/8kbS9pKn4/TdJpPXz/jSQ9hN8fkzS8NR4u6bFerQVruE7SHoO5FknvkXSvpG0HYx2SNmjdwLtKmjJY10bSU5LWtr/1dB2S3ivpSbViact6Hb0049eX9Cx+f671t8HCoEphp5Q2krSVpDsHYy0t0/l+9QmFTs99gqKDsSffl/Q1SazgGIx1ZEnTUkr3pJTarVl7vY7lKtvey4e9v9K1RlIBKaVhkq6WdHLOuXP533JEznlBznlL9X2zbpNS+sgSXrLMkVLaT9K8nPM9Szx4+WNUznlr9bmZX0gp7bSkFywHLJVs+5LQy4f9OUnsDLiBpOc7HNsLDEgKe1kjpfQO9T3ol+ScrxnMtUhSzvkPkmaqL6bR63WMknRASukpSZdL2jWlNGEQ1qGc8/Otn/MkXStpm0FYx1LJti8JvXzY75a0aUrpAy2V2iMlTerh+zsmqU8CWxqgFPbSIvUVwf9fSY/knP91sNaSUnpfSmn11vjdknaX9Giv15FzPi3nvEHOeSP13Q835pyP7vU6UkqrpJRWbY8l7SnpoV6vI+f8O0nPppTaeuZt2fZls47lHfiwQMM+kn4t6XFJ3+jh+14maY6k+er73/M4SWupLzA0u/VzzR6sYwf1uS6/lHR/698+vV6LpC0k3ddax0OS/lfr7z3fE6xpZy0K0PV6PzZWXz/DByQ93L43B+ke2VLSrNa1mShpjWW1jsigCwQagsigCwQagnjYA4GGIB72QKAhiIc9EGgI4mEPBBqCeNgDgYYgHvZAoCGIhz0QaAj+H4eWYk2xyVd1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = 7099+10\n",
    "plt.imshow(np.array(imgs[z]*255).astype(np.uint8), cmap=\"gray\")\n",
    "print(\"Ransomware family: \",list(prelim_dataset.class_indices)[np.argmax(labels[z])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76617a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adialer.C': 0,\n",
       " 'Agent.FYI': 1,\n",
       " 'Allaple.A': 2,\n",
       " 'Allaple.L': 3,\n",
       " 'Alueron.gen!J': 4,\n",
       " 'Autorun.K': 5,\n",
       " 'C2LOP.P': 6,\n",
       " 'C2LOP.gen!g': 7,\n",
       " 'Dialplatform.B': 8,\n",
       " 'Dontovo.A': 9,\n",
       " 'Fakerean': 10,\n",
       " 'Instantaccess': 11,\n",
       " 'Lolyda.AA1': 12,\n",
       " 'Lolyda.AA2': 13,\n",
       " 'Lolyda.AA3': 14,\n",
       " 'Lolyda.AT': 15,\n",
       " 'Malex.gen!J': 16,\n",
       " 'Obfuscator.AD': 17,\n",
       " 'Rbot!gen': 18,\n",
       " 'Skintrim.N': 19,\n",
       " 'Swizzor.gen!E': 20,\n",
       " 'Swizzor.gen!I': 21,\n",
       " 'VB.AT': 22,\n",
       " 'Wintrim.BX': 23,\n",
       " 'Yuner.A': 24}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe2ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 26\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        15040     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 236,609\n",
      "Trainable params: 236,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 9792)              1507968   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 9792)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 153)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       313472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 2,352,257\n",
      "Trainable params: 2,352,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70546",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf8cba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c67",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af608a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b649da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'{new_dir}'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "146/146 [==============================] - 22s 122ms/step - g_loss: 1.1760 - d_loss: 0.6337\n",
      "Epoch 2/80\n",
      "146/146 [==============================] - 17s 117ms/step - g_loss: 2.0089 - d_loss: 0.5668\n",
      "Epoch 3/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 5.9220 - d_loss: 0.0371\n",
      "Epoch 4/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 2.1409 - d_loss: 0.5117\n",
      "Epoch 5/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 4.0696 - d_loss: 0.1339\n",
      "Epoch 6/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 2.5782 - d_loss: 0.5469\n",
      "Epoch 7/80\n",
      "146/146 [==============================] - 17s 118ms/step - g_loss: 1.3216 - d_loss: 0.7214\n",
      "Epoch 8/80\n",
      "146/146 [==============================] - 17s 118ms/step - g_loss: 0.8844 - d_loss: 0.6774\n",
      "Epoch 9/80\n",
      "146/146 [==============================] - 17s 118ms/step - g_loss: 0.8962 - d_loss: 0.6587\n",
      "Epoch 10/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7404 - d_loss: 0.7352\n",
      "Epoch 11/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.9267 - d_loss: 0.6100\n",
      "Epoch 12/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.9176 - d_loss: 0.6542\n",
      "Epoch 13/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 1.6618 - d_loss: 0.5126\n",
      "Epoch 14/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7950 - d_loss: 0.8167\n",
      "Epoch 15/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7617 - d_loss: 0.6863\n",
      "Epoch 16/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.9509 - d_loss: 0.6021\n",
      "Epoch 17/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.9026 - d_loss: 0.6706\n",
      "Epoch 18/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.9331 - d_loss: 0.7077\n",
      "Epoch 19/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 1.4012 - d_loss: 0.5892\n",
      "Epoch 20/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8529 - d_loss: 0.7396\n",
      "Epoch 21/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 0.7616 - d_loss: 0.6792\n",
      "Epoch 22/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 0.8088 - d_loss: 0.6894\n",
      "Epoch 23/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8410 - d_loss: 0.6573\n",
      "Epoch 24/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8469 - d_loss: 0.6579\n",
      "Epoch 25/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7867 - d_loss: 0.6922\n",
      "Epoch 26/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7940 - d_loss: 0.6884\n",
      "Epoch 27/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7808 - d_loss: 0.6926\n",
      "Epoch 28/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.7696 - d_loss: 0.6635\n",
      "Epoch 29/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 0.8396 - d_loss: 0.6735\n",
      "Epoch 30/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 0.7890 - d_loss: 0.6732\n",
      "Epoch 31/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.8889 - d_loss: 0.6479\n",
      "Epoch 32/80\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 0.8329 - d_loss: 0.7089\n",
      "Epoch 33/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8629 - d_loss: 0.6488\n",
      "Epoch 34/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7847 - d_loss: 0.6842\n",
      "Epoch 35/80\n",
      "146/146 [==============================] - 17s 120ms/step - g_loss: 0.8620 - d_loss: 0.7093\n",
      "Epoch 36/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8503 - d_loss: 0.7832\n",
      "Epoch 37/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7716 - d_loss: 0.7031\n",
      "Epoch 38/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7528 - d_loss: 0.7199\n",
      "Epoch 39/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7939 - d_loss: 0.6775\n",
      "Epoch 40/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7605 - d_loss: 0.6846\n",
      "Epoch 41/80\n",
      "146/146 [==============================] - 17s 118ms/step - g_loss: 0.7596 - d_loss: 0.6888\n",
      "Epoch 42/80\n",
      "146/146 [==============================] - 17s 118ms/step - g_loss: 0.7788 - d_loss: 0.6735\n",
      "Epoch 43/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.7695 - d_loss: 0.6905\n",
      "Epoch 44/80\n",
      "146/146 [==============================] - 17s 117ms/step - g_loss: 0.7548 - d_loss: 0.7069\n",
      "Epoch 45/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7564 - d_loss: 0.6822\n",
      "Epoch 46/80\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 0.7571 - d_loss: 0.6927\n",
      "Epoch 47/80\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 0.7404 - d_loss: 0.7105\n",
      "Epoch 48/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.7378 - d_loss: 0.6960\n",
      "Epoch 49/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.7606 - d_loss: 0.6954\n",
      "Epoch 50/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7415 - d_loss: 0.6855\n",
      "Epoch 51/80\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.7702 - d_loss: 0.6946\n",
      "Epoch 52/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7622 - d_loss: 0.7318\n",
      "Epoch 53/80\n",
      "146/146 [==============================] - 17s 120ms/step - g_loss: 0.7845 - d_loss: 0.7120\n",
      "Epoch 54/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.8262 - d_loss: 0.7331\n",
      "Epoch 55/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7746 - d_loss: 0.6980\n",
      "Epoch 56/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.8261 - d_loss: 0.6738\n",
      "Epoch 57/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7541 - d_loss: 0.6857\n",
      "Epoch 58/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7396 - d_loss: 0.6908\n",
      "Epoch 59/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7521 - d_loss: 0.6824\n",
      "Epoch 60/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7588 - d_loss: 0.6831\n",
      "Epoch 61/80\n",
      "146/146 [==============================] - 18s 120ms/step - g_loss: 0.7298 - d_loss: 0.7054\n",
      "Epoch 62/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7726 - d_loss: 0.7128\n",
      "Epoch 63/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7571 - d_loss: 0.7073\n",
      "Epoch 64/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.8224 - d_loss: 0.6849\n",
      "Epoch 65/80\n",
      "146/146 [==============================] - 17s 115ms/step - g_loss: 0.7924 - d_loss: 0.6910\n",
      "Epoch 66/80\n",
      "146/146 [==============================] - 18s 127ms/step - g_loss: 0.7403 - d_loss: 0.6840\n",
      "Epoch 67/80\n",
      "146/146 [==============================] - 17s 116ms/step - g_loss: 0.7848 - d_loss: 0.7142\n",
      "Epoch 68/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7617 - d_loss: 0.7121\n",
      "Epoch 69/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7667 - d_loss: 0.7107\n",
      "Epoch 70/80\n",
      "146/146 [==============================] - 17s 119ms/step - g_loss: 0.7908 - d_loss: 0.6969\n",
      "Epoch 71/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.8123 - d_loss: 0.6849\n",
      "Epoch 72/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.8021 - d_loss: 0.6845\n",
      "Epoch 73/80\n",
      "146/146 [==============================] - 18s 121ms/step - g_loss: 0.8288 - d_loss: 0.6723\n",
      "Epoch 74/80\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7850 - d_loss: 0.6945\n",
      "Epoch 75/80\n",
      "146/146 [==============================] - 17s 117ms/step - g_loss: 0.7673 - d_loss: 0.7069\n",
      "Epoch 76/80\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 0.7652 - d_loss: 0.7115\n",
      "Epoch 77/80\n",
      "146/146 [==============================] - 19s 127ms/step - g_loss: 0.8051 - d_loss: 0.6900\n",
      "Epoch 78/80\n",
      "146/146 [==============================] - 19s 128ms/step - g_loss: 0.7720 - d_loss: 0.6771\n",
      "Epoch 79/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 18s 122ms/step - g_loss: 0.7761 - d_loss: 0.7108\n",
      "Epoch 80/80\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.7527 - d_loss: 0.6965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9bba49f10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Interpolating between classes with the trained GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 2000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "780b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0de5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79dfad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    for j in range(num_interpolation):\n",
    "        np_array = np.squeeze(converted_images[j], axis=2)\n",
    "        im = Image.fromarray((np_array))\n",
    "        im.save(f\"{class_dir}/gen_imgs_{class_name}_{j}.png\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
