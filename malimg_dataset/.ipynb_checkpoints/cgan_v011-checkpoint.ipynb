{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 256\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 500\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n",
    "\n",
    "# If weights are used: Weight factor\n",
    "wf = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v021\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/malimg_dataset\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/malimg_dataset/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/numpy_arrays/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/fixed_malimg_dataset\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/malimg-cgan-local-v{new_vnum:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9339 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda172b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ransomware family:  Lolyda.AT\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = 7099+10\n",
    "plt.imshow(np.array(imgs[z]*255).astype(np.uint8), cmap=\"gray\")\n",
    "print(\"Ransomware family: \",list(prelim_dataset.class_indices)[np.argmax(labels[z])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adialer.C': 0,\n",
       " 'Agent.FYI': 1,\n",
       " 'Allaple.A': 2,\n",
       " 'Allaple.L': 3,\n",
       " 'Alueron.gen!J': 4,\n",
       " 'Autorun.K': 5,\n",
       " 'C2LOP.P': 6,\n",
       " 'C2LOP.gen!g': 7,\n",
       " 'Dialplatform.B': 8,\n",
       " 'Dontovo.A': 9,\n",
       " 'Fakerean': 10,\n",
       " 'Instantaccess': 11,\n",
       " 'Lolyda.AA1': 12,\n",
       " 'Lolyda.AA2': 13,\n",
       " 'Lolyda.AA3': 14,\n",
       " 'Lolyda.AT': 15,\n",
       " 'Malex.gen!J': 16,\n",
       " 'Obfuscator.AD': 17,\n",
       " 'Rbot!gen': 18,\n",
       " 'Skintrim.N': 19,\n",
       " 'Swizzor.gen!E': 20,\n",
       " 'Swizzor.gen!I': 21,\n",
       " 'VB.AT': 22,\n",
       " 'Wintrim.BX': 23,\n",
       " 'Yuner.A': 24}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 26\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        15040     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 384,321\n",
      "Trainable params: 384,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 17984)             5071488   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 17984)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 281)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       575616    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 6,177,921\n",
      "Trainable params: 6,177,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70546",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8cba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c67",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af608a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b649da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'{new_dir}'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "146/146 [==============================] - 22s 127ms/step - g_loss: 2.3694 - d_loss: 0.6295\n",
      "Epoch 2/500\n",
      "146/146 [==============================] - 18s 122ms/step - g_loss: 1.2407 - d_loss: 1.1415\n",
      "Epoch 3/500\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 2.9374 - d_loss: 0.7142\n",
      "Epoch 4/500\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 2.4822 - d_loss: 0.3098\n",
      "Epoch 5/500\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 4.7145 - d_loss: 0.2620\n",
      "Epoch 6/500\n",
      "146/146 [==============================] - 18s 123ms/step - g_loss: 4.7789 - d_loss: 0.1598\n",
      "Epoch 7/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 5.8904 - d_loss: 0.0442\n",
      "Epoch 8/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 6.2806 - d_loss: 0.0464\n",
      "Epoch 9/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 7.5340 - d_loss: 0.0336\n",
      "Epoch 10/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 5.8740 - d_loss: 0.0813\n",
      "Epoch 11/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 7.6490 - d_loss: 0.0159\n",
      "Epoch 12/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 8.0234 - d_loss: 0.0128\n",
      "Epoch 13/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 8.9733 - d_loss: 0.0018\n",
      "Epoch 14/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 8.5956 - d_loss: 0.0024\n",
      "Epoch 15/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 8.1667 - d_loss: 0.0132\n",
      "Epoch 16/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 8.2198 - d_loss: 0.0136\n",
      "Epoch 17/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 7.8086 - d_loss: 0.0131\n",
      "Epoch 18/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 5.0524 - d_loss: 0.4546\n",
      "Epoch 19/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 1.3714 - d_loss: 0.5905\n",
      "Epoch 20/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8214 - d_loss: 0.7198\n",
      "Epoch 21/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8958 - d_loss: 0.6254\n",
      "Epoch 22/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.7802 - d_loss: 0.6816\n",
      "Epoch 23/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 1.1718 - d_loss: 0.5684\n",
      "Epoch 24/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8223 - d_loss: 0.6666\n",
      "Epoch 25/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8239 - d_loss: 0.6667\n",
      "Epoch 26/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8063 - d_loss: 0.6774\n",
      "Epoch 27/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8650 - d_loss: 0.6195\n",
      "Epoch 28/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8589 - d_loss: 0.6267\n",
      "Epoch 29/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8299 - d_loss: 0.6564\n",
      "Epoch 30/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8188 - d_loss: 0.6598\n",
      "Epoch 31/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8734 - d_loss: 0.6366\n",
      "Epoch 32/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8759 - d_loss: 0.6383\n",
      "Epoch 33/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8283 - d_loss: 0.6527\n",
      "Epoch 34/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9566 - d_loss: 0.6457\n",
      "Epoch 35/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8642 - d_loss: 0.6515\n",
      "Epoch 36/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9549 - d_loss: 0.6277\n",
      "Epoch 37/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8681 - d_loss: 0.6535\n",
      "Epoch 38/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8932 - d_loss: 0.6438\n",
      "Epoch 39/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 1.0723 - d_loss: 0.6610\n",
      "Epoch 40/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8804 - d_loss: 0.6663\n",
      "Epoch 41/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9250 - d_loss: 0.6670\n",
      "Epoch 42/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8718 - d_loss: 0.6959\n",
      "Epoch 43/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8628 - d_loss: 0.6570\n",
      "Epoch 44/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8928 - d_loss: 0.6316\n",
      "Epoch 45/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8182 - d_loss: 0.6681\n",
      "Epoch 46/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8368 - d_loss: 0.6935\n",
      "Epoch 47/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8472 - d_loss: 0.6594\n",
      "Epoch 48/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8997 - d_loss: 0.6848\n",
      "Epoch 49/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8527 - d_loss: 0.6772\n",
      "Epoch 50/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8651 - d_loss: 0.6912\n",
      "Epoch 51/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8943 - d_loss: 0.6602\n",
      "Epoch 52/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8512 - d_loss: 0.7145\n",
      "Epoch 53/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8404 - d_loss: 0.6788\n",
      "Epoch 54/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8157 - d_loss: 0.6961\n",
      "Epoch 55/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8240 - d_loss: 0.6805\n",
      "Epoch 56/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8738 - d_loss: 0.6695\n",
      "Epoch 57/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8227 - d_loss: 0.6785\n",
      "Epoch 58/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8276 - d_loss: 0.6726\n",
      "Epoch 59/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8123 - d_loss: 0.6996\n",
      "Epoch 60/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8238 - d_loss: 0.6813\n",
      "Epoch 61/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8364 - d_loss: 0.6861\n",
      "Epoch 62/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8442 - d_loss: 0.6749\n",
      "Epoch 63/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9010 - d_loss: 0.7230\n",
      "Epoch 64/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8300 - d_loss: 0.6929\n",
      "Epoch 65/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8376 - d_loss: 0.6850\n",
      "Epoch 66/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8400 - d_loss: 0.6820\n",
      "Epoch 67/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8454 - d_loss: 0.6798\n",
      "Epoch 68/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8416 - d_loss: 0.6925\n",
      "Epoch 69/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8324 - d_loss: 0.6822\n",
      "Epoch 70/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8345 - d_loss: 0.7063\n",
      "Epoch 71/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8342 - d_loss: 0.7047\n",
      "Epoch 72/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8295 - d_loss: 0.7051\n",
      "Epoch 73/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8243 - d_loss: 0.6774\n",
      "Epoch 74/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7838 - d_loss: 0.6825\n",
      "Epoch 75/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8483 - d_loss: 0.6969\n",
      "Epoch 76/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8572 - d_loss: 0.6788\n",
      "Epoch 77/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8743 - d_loss: 0.6572\n",
      "Epoch 78/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8298 - d_loss: 0.6893\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8204 - d_loss: 0.6808\n",
      "Epoch 80/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8206 - d_loss: 0.7126\n",
      "Epoch 81/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8408 - d_loss: 0.7126\n",
      "Epoch 82/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9065 - d_loss: 0.7289\n",
      "Epoch 83/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8273 - d_loss: 0.6765\n",
      "Epoch 84/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9342 - d_loss: 0.7064\n",
      "Epoch 85/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7957 - d_loss: 0.7137\n",
      "Epoch 86/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8200 - d_loss: 0.6720\n",
      "Epoch 87/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8207 - d_loss: 0.6854\n",
      "Epoch 88/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8098 - d_loss: 0.6728\n",
      "Epoch 89/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8431 - d_loss: 0.6618\n",
      "Epoch 90/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8607 - d_loss: 0.6751\n",
      "Epoch 91/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8173 - d_loss: 0.6986\n",
      "Epoch 92/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8841 - d_loss: 0.6592\n",
      "Epoch 93/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8579 - d_loss: 0.6421\n",
      "Epoch 94/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8214 - d_loss: 0.6740\n",
      "Epoch 95/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8810 - d_loss: 0.6544\n",
      "Epoch 96/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9245 - d_loss: 0.7212\n",
      "Epoch 97/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8102 - d_loss: 0.6994\n",
      "Epoch 98/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8796 - d_loss: 0.6873\n",
      "Epoch 99/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8160 - d_loss: 0.6929\n",
      "Epoch 100/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8033 - d_loss: 0.7014\n",
      "Epoch 101/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8309 - d_loss: 0.6588\n",
      "Epoch 102/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8552 - d_loss: 0.6868\n",
      "Epoch 103/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9070 - d_loss: 0.6659\n",
      "Epoch 104/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7991 - d_loss: 0.6924\n",
      "Epoch 105/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8192 - d_loss: 0.7046\n",
      "Epoch 106/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8393 - d_loss: 0.6779\n",
      "Epoch 107/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8660 - d_loss: 0.7056\n",
      "Epoch 108/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8642 - d_loss: 0.7365\n",
      "Epoch 109/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8202 - d_loss: 0.6510\n",
      "Epoch 110/500\n",
      "146/146 [==============================] - 19s 131ms/step - g_loss: 0.7887 - d_loss: 0.7129\n",
      "Epoch 111/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7890 - d_loss: 0.6980\n",
      "Epoch 112/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8347 - d_loss: 0.6765\n",
      "Epoch 113/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8473 - d_loss: 0.6596\n",
      "Epoch 114/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8685 - d_loss: 0.6655\n",
      "Epoch 115/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8827 - d_loss: 0.6684\n",
      "Epoch 116/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8692 - d_loss: 0.7007\n",
      "Epoch 117/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8890 - d_loss: 0.6657\n",
      "Epoch 118/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.7664 - d_loss: 0.7192\n",
      "Epoch 119/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8173 - d_loss: 0.6911\n",
      "Epoch 120/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8771 - d_loss: 0.7051\n",
      "Epoch 121/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8315 - d_loss: 0.6768\n",
      "Epoch 122/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8503 - d_loss: 0.6673\n",
      "Epoch 123/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8082 - d_loss: 0.6908\n",
      "Epoch 124/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8035 - d_loss: 0.6677\n",
      "Epoch 125/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8544 - d_loss: 0.6699\n",
      "Epoch 126/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8668 - d_loss: 0.6568\n",
      "Epoch 127/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8605 - d_loss: 0.6854\n",
      "Epoch 128/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8910 - d_loss: 0.6860\n",
      "Epoch 129/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8461 - d_loss: 0.6818\n",
      "Epoch 130/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7906 - d_loss: 0.7210\n",
      "Epoch 131/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8659 - d_loss: 0.6783\n",
      "Epoch 132/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8912 - d_loss: 0.6841\n",
      "Epoch 133/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8832 - d_loss: 0.6534\n",
      "Epoch 134/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8441 - d_loss: 0.6629\n",
      "Epoch 135/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8846 - d_loss: 0.6541\n",
      "Epoch 136/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9011 - d_loss: 0.6522\n",
      "Epoch 137/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8402 - d_loss: 0.6650\n",
      "Epoch 138/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8219 - d_loss: 0.6680\n",
      "Epoch 139/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8858 - d_loss: 0.6413\n",
      "Epoch 140/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8826 - d_loss: 0.6605\n",
      "Epoch 141/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8885 - d_loss: 0.6789\n",
      "Epoch 142/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8932 - d_loss: 0.6684\n",
      "Epoch 143/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8699 - d_loss: 0.6391\n",
      "Epoch 144/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8226 - d_loss: 0.6767\n",
      "Epoch 145/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9139 - d_loss: 0.6572\n",
      "Epoch 146/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8499 - d_loss: 0.7299\n",
      "Epoch 147/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8684 - d_loss: 0.6786\n",
      "Epoch 148/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9061 - d_loss: 0.6386\n",
      "Epoch 149/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8281 - d_loss: 0.7123\n",
      "Epoch 150/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8268 - d_loss: 0.6880\n",
      "Epoch 151/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8924 - d_loss: 0.6865\n",
      "Epoch 152/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9223 - d_loss: 0.7116\n",
      "Epoch 153/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8437 - d_loss: 0.6634\n",
      "Epoch 154/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8196 - d_loss: 0.7213\n",
      "Epoch 155/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8756 - d_loss: 0.6651\n",
      "Epoch 156/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9090 - d_loss: 0.6632\n",
      "Epoch 157/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8771 - d_loss: 0.6772\n",
      "Epoch 158/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8925 - d_loss: 0.6481\n",
      "Epoch 159/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9235 - d_loss: 0.6643\n",
      "Epoch 160/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8636 - d_loss: 0.6583\n",
      "Epoch 161/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8864 - d_loss: 0.6515\n",
      "Epoch 162/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8528 - d_loss: 0.6793\n",
      "Epoch 163/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8909 - d_loss: 0.6723\n",
      "Epoch 164/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8450 - d_loss: 0.6625\n",
      "Epoch 165/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8099 - d_loss: 0.7135\n",
      "Epoch 166/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9088 - d_loss: 0.6627\n",
      "Epoch 167/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8447 - d_loss: 0.6774\n",
      "Epoch 168/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8743 - d_loss: 0.6572\n",
      "Epoch 169/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8719 - d_loss: 0.6847\n",
      "Epoch 170/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9015 - d_loss: 0.6661\n",
      "Epoch 171/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8354 - d_loss: 0.6674\n",
      "Epoch 172/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8661 - d_loss: 0.6686\n",
      "Epoch 173/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8736 - d_loss: 0.6706\n",
      "Epoch 174/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8477 - d_loss: 0.6753\n",
      "Epoch 175/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8333 - d_loss: 0.6934\n",
      "Epoch 176/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8723 - d_loss: 0.6756\n",
      "Epoch 177/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9224 - d_loss: 0.6431\n",
      "Epoch 178/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9069 - d_loss: 0.6600\n",
      "Epoch 179/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8322 - d_loss: 0.7049\n",
      "Epoch 180/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8510 - d_loss: 0.6404\n",
      "Epoch 181/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9046 - d_loss: 0.6842\n",
      "Epoch 182/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9444 - d_loss: 0.6696\n",
      "Epoch 183/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9023 - d_loss: 0.6410\n",
      "Epoch 184/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8869 - d_loss: 0.6427\n",
      "Epoch 185/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9497 - d_loss: 0.6593\n",
      "Epoch 186/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8298 - d_loss: 0.7241\n",
      "Epoch 187/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8184 - d_loss: 0.6750\n",
      "Epoch 188/500\n",
      "146/146 [==============================] - 21s 145ms/step - g_loss: 0.8547 - d_loss: 0.6895\n",
      "Epoch 189/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8095 - d_loss: 0.6767\n",
      "Epoch 190/500\n",
      "146/146 [==============================] - 19s 128ms/step - g_loss: 0.8177 - d_loss: 0.6645\n",
      "Epoch 191/500\n",
      "146/146 [==============================] - 19s 130ms/step - g_loss: 0.7891 - d_loss: 0.6902\n",
      "Epoch 192/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8260 - d_loss: 0.7021\n",
      "Epoch 193/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8130 - d_loss: 0.6783\n",
      "Epoch 194/500\n",
      "146/146 [==============================] - 20s 135ms/step - g_loss: 0.8404 - d_loss: 0.6619\n",
      "Epoch 195/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8965 - d_loss: 0.6804\n",
      "Epoch 196/500\n",
      "146/146 [==============================] - 19s 130ms/step - g_loss: 0.8434 - d_loss: 0.6959\n",
      "Epoch 197/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8787 - d_loss: 0.6486\n",
      "Epoch 198/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8894 - d_loss: 0.6643\n",
      "Epoch 199/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8632 - d_loss: 0.6346\n",
      "Epoch 200/500\n",
      "146/146 [==============================] - 19s 127ms/step - g_loss: 0.8611 - d_loss: 0.6851\n",
      "Epoch 201/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8978 - d_loss: 0.6528\n",
      "Epoch 202/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.9111 - d_loss: 0.6805\n",
      "Epoch 203/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8806 - d_loss: 0.6741\n",
      "Epoch 204/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8672 - d_loss: 0.7495\n",
      "Epoch 205/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8470 - d_loss: 0.6607\n",
      "Epoch 206/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8394 - d_loss: 0.6730\n",
      "Epoch 207/500\n",
      "146/146 [==============================] - 18s 124ms/step - g_loss: 0.8510 - d_loss: 0.6853\n",
      "Epoch 208/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8670 - d_loss: 0.6766\n",
      "Epoch 209/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8479 - d_loss: 0.6884\n",
      "Epoch 210/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8858 - d_loss: 0.6790\n",
      "Epoch 211/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9058 - d_loss: 0.6555\n",
      "Epoch 212/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9324 - d_loss: 0.6989\n",
      "Epoch 213/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.9061 - d_loss: 0.6544\n",
      "Epoch 214/500\n",
      "146/146 [==============================] - 19s 130ms/step - g_loss: 0.8981 - d_loss: 0.6576\n",
      "Epoch 215/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8720 - d_loss: 0.6641\n",
      "Epoch 216/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8610 - d_loss: 0.6680\n",
      "Epoch 217/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9037 - d_loss: 0.6429\n",
      "Epoch 218/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8374 - d_loss: 0.6671\n",
      "Epoch 219/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8813 - d_loss: 0.6388\n",
      "Epoch 220/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8654 - d_loss: 0.6551\n",
      "Epoch 221/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9147 - d_loss: 0.6316\n",
      "Epoch 222/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.9030 - d_loss: 0.6590\n",
      "Epoch 223/500\n",
      "146/146 [==============================] - 19s 127ms/step - g_loss: 0.8807 - d_loss: 0.6896\n",
      "Epoch 224/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.9048 - d_loss: 0.6758\n",
      "Epoch 225/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8843 - d_loss: 0.6574\n",
      "Epoch 226/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9118 - d_loss: 0.6405\n",
      "Epoch 227/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8564 - d_loss: 0.7073\n",
      "Epoch 228/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8708 - d_loss: 0.6563\n",
      "Epoch 229/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8406 - d_loss: 0.6589\n",
      "Epoch 230/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8911 - d_loss: 0.6622\n",
      "Epoch 231/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8441 - d_loss: 0.6641\n",
      "Epoch 232/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8902 - d_loss: 0.6447\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8353 - d_loss: 0.6706\n",
      "Epoch 234/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9103 - d_loss: 0.6947\n",
      "Epoch 235/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8322 - d_loss: 0.6757\n",
      "Epoch 236/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8539 - d_loss: 0.6754\n",
      "Epoch 237/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9050 - d_loss: 0.6819\n",
      "Epoch 238/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8386 - d_loss: 0.6697\n",
      "Epoch 239/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8261 - d_loss: 0.7276\n",
      "Epoch 240/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8358 - d_loss: 0.6664\n",
      "Epoch 241/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.7869 - d_loss: 0.7020\n",
      "Epoch 242/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8368 - d_loss: 0.6823\n",
      "Epoch 243/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8405 - d_loss: 0.6405\n",
      "Epoch 244/500\n",
      "146/146 [==============================] - 18s 126ms/step - g_loss: 0.8420 - d_loss: 0.6700\n",
      "Epoch 245/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8983 - d_loss: 0.6373\n",
      "Epoch 246/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8725 - d_loss: 0.7037\n",
      "Epoch 247/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8450 - d_loss: 0.6672\n",
      "Epoch 248/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8787 - d_loss: 0.6514\n",
      "Epoch 249/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8438 - d_loss: 0.6715\n",
      "Epoch 250/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8863 - d_loss: 0.6388\n",
      "Epoch 251/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9051 - d_loss: 0.7156\n",
      "Epoch 252/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9361 - d_loss: 0.6379\n",
      "Epoch 253/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9314 - d_loss: 0.6228\n",
      "Epoch 254/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8804 - d_loss: 0.6390\n",
      "Epoch 255/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8620 - d_loss: 0.6944\n",
      "Epoch 256/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9089 - d_loss: 0.6194\n",
      "Epoch 257/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8520 - d_loss: 0.7072\n",
      "Epoch 258/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8867 - d_loss: 0.6607\n",
      "Epoch 259/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8907 - d_loss: 0.6412\n",
      "Epoch 260/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9094 - d_loss: 0.6313\n",
      "Epoch 261/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9158 - d_loss: 0.6318\n",
      "Epoch 262/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8850 - d_loss: 0.6668\n",
      "Epoch 263/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 1.1764 - d_loss: 0.5924\n",
      "Epoch 264/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9315 - d_loss: 0.6278\n",
      "Epoch 265/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 1.0041 - d_loss: 0.6193\n",
      "Epoch 266/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8139 - d_loss: 0.7103\n",
      "Epoch 267/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8254 - d_loss: 0.6872\n",
      "Epoch 268/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8697 - d_loss: 0.6570\n",
      "Epoch 269/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8403 - d_loss: 0.6920\n",
      "Epoch 270/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8729 - d_loss: 0.6586\n",
      "Epoch 271/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9636 - d_loss: 0.6446\n",
      "Epoch 272/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9214 - d_loss: 0.6219\n",
      "Epoch 273/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9723 - d_loss: 0.6160\n",
      "Epoch 274/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9073 - d_loss: 0.6225\n",
      "Epoch 275/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8911 - d_loss: 0.6685\n",
      "Epoch 276/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9604 - d_loss: 0.6243\n",
      "Epoch 277/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8751 - d_loss: 0.6687\n",
      "Epoch 278/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9270 - d_loss: 0.6276\n",
      "Epoch 279/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9373 - d_loss: 0.6996\n",
      "Epoch 280/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9139 - d_loss: 0.6560\n",
      "Epoch 281/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9152 - d_loss: 0.6234\n",
      "Epoch 282/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9793 - d_loss: 0.5965\n",
      "Epoch 283/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9368 - d_loss: 0.6761\n",
      "Epoch 284/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9984 - d_loss: 0.5827\n",
      "Epoch 285/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9696 - d_loss: 0.6810\n",
      "Epoch 286/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 1.0496 - d_loss: 0.7170\n",
      "Epoch 287/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9235 - d_loss: 0.6603\n",
      "Epoch 288/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9427 - d_loss: 0.6141\n",
      "Epoch 289/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9225 - d_loss: 0.6300\n",
      "Epoch 290/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9725 - d_loss: 0.6091\n",
      "Epoch 291/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9877 - d_loss: 0.6741\n",
      "Epoch 292/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9250 - d_loss: 0.6526\n",
      "Epoch 293/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9883 - d_loss: 0.6905\n",
      "Epoch 294/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8959 - d_loss: 0.6653\n",
      "Epoch 295/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9318 - d_loss: 0.6423\n",
      "Epoch 296/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8737 - d_loss: 0.6947\n",
      "Epoch 297/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9330 - d_loss: 0.6370\n",
      "Epoch 298/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9101 - d_loss: 0.6402\n",
      "Epoch 299/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9493 - d_loss: 0.6346\n",
      "Epoch 300/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9298 - d_loss: 0.6236\n",
      "Epoch 301/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9895 - d_loss: 0.6357\n",
      "Epoch 302/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8634 - d_loss: 0.6981\n",
      "Epoch 303/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9244 - d_loss: 0.6345\n",
      "Epoch 304/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8729 - d_loss: 0.6845\n",
      "Epoch 305/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8634 - d_loss: 0.6497\n",
      "Epoch 306/500\n",
      "146/146 [==============================] - 19s 131ms/step - g_loss: 0.9392 - d_loss: 0.6242\n",
      "Epoch 307/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9089 - d_loss: 0.7090\n",
      "Epoch 308/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9252 - d_loss: 0.6906\n",
      "Epoch 309/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9564 - d_loss: 0.6754\n",
      "Epoch 310/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9708 - d_loss: 0.6048\n",
      "Epoch 311/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9919 - d_loss: 0.6116\n",
      "Epoch 312/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8342 - d_loss: 0.7188\n",
      "Epoch 313/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9435 - d_loss: 0.6133\n",
      "Epoch 314/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8909 - d_loss: 0.6886\n",
      "Epoch 315/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9660 - d_loss: 0.6173\n",
      "Epoch 316/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9597 - d_loss: 0.6027\n",
      "Epoch 317/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9220 - d_loss: 0.6529\n",
      "Epoch 318/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9471 - d_loss: 0.6006\n",
      "Epoch 319/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9407 - d_loss: 0.6485\n",
      "Epoch 320/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8783 - d_loss: 0.6845\n",
      "Epoch 321/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9347 - d_loss: 0.6521\n",
      "Epoch 322/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9947 - d_loss: 0.6520\n",
      "Epoch 323/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8843 - d_loss: 0.6858\n",
      "Epoch 324/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8805 - d_loss: 0.6472\n",
      "Epoch 325/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9575 - d_loss: 0.6322\n",
      "Epoch 326/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9792 - d_loss: 0.6016\n",
      "Epoch 327/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9437 - d_loss: 0.6689\n",
      "Epoch 328/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9173 - d_loss: 0.6505\n",
      "Epoch 329/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9827 - d_loss: 0.6307\n",
      "Epoch 330/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9986 - d_loss: 0.5857\n",
      "Epoch 331/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9656 - d_loss: 0.6109\n",
      "Epoch 332/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9070 - d_loss: 0.6826\n",
      "Epoch 333/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.8956 - d_loss: 0.6755\n",
      "Epoch 334/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9218 - d_loss: 0.6381\n",
      "Epoch 335/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9544 - d_loss: 0.6288\n",
      "Epoch 336/500\n",
      "146/146 [==============================] - 18s 125ms/step - g_loss: 0.9533 - d_loss: 0.6139\n",
      "Epoch 337/500\n",
      "146/146 [==============================] - 24s 166ms/step - g_loss: 0.9546 - d_loss: 0.6238\n",
      "Epoch 338/500\n",
      "146/146 [==============================] - 21s 143ms/step - g_loss: 0.9503 - d_loss: 0.6263\n",
      "Epoch 339/500\n",
      "146/146 [==============================] - 23s 154ms/step - g_loss: 0.9914 - d_loss: 0.6298\n",
      "Epoch 340/500\n",
      "146/146 [==============================] - 20s 139ms/step - g_loss: 0.8678 - d_loss: 0.6740\n",
      "Epoch 341/500\n",
      "146/146 [==============================] - 21s 146ms/step - g_loss: 0.9332 - d_loss: 0.6323\n",
      "Epoch 342/500\n",
      "146/146 [==============================] - 20s 139ms/step - g_loss: 1.0015 - d_loss: 0.6081\n",
      "Epoch 343/500\n",
      "146/146 [==============================] - 21s 146ms/step - g_loss: 1.0916 - d_loss: 0.5851\n",
      "Epoch 344/500\n",
      "146/146 [==============================] - 26s 180ms/step - g_loss: 0.8643 - d_loss: 0.7151\n",
      "Epoch 345/500\n",
      "146/146 [==============================] - 21s 145ms/step - g_loss: 0.8991 - d_loss: 0.6591\n",
      "Epoch 346/500\n",
      "146/146 [==============================] - 20s 135ms/step - g_loss: 0.9064 - d_loss: 0.6803\n",
      "Epoch 347/500\n",
      "146/146 [==============================] - 20s 136ms/step - g_loss: 0.9027 - d_loss: 0.6148\n",
      "Epoch 348/500\n",
      "146/146 [==============================] - 21s 145ms/step - g_loss: 0.8936 - d_loss: 0.6703\n",
      "Epoch 349/500\n",
      "146/146 [==============================] - 23s 157ms/step - g_loss: 0.9557 - d_loss: 0.6532\n",
      "Epoch 350/500\n",
      " 65/146 [============>.................] - ETA: 12s - g_loss: 0.9088 - d_loss: 0.6300"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5208/4257128501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m cond_gan.fit(dataset, epochs=epoch_t, \n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGANMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Interpolating between classes with the trained GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Choose the number of intermediate images that would be generated in\n",
    "# between the interpolation + 2 (start and last images).\n",
    "num_interpolation = 2000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "780b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0de5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dfad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    for j in range(num_interpolation):\n",
    "        np_array = np.squeeze(converted_images[j], axis=2)\n",
    "        im = Image.fromarray((np_array))\n",
    "        im.save(f\"{class_dir}/gen_imgs_{class_name}_{j}.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd28f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
