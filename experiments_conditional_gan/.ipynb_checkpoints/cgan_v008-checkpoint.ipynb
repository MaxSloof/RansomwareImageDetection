{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546011",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "\n",
    "Used to generate new training data for the ransomware families to overcome the skewed distribution of training data towards the benign samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d338ac",
   "metadata": {},
   "source": [
    "**Change parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b44d3f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d37ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Color mode\n",
    "ch = 'grayscale'\n",
    "\n",
    "# Image size\n",
    "iw, ih = 64,64\n",
    "im_size = (iw,ih)\n",
    "\n",
    "# Latent dim size\n",
    "latent_dim = 128\n",
    "\n",
    "# Number of Epochs\n",
    "epoch_t = 2000 # Add this point this model has run 2000 epochs through using the checkpoints (see ckpt_cgan_v009)\n",
    "\n",
    "# Computation environment: Kaggle (0) or Local (1)\n",
    "cenv = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd651cb4",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd315bc2",
   "metadata": {},
   "source": [
    "Automatic notebook preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50855d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(ch == 'rgb'):\n",
    "    chnum = 3\n",
    "elif(ch == 'grayscale'):\n",
    "    chnum = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca175ee",
   "metadata": {},
   "source": [
    "Create new folder to save the output of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193e04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 matches(es) found\n",
      "--------------\n",
      "New folder name: cgan-local-v008\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "if cenv == 1:\n",
    "    file_exists = []\n",
    "    vnum = 1\n",
    "    dir = \"C:/Users/Max/Documents/GitHub/conditional_gan\"\n",
    "    for files in os.listdir(dir):\n",
    "        if \"cgan\" in files:\n",
    "            try:\n",
    "                vnum = max(vnum, int(files[-3:]))\n",
    "            except: \n",
    "                continue\n",
    "            new_vnum = vnum + 1\n",
    "            file_exists.append(True)\n",
    "        else: \n",
    "            file_exists.append(False)\n",
    "    # If this is the first notebook you want to save, a new folder will be created with version #001\n",
    "    if sum(file_exists) == 0:\n",
    "        new_vnum = 1\n",
    "        print(\"No matches found\")\n",
    "\n",
    "    else: \n",
    "        print(f\"{sum(file_exists)} matches(es) found\")\n",
    "        print(\"--------------\")\n",
    "\n",
    "    # Print new folder name\n",
    "    print(f\"New folder name: cgan-local-v{new_vnum:03}\")\n",
    "    print(\"--------------\")\n",
    "    \n",
    "    # Create new folder with the name of the notebook and the version number\n",
    "    new_dir = f\"C://Users/Max/Documents/GitHub/conditional_gan/cgan-local-v{new_vnum:03}\"\n",
    "    os.makedirs(new_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30853b",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d54d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    path_root = \"/kaggle/input/data-wo-benign\"\n",
    "    path_save_imgs = \"/kaggle/working/numpy_arrays/\"\n",
    "if cenv == 1:\n",
    "    path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\"\n",
    "    path_save_imgs = f\"C:/Users/Max/Documents/image_data/cgan-local-v{new_vnum:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6642f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1/255 # Pixel values need to be scaled between 0 and 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4549c79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = ch,\n",
    "    target_size = im_size,\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=False\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e987c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = prelim_dataset.samples\n",
    "num_classes = max(prelim_dataset.labels) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba425506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BetterSurf': 0,\n",
       " 'Eksor.A': 1,\n",
       " 'Obfuscator.AFQ': 2,\n",
       " 'Occamy.C': 3,\n",
       " 'OnLineGames.CTB': 4,\n",
       " 'Reveton.A': 5,\n",
       " 'Sfone': 6,\n",
       " 'VB.IL': 7,\n",
       " 'Zbot': 8,\n",
       " 'Zbot!CI': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_dataset.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b0729",
   "metadata": {},
   "source": [
    "Create tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c29159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250f6a3",
   "metadata": {},
   "source": [
    "Calculate number of input channel for Gen and Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8c4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 11\n"
     ]
    }
   ],
   "source": [
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = chnum + num_classes\n",
    "print(generator_in_channels, discriminator_in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5518b3",
   "metadata": {},
   "source": [
    "# Creating discriminator and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5807858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the discriminator.\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((iw, ih, discriminator_in_channels)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73d69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator.\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer((generator_in_channels,)),\n",
    "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "        # 7x7x(128 + num_classes) map.\n",
    "        layers.Dense(8 * 8 * generator_in_channels),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((8, 8, generator_in_channels)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f2b6070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 64)        6400      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 375,681\n",
      "Trainable params: 375,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8019d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8832)              1227648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8832)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 138)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       282752    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 1)         6273      \n",
      "=================================================================\n",
      "Total params: 2,041,217\n",
      "Trainable params: 2,041,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291bdf",
   "metadata": {},
   "source": [
    "**Create Conditional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1fa8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[ih * iw]\n",
    "        )\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, iw, ih, num_classes)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e70546",
   "metadata": {},
   "source": [
    "**Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8cba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "d_optimizer=keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer=keras.optimizers.Adam(learning_rate=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c67",
   "metadata": {},
   "source": [
    "**Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af608a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Save the model every 5 epochs \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b649da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cenv == 0:\n",
    "    checkpoint_dir = '/kaggle/working/checkpoints'\n",
    "if cenv == 1:\n",
    "    checkpoint_dir = f'{new_dir}'\n",
    "    \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_optimizer,\n",
    "                                 discriminator_optimizer=d_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7c93",
   "metadata": {},
   "source": [
    "# Training C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2338e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "196/196 [==============================] - 28s 117ms/step - g_loss: 1.4519 - d_loss: 0.6379\n",
      "Epoch 2/500\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 5.7074 - d_loss: 0.0889\n",
      "Epoch 3/500\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 5.1318 - d_loss: 0.4132\n",
      "Epoch 4/500\n",
      "196/196 [==============================] - 22s 112ms/step - g_loss: 5.7961 - d_loss: 0.0952\n",
      "Epoch 5/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 2.6814 - d_loss: 0.1633\n",
      "Epoch 6/500\n",
      "196/196 [==============================] - 23s 116ms/step - g_loss: 5.1898 - d_loss: 0.0337\n",
      "Epoch 7/500\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 2.8507 - d_loss: 0.3967\n",
      "Epoch 8/500\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 3.6308 - d_loss: 0.2321\n",
      "Epoch 9/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.5987 - d_loss: 0.3670\n",
      "Epoch 10/500\n",
      "196/196 [==============================] - 23s 117ms/step - g_loss: 1.6031 - d_loss: 0.5304\n",
      "Epoch 11/500\n",
      "196/196 [==============================] - 23s 115ms/step - g_loss: 1.9132 - d_loss: 0.4930\n",
      "Epoch 12/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 2.0796 - d_loss: 0.4257\n",
      "Epoch 13/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.8032 - d_loss: 0.4804\n",
      "Epoch 14/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7741 - d_loss: 0.4878\n",
      "Epoch 15/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7565 - d_loss: 0.5475\n",
      "Epoch 16/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.6765 - d_loss: 0.4713\n",
      "Epoch 17/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.6531 - d_loss: 0.5767\n",
      "Epoch 18/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4745 - d_loss: 0.5111\n",
      "Epoch 19/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.5098 - d_loss: 0.4904\n",
      "Epoch 20/500\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.4710 - d_loss: 0.5136\n",
      "Epoch 21/500\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.5396 - d_loss: 0.5645\n",
      "Epoch 22/500\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 1.4118 - d_loss: 0.5446\n",
      "Epoch 23/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3704 - d_loss: 0.5059\n",
      "Epoch 24/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3009 - d_loss: 0.5309\n",
      "Epoch 25/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3559 - d_loss: 0.5516\n",
      "Epoch 26/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1639 - d_loss: 0.5716\n",
      "Epoch 27/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1773 - d_loss: 0.5986\n",
      "Epoch 28/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2198 - d_loss: 0.5780\n",
      "Epoch 29/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1836 - d_loss: 0.5639\n",
      "Epoch 30/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1860 - d_loss: 0.5456\n",
      "Epoch 31/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2066 - d_loss: 0.5503\n",
      "Epoch 32/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2870 - d_loss: 0.5122\n",
      "Epoch 33/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2771 - d_loss: 0.5566\n",
      "Epoch 34/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.4248 - d_loss: 0.6403\n",
      "Epoch 35/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2507 - d_loss: 0.6319\n",
      "Epoch 36/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2238 - d_loss: 0.5930\n",
      "Epoch 37/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2675 - d_loss: 0.6287\n",
      "Epoch 38/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2360 - d_loss: 0.5469\n",
      "Epoch 39/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2880 - d_loss: 0.5330\n",
      "Epoch 40/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3334 - d_loss: 0.5341\n",
      "Epoch 41/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.4372 - d_loss: 0.5567\n",
      "Epoch 42/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.4505 - d_loss: 0.5199\n",
      "Epoch 43/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.6044 - d_loss: 0.5221\n",
      "Epoch 44/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.6549 - d_loss: 0.5431\n",
      "Epoch 45/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.5751 - d_loss: 0.5250\n",
      "Epoch 46/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.6149 - d_loss: 0.5146\n",
      "Epoch 47/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.4959 - d_loss: 0.5537\n",
      "Epoch 48/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.6344 - d_loss: 0.5163\n",
      "Epoch 49/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7418 - d_loss: 0.4913\n",
      "Epoch 50/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.5425 - d_loss: 0.5151\n",
      "Epoch 51/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.5724 - d_loss: 0.5466\n",
      "Epoch 52/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.6247 - d_loss: 0.5258\n",
      "Epoch 53/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7719 - d_loss: 0.5027\n",
      "Epoch 54/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7169 - d_loss: 0.5109\n",
      "Epoch 55/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.8758 - d_loss: 0.5543\n",
      "Epoch 56/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.7961 - d_loss: 0.5330\n",
      "Epoch 57/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.8167 - d_loss: 0.5275\n",
      "Epoch 58/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.8212 - d_loss: 0.5179\n",
      "Epoch 59/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.9008 - d_loss: 0.5526\n",
      "Epoch 60/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.6449 - d_loss: 0.5624\n",
      "Epoch 61/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2850 - d_loss: 0.5813\n",
      "Epoch 62/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0145 - d_loss: 0.6018\n",
      "Epoch 63/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1012 - d_loss: 0.6797\n",
      "Epoch 64/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0330 - d_loss: 0.6736\n",
      "Epoch 65/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9358 - d_loss: 0.6482\n",
      "Epoch 66/500\n",
      "196/196 [==============================] - 22s 115ms/step - g_loss: 0.9105 - d_loss: 0.7322\n",
      "Epoch 67/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9144 - d_loss: 0.6547\n",
      "Epoch 68/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9542 - d_loss: 0.6597\n",
      "Epoch 69/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0586 - d_loss: 0.6573\n",
      "Epoch 70/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0366 - d_loss: 0.6321\n",
      "Epoch 71/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0670 - d_loss: 0.7007\n",
      "Epoch 72/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9553 - d_loss: 0.6702\n",
      "Epoch 73/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9453 - d_loss: 0.6678\n",
      "Epoch 74/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0405 - d_loss: 0.6467\n",
      "Epoch 75/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0424 - d_loss: 0.6858\n",
      "Epoch 76/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0400 - d_loss: 0.6608\n",
      "Epoch 77/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9310 - d_loss: 0.6576\n",
      "Epoch 78/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9259 - d_loss: 0.6759\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0043 - d_loss: 0.6783\n",
      "Epoch 80/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1550 - d_loss: 0.6551\n",
      "Epoch 81/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9738 - d_loss: 0.6617\n",
      "Epoch 82/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0839 - d_loss: 0.6671\n",
      "Epoch 83/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1338 - d_loss: 0.6413\n",
      "Epoch 84/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9571 - d_loss: 0.6475\n",
      "Epoch 85/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9510 - d_loss: 0.6789\n",
      "Epoch 86/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0331 - d_loss: 0.6456\n",
      "Epoch 87/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9986 - d_loss: 0.6376\n",
      "Epoch 88/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1135 - d_loss: 0.6311\n",
      "Epoch 89/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9697 - d_loss: 0.6467\n",
      "Epoch 90/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9523 - d_loss: 0.6421\n",
      "Epoch 91/500\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0282 - d_loss: 0.6960\n",
      "Epoch 92/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0269 - d_loss: 0.6564\n",
      "Epoch 93/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0248 - d_loss: 0.6741\n",
      "Epoch 94/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9493 - d_loss: 0.6669\n",
      "Epoch 95/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0669 - d_loss: 0.6671\n",
      "Epoch 96/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8801 - d_loss: 0.6758\n",
      "Epoch 97/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9062 - d_loss: 0.6777\n",
      "Epoch 98/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8653 - d_loss: 0.6727\n",
      "Epoch 99/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9335 - d_loss: 0.6616\n",
      "Epoch 100/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9632 - d_loss: 0.6491\n",
      "Epoch 101/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0391 - d_loss: 0.6353\n",
      "Epoch 102/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0158 - d_loss: 0.7381\n",
      "Epoch 103/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9761 - d_loss: 0.6522\n",
      "Epoch 104/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9309 - d_loss: 0.6682\n",
      "Epoch 105/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8833 - d_loss: 0.7298\n",
      "Epoch 106/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9332 - d_loss: 0.6694\n",
      "Epoch 107/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8616 - d_loss: 0.6669\n",
      "Epoch 108/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9282 - d_loss: 0.6534\n",
      "Epoch 109/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8844 - d_loss: 0.6773\n",
      "Epoch 110/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1456 - d_loss: 0.6981\n",
      "Epoch 111/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1170 - d_loss: 0.6363\n",
      "Epoch 112/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9697 - d_loss: 0.6297\n",
      "Epoch 113/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0001 - d_loss: 0.6683\n",
      "Epoch 114/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9694 - d_loss: 0.6702\n",
      "Epoch 115/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1854 - d_loss: 0.6518\n",
      "Epoch 116/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9382 - d_loss: 0.6747\n",
      "Epoch 117/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9202 - d_loss: 0.6677\n",
      "Epoch 118/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0058 - d_loss: 0.6457\n",
      "Epoch 119/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0412 - d_loss: 0.7121\n",
      "Epoch 120/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9616 - d_loss: 0.6456\n",
      "Epoch 121/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0251 - d_loss: 0.6276\n",
      "Epoch 122/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9824 - d_loss: 0.6635\n",
      "Epoch 123/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9133 - d_loss: 0.6522\n",
      "Epoch 124/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9104 - d_loss: 0.6637\n",
      "Epoch 125/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1291 - d_loss: 0.6949\n",
      "Epoch 126/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9944 - d_loss: 0.6434\n",
      "Epoch 127/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0903 - d_loss: 0.6684\n",
      "Epoch 128/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0744 - d_loss: 0.6993\n",
      "Epoch 129/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9840 - d_loss: 0.6375\n",
      "Epoch 130/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9578 - d_loss: 0.6510\n",
      "Epoch 131/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0541 - d_loss: 0.6929\n",
      "Epoch 132/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0047 - d_loss: 0.6809\n",
      "Epoch 133/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9461 - d_loss: 0.6992\n",
      "Epoch 134/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0269 - d_loss: 0.6783\n",
      "Epoch 135/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9237 - d_loss: 0.6583\n",
      "Epoch 136/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0375 - d_loss: 0.6573\n",
      "Epoch 137/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9298 - d_loss: 0.6597\n",
      "Epoch 138/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9179 - d_loss: 0.6745\n",
      "Epoch 139/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9597 - d_loss: 0.6698\n",
      "Epoch 140/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0248 - d_loss: 0.7111\n",
      "Epoch 141/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9373 - d_loss: 0.6858\n",
      "Epoch 142/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9740 - d_loss: 0.6418\n",
      "Epoch 143/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9635 - d_loss: 0.6519\n",
      "Epoch 144/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9847 - d_loss: 0.7173\n",
      "Epoch 145/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0179 - d_loss: 0.6205\n",
      "Epoch 146/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9916 - d_loss: 0.6724\n",
      "Epoch 147/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9741 - d_loss: 0.6313\n",
      "Epoch 148/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9171 - d_loss: 0.6543\n",
      "Epoch 149/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9837 - d_loss: 0.6400\n",
      "Epoch 150/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1442 - d_loss: 0.6488\n",
      "Epoch 151/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9836 - d_loss: 0.6458\n",
      "Epoch 152/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9048 - d_loss: 0.6752\n",
      "Epoch 153/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9015 - d_loss: 0.6573\n",
      "Epoch 154/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1475 - d_loss: 0.6163\n",
      "Epoch 155/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0115 - d_loss: 0.6442\n",
      "Epoch 156/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9809 - d_loss: 0.6468\n",
      "Epoch 157/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1037 - d_loss: 0.6475\n",
      "Epoch 158/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9584 - d_loss: 0.6669\n",
      "Epoch 159/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9704 - d_loss: 0.6477\n",
      "Epoch 160/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9598 - d_loss: 0.6554\n",
      "Epoch 161/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9927 - d_loss: 0.6571\n",
      "Epoch 162/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1476 - d_loss: 0.6236\n",
      "Epoch 163/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9887 - d_loss: 0.6483\n",
      "Epoch 164/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9276 - d_loss: 0.6695\n",
      "Epoch 165/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9600 - d_loss: 0.6991\n",
      "Epoch 166/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9047 - d_loss: 0.6816\n",
      "Epoch 167/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8966 - d_loss: 0.6880\n",
      "Epoch 168/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9295 - d_loss: 0.6570\n",
      "Epoch 169/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9634 - d_loss: 0.7173\n",
      "Epoch 170/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8784 - d_loss: 0.6598\n",
      "Epoch 171/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9164 - d_loss: 0.6719\n",
      "Epoch 172/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8586 - d_loss: 0.6789\n",
      "Epoch 173/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9580 - d_loss: 0.6645\n",
      "Epoch 174/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8954 - d_loss: 0.6664\n",
      "Epoch 175/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8734 - d_loss: 0.6510\n",
      "Epoch 176/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8780 - d_loss: 0.7047\n",
      "Epoch 177/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9786 - d_loss: 0.6532\n",
      "Epoch 178/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9351 - d_loss: 0.6700\n",
      "Epoch 179/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9550 - d_loss: 0.6687\n",
      "Epoch 180/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9433 - d_loss: 0.6422\n",
      "Epoch 181/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9318 - d_loss: 0.6654\n",
      "Epoch 182/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2123 - d_loss: 0.6503\n",
      "Epoch 183/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9818 - d_loss: 0.6566\n",
      "Epoch 184/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9227 - d_loss: 0.6824\n",
      "Epoch 185/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9824 - d_loss: 0.6908\n",
      "Epoch 186/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0486 - d_loss: 0.6344\n",
      "Epoch 187/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1044 - d_loss: 0.6447\n",
      "Epoch 188/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0471 - d_loss: 0.7441\n",
      "Epoch 189/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0090 - d_loss: 0.7291\n",
      "Epoch 190/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0814 - d_loss: 0.6435\n",
      "Epoch 191/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9297 - d_loss: 0.6524\n",
      "Epoch 192/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0405 - d_loss: 0.6402\n",
      "Epoch 193/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0114 - d_loss: 0.6347\n",
      "Epoch 194/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0614 - d_loss: 0.6386\n",
      "Epoch 195/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0177 - d_loss: 0.6587\n",
      "Epoch 196/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9660 - d_loss: 0.6835\n",
      "Epoch 197/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0417 - d_loss: 0.6693\n",
      "Epoch 198/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9062 - d_loss: 0.6790\n",
      "Epoch 199/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0278 - d_loss: 0.6320\n",
      "Epoch 200/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9906 - d_loss: 0.6685\n",
      "Epoch 201/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9968 - d_loss: 0.6653\n",
      "Epoch 202/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0718 - d_loss: 0.6268\n",
      "Epoch 203/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9869 - d_loss: 0.6458\n",
      "Epoch 204/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9611 - d_loss: 0.6472\n",
      "Epoch 205/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0289 - d_loss: 0.7580\n",
      "Epoch 206/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9848 - d_loss: 0.6638\n",
      "Epoch 207/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9347 - d_loss: 0.6986\n",
      "Epoch 208/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9309 - d_loss: 0.6482\n",
      "Epoch 209/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9999 - d_loss: 0.6316\n",
      "Epoch 210/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0942 - d_loss: 0.6921\n",
      "Epoch 211/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9557 - d_loss: 0.6516\n",
      "Epoch 212/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9824 - d_loss: 0.6664\n",
      "Epoch 213/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9603 - d_loss: 0.6444\n",
      "Epoch 214/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9744 - d_loss: 0.6778\n",
      "Epoch 215/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0048 - d_loss: 0.6534\n",
      "Epoch 216/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1110 - d_loss: 0.6477\n",
      "Epoch 217/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0020 - d_loss: 0.6998\n",
      "Epoch 218/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0136 - d_loss: 0.6183\n",
      "Epoch 219/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1169 - d_loss: 0.6283\n",
      "Epoch 220/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1293 - d_loss: 0.6181\n",
      "Epoch 221/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1197 - d_loss: 0.5794\n",
      "Epoch 222/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1786 - d_loss: 0.6287\n",
      "Epoch 223/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0736 - d_loss: 0.6026\n",
      "Epoch 224/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1633 - d_loss: 0.6068\n",
      "Epoch 225/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1699 - d_loss: 0.5848\n",
      "Epoch 226/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0095 - d_loss: 0.6718\n",
      "Epoch 227/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1351 - d_loss: 0.6865\n",
      "Epoch 228/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9529 - d_loss: 0.6537\n",
      "Epoch 229/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8976 - d_loss: 0.6528\n",
      "Epoch 230/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9438 - d_loss: 0.6462\n",
      "Epoch 231/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9823 - d_loss: 0.6715\n",
      "Epoch 232/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9423 - d_loss: 0.6984\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9805 - d_loss: 0.6541\n",
      "Epoch 234/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9782 - d_loss: 0.6915\n",
      "Epoch 235/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9376 - d_loss: 0.6900\n",
      "Epoch 236/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9514 - d_loss: 0.6502\n",
      "Epoch 237/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9598 - d_loss: 0.6731\n",
      "Epoch 238/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0028 - d_loss: 0.6461\n",
      "Epoch 239/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9560 - d_loss: 0.6753\n",
      "Epoch 240/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9884 - d_loss: 0.6876\n",
      "Epoch 241/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9948 - d_loss: 0.7080\n",
      "Epoch 242/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0021 - d_loss: 0.6416\n",
      "Epoch 243/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0227 - d_loss: 0.6159\n",
      "Epoch 244/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0353 - d_loss: 0.7141\n",
      "Epoch 245/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0022 - d_loss: 0.6685\n",
      "Epoch 246/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9753 - d_loss: 0.6313\n",
      "Epoch 247/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9538 - d_loss: 0.6697\n",
      "Epoch 248/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9659 - d_loss: 0.6092\n",
      "Epoch 249/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9465 - d_loss: 0.6482\n",
      "Epoch 250/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.2381 - d_loss: 0.6428\n",
      "Epoch 251/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9537 - d_loss: 0.6202\n",
      "Epoch 252/500\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 0.9609 - d_loss: 0.6501\n",
      "Epoch 253/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0725 - d_loss: 0.6577\n",
      "Epoch 254/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9510 - d_loss: 0.6255\n",
      "Epoch 255/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0892 - d_loss: 0.6088\n",
      "Epoch 256/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9849 - d_loss: 0.6468\n",
      "Epoch 257/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9583 - d_loss: 0.6306\n",
      "Epoch 258/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1404 - d_loss: 0.6717\n",
      "Epoch 259/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0061 - d_loss: 0.6160\n",
      "Epoch 260/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0872 - d_loss: 0.5965\n",
      "Epoch 261/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0062 - d_loss: 0.6700\n",
      "Epoch 262/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9418 - d_loss: 0.6582\n",
      "Epoch 263/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1663 - d_loss: 0.6138\n",
      "Epoch 264/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0729 - d_loss: 0.6378\n",
      "Epoch 265/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0765 - d_loss: 0.6104\n",
      "Epoch 266/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.3093 - d_loss: 0.5984\n",
      "Epoch 267/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9966 - d_loss: 0.6205\n",
      "Epoch 268/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9472 - d_loss: 0.6708\n",
      "Epoch 269/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9494 - d_loss: 0.6421\n",
      "Epoch 270/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9542 - d_loss: 0.6492\n",
      "Epoch 271/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9330 - d_loss: 0.6570\n",
      "Epoch 272/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9260 - d_loss: 0.6386\n",
      "Epoch 273/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0306 - d_loss: 0.6775\n",
      "Epoch 274/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0843 - d_loss: 0.6169\n",
      "Epoch 275/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9096 - d_loss: 0.6424\n",
      "Epoch 276/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0109 - d_loss: 0.6180\n",
      "Epoch 277/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0414 - d_loss: 0.6511\n",
      "Epoch 278/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0139 - d_loss: 0.6244\n",
      "Epoch 279/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9772 - d_loss: 0.6322\n",
      "Epoch 280/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9739 - d_loss: 0.6362\n",
      "Epoch 281/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0052 - d_loss: 0.6466\n",
      "Epoch 282/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0662 - d_loss: 0.6673\n",
      "Epoch 283/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9547 - d_loss: 0.6836\n",
      "Epoch 284/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0497 - d_loss: 0.6761\n",
      "Epoch 285/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2075 - d_loss: 0.6810\n",
      "Epoch 286/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0669 - d_loss: 0.6358\n",
      "Epoch 287/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9376 - d_loss: 0.6486\n",
      "Epoch 288/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9387 - d_loss: 0.6544\n",
      "Epoch 289/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9251 - d_loss: 0.6772\n",
      "Epoch 290/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1846 - d_loss: 0.6064\n",
      "Epoch 291/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9492 - d_loss: 0.6407\n",
      "Epoch 292/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9226 - d_loss: 0.6410\n",
      "Epoch 293/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8828 - d_loss: 0.6885\n",
      "Epoch 294/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9524 - d_loss: 0.7133\n",
      "Epoch 295/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9333 - d_loss: 0.6535\n",
      "Epoch 296/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9928 - d_loss: 0.6973\n",
      "Epoch 297/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9080 - d_loss: 0.6587\n",
      "Epoch 298/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9420 - d_loss: 0.6870\n",
      "Epoch 299/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9316 - d_loss: 0.6335\n",
      "Epoch 300/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8732 - d_loss: 0.6631\n",
      "Epoch 301/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9549 - d_loss: 0.6528\n",
      "Epoch 302/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9779 - d_loss: 0.6374\n",
      "Epoch 303/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0844 - d_loss: 0.6238\n",
      "Epoch 304/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8707 - d_loss: 0.6916\n",
      "Epoch 305/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8708 - d_loss: 0.6921\n",
      "Epoch 306/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8839 - d_loss: 0.6584\n",
      "Epoch 307/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8427 - d_loss: 0.6695\n",
      "Epoch 308/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0462 - d_loss: 0.6720\n",
      "Epoch 309/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9243 - d_loss: 0.6711\n",
      "Epoch 310/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8908 - d_loss: 0.6523\n",
      "Epoch 311/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9153 - d_loss: 0.6620\n",
      "Epoch 312/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9304 - d_loss: 0.6602\n",
      "Epoch 313/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9682 - d_loss: 0.6573\n",
      "Epoch 314/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0299 - d_loss: 0.6149\n",
      "Epoch 315/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1641 - d_loss: 0.6422\n",
      "Epoch 316/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1213 - d_loss: 0.5423\n",
      "Epoch 317/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8919 - d_loss: 0.6680\n",
      "Epoch 318/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8821 - d_loss: 0.6363\n",
      "Epoch 319/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9876 - d_loss: 0.6242\n",
      "Epoch 320/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9747 - d_loss: 0.7101\n",
      "Epoch 321/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9496 - d_loss: 0.6577\n",
      "Epoch 322/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9242 - d_loss: 0.6515\n",
      "Epoch 323/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8843 - d_loss: 0.6517\n",
      "Epoch 324/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9068 - d_loss: 0.6585\n",
      "Epoch 325/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9747 - d_loss: 0.6306\n",
      "Epoch 326/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0914 - d_loss: 0.6539\n",
      "Epoch 327/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9520 - d_loss: 0.6458\n",
      "Epoch 328/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9580 - d_loss: 0.6481\n",
      "Epoch 329/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0075 - d_loss: 0.6485\n",
      "Epoch 330/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9834 - d_loss: 0.6963\n",
      "Epoch 331/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9690 - d_loss: 0.6672\n",
      "Epoch 332/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9896 - d_loss: 0.6736\n",
      "Epoch 333/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0351 - d_loss: 0.6011\n",
      "Epoch 334/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9761 - d_loss: 0.6852\n",
      "Epoch 335/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9526 - d_loss: 0.6036\n",
      "Epoch 336/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0985 - d_loss: 0.6048\n",
      "Epoch 337/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9267 - d_loss: 0.6692\n",
      "Epoch 338/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9607 - d_loss: 0.6260\n",
      "Epoch 339/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0138 - d_loss: 0.6244\n",
      "Epoch 340/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9781 - d_loss: 0.6454\n",
      "Epoch 341/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9008 - d_loss: 0.6408\n",
      "Epoch 342/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9324 - d_loss: 0.6424\n",
      "Epoch 343/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9338 - d_loss: 0.6622\n",
      "Epoch 344/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0196 - d_loss: 0.6574\n",
      "Epoch 345/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9736 - d_loss: 0.6404\n",
      "Epoch 346/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0156 - d_loss: 0.6965\n",
      "Epoch 347/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0915 - d_loss: 0.5907\n",
      "Epoch 348/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1536 - d_loss: 0.6028\n",
      "Epoch 349/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1128 - d_loss: 0.6390\n",
      "Epoch 350/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0729 - d_loss: 0.6168\n",
      "Epoch 351/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1299 - d_loss: 0.6053\n",
      "Epoch 352/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0619 - d_loss: 0.5963\n",
      "Epoch 353/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0961 - d_loss: 0.5812\n",
      "Epoch 354/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0448 - d_loss: 0.6036\n",
      "Epoch 355/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0630 - d_loss: 0.6159\n",
      "Epoch 356/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0697 - d_loss: 0.6383\n",
      "Epoch 357/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9974 - d_loss: 0.6162\n",
      "Epoch 358/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9812 - d_loss: 0.6549\n",
      "Epoch 359/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9214 - d_loss: 0.7025\n",
      "Epoch 360/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0236 - d_loss: 0.6431\n",
      "Epoch 361/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9619 - d_loss: 0.6489\n",
      "Epoch 362/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9817 - d_loss: 0.6393\n",
      "Epoch 363/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9578 - d_loss: 0.6680\n",
      "Epoch 364/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8902 - d_loss: 0.6495\n",
      "Epoch 365/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9259 - d_loss: 0.6418\n",
      "Epoch 366/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.8960 - d_loss: 0.6587\n",
      "Epoch 367/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0814 - d_loss: 0.6152\n",
      "Epoch 368/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9691 - d_loss: 0.6380\n",
      "Epoch 369/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1035 - d_loss: 0.6929\n",
      "Epoch 370/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0127 - d_loss: 0.7430\n",
      "Epoch 371/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0261 - d_loss: 0.6929\n",
      "Epoch 372/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9091 - d_loss: 0.6528\n",
      "Epoch 373/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9779 - d_loss: 0.6218\n",
      "Epoch 374/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0337 - d_loss: 0.6193\n",
      "Epoch 375/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0275 - d_loss: 0.6120\n",
      "Epoch 376/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2094 - d_loss: 0.6460\n",
      "Epoch 377/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1567 - d_loss: 0.5818\n",
      "Epoch 378/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0011 - d_loss: 0.6256\n",
      "Epoch 379/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9795 - d_loss: 0.6385\n",
      "Epoch 380/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0381 - d_loss: 0.6317\n",
      "Epoch 381/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0056 - d_loss: 0.6603\n",
      "Epoch 382/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0133 - d_loss: 0.6517\n",
      "Epoch 383/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0302 - d_loss: 0.6399\n",
      "Epoch 384/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8958 - d_loss: 0.6535\n",
      "Epoch 385/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0321 - d_loss: 0.6366\n",
      "Epoch 386/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9610 - d_loss: 0.6533\n",
      "Epoch 387/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9241 - d_loss: 0.6323\n",
      "Epoch 388/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9910 - d_loss: 0.6705\n",
      "Epoch 389/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9649 - d_loss: 0.7069\n",
      "Epoch 390/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9181 - d_loss: 0.6166\n",
      "Epoch 391/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9948 - d_loss: 0.6049\n",
      "Epoch 392/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9691 - d_loss: 0.6242\n",
      "Epoch 393/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0241 - d_loss: 0.5985\n",
      "Epoch 394/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0746 - d_loss: 0.6357\n",
      "Epoch 395/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1217 - d_loss: 0.6441\n",
      "Epoch 396/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0741 - d_loss: 0.6108\n",
      "Epoch 397/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0320 - d_loss: 0.6112\n",
      "Epoch 398/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9635 - d_loss: 0.6114\n",
      "Epoch 399/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9754 - d_loss: 0.6502\n",
      "Epoch 400/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9089 - d_loss: 0.6360\n",
      "Epoch 401/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8818 - d_loss: 0.6400\n",
      "Epoch 402/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9493 - d_loss: 0.6686\n",
      "Epoch 403/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9173 - d_loss: 0.6925\n",
      "Epoch 404/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9978 - d_loss: 0.6357\n",
      "Epoch 405/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9635 - d_loss: 0.6376\n",
      "Epoch 406/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9402 - d_loss: 0.6794\n",
      "Epoch 407/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9314 - d_loss: 0.6253\n",
      "Epoch 408/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9211 - d_loss: 0.6257\n",
      "Epoch 409/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0044 - d_loss: 0.6102\n",
      "Epoch 410/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.1306 - d_loss: 0.6373\n",
      "Epoch 411/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0243 - d_loss: 0.6212\n",
      "Epoch 412/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9740 - d_loss: 0.6514\n",
      "Epoch 413/500\n",
      "196/196 [==============================] - 23s 118ms/step - g_loss: 1.0223 - d_loss: 0.6512\n",
      "Epoch 414/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9754 - d_loss: 0.6255\n",
      "Epoch 415/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9293 - d_loss: 0.6227\n",
      "Epoch 416/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9727 - d_loss: 0.6359\n",
      "Epoch 417/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9089 - d_loss: 0.6546\n",
      "Epoch 418/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9680 - d_loss: 0.6540\n",
      "Epoch 419/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9799 - d_loss: 0.6497\n",
      "Epoch 420/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0005 - d_loss: 0.6073\n",
      "Epoch 421/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0788 - d_loss: 0.6076\n",
      "Epoch 422/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9531 - d_loss: 0.6351\n",
      "Epoch 423/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0206 - d_loss: 0.6642\n",
      "Epoch 424/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0000 - d_loss: 0.6223\n",
      "Epoch 425/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9607 - d_loss: 0.6375\n",
      "Epoch 426/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0362 - d_loss: 0.5964\n",
      "Epoch 427/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9598 - d_loss: 0.6311\n",
      "Epoch 428/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0255 - d_loss: 0.6106\n",
      "Epoch 429/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9761 - d_loss: 0.6106\n",
      "Epoch 430/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8762 - d_loss: 0.6781\n",
      "Epoch 431/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9110 - d_loss: 0.6539\n",
      "Epoch 432/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9103 - d_loss: 0.6304\n",
      "Epoch 433/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9307 - d_loss: 0.6369\n",
      "Epoch 434/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8960 - d_loss: 0.6345\n",
      "Epoch 435/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9605 - d_loss: 0.6315\n",
      "Epoch 436/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9209 - d_loss: 0.6325\n",
      "Epoch 437/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8646 - d_loss: 0.6584\n",
      "Epoch 438/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9162 - d_loss: 0.6315\n",
      "Epoch 439/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9038 - d_loss: 0.6542\n",
      "Epoch 440/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2285 - d_loss: 0.5930\n",
      "Epoch 441/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.3314 - d_loss: 0.5335\n",
      "Epoch 442/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9806 - d_loss: 0.6295\n",
      "Epoch 443/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8857 - d_loss: 0.6416\n",
      "Epoch 444/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9119 - d_loss: 0.6344\n",
      "Epoch 445/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8851 - d_loss: 0.6466\n",
      "Epoch 446/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9619 - d_loss: 0.5931\n",
      "Epoch 447/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9409 - d_loss: 0.6331\n",
      "Epoch 448/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9290 - d_loss: 0.6262\n",
      "Epoch 449/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0568 - d_loss: 0.6150\n",
      "Epoch 450/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8816 - d_loss: 0.6536\n",
      "Epoch 451/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0080 - d_loss: 0.6454\n",
      "Epoch 452/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9580 - d_loss: 0.6064\n",
      "Epoch 453/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9438 - d_loss: 0.6347\n",
      "Epoch 454/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1397 - d_loss: 0.6413\n",
      "Epoch 455/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8879 - d_loss: 0.6386\n",
      "Epoch 456/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8843 - d_loss: 0.6492\n",
      "Epoch 457/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8824 - d_loss: 0.6570\n",
      "Epoch 458/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9006 - d_loss: 0.6495\n",
      "Epoch 459/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0029 - d_loss: 0.6244\n",
      "Epoch 460/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9586 - d_loss: 0.7561\n",
      "Epoch 461/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0308 - d_loss: 0.6601\n",
      "Epoch 462/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1095 - d_loss: 0.7272\n",
      "Epoch 463/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0776 - d_loss: 0.6351\n",
      "Epoch 464/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9687 - d_loss: 0.6307\n",
      "Epoch 465/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9528 - d_loss: 0.6360\n",
      "Epoch 466/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0148 - d_loss: 0.5966\n",
      "Epoch 467/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9435 - d_loss: 0.6234\n",
      "Epoch 468/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9488 - d_loss: 0.6296\n",
      "Epoch 469/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9252 - d_loss: 0.6442\n",
      "Epoch 470/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0521 - d_loss: 0.6149\n",
      "Epoch 471/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8845 - d_loss: 0.6641\n",
      "Epoch 472/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9465 - d_loss: 0.6186\n",
      "Epoch 473/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0599 - d_loss: 0.6225\n",
      "Epoch 474/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0515 - d_loss: 0.6030\n",
      "Epoch 475/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0375 - d_loss: 0.6217\n",
      "Epoch 476/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9748 - d_loss: 0.6561\n",
      "Epoch 477/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9366 - d_loss: 0.6433\n",
      "Epoch 478/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0018 - d_loss: 0.5920\n",
      "Epoch 479/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9297 - d_loss: 0.6477\n",
      "Epoch 480/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9212 - d_loss: 0.6190\n",
      "Epoch 481/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9616 - d_loss: 0.6214\n",
      "Epoch 482/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0100 - d_loss: 0.6028\n",
      "Epoch 483/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9890 - d_loss: 0.6274\n",
      "Epoch 484/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 1.0328 - d_loss: 0.6409\n",
      "Epoch 485/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9972 - d_loss: 0.6636\n",
      "Epoch 486/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.2037 - d_loss: 0.6472\n",
      "Epoch 487/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.1460 - d_loss: 0.6866\n",
      "Epoch 488/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9494 - d_loss: 0.6527\n",
      "Epoch 489/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9762 - d_loss: 0.6208\n",
      "Epoch 490/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0554 - d_loss: 0.6505\n",
      "Epoch 491/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0891 - d_loss: 0.6450\n",
      "Epoch 492/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9127 - d_loss: 0.6329\n",
      "Epoch 493/500\n",
      "196/196 [==============================] - 22s 113ms/step - g_loss: 0.9169 - d_loss: 0.6379\n",
      "Epoch 494/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0153 - d_loss: 0.6322\n",
      "Epoch 495/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.8908 - d_loss: 0.6843\n",
      "Epoch 496/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9581 - d_loss: 0.6170\n",
      "Epoch 497/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9620 - d_loss: 0.6375\n",
      "Epoch 498/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0275 - d_loss: 0.6484\n",
      "Epoch 499/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 1.0519 - d_loss: 0.5466\n",
      "Epoch 500/500\n",
      "196/196 [==============================] - 22s 114ms/step - g_loss: 0.9578 - d_loss: 0.6381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24a5c954790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_gan = ConditionalGAN(\n",
    "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    ")\n",
    "cond_gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "cond_gan.fit(dataset, epochs=epoch_t, \n",
    "        callbacks=GANMonitor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6263e9",
   "metadata": {},
   "source": [
    "# Create new training images using the Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a0397cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first extract the trained generator from our Conditiona GAN.\n",
    "trained_gen = cond_gan.generator\n",
    "\n",
    "# Number of images that are generated per class\n",
    "num_interpolation = 15000  # @param {type:\"integer\"}\n",
    "\n",
    "# Sample noise for the interpolation.\n",
    "interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    # Convert the start and end labels to one-hot encoded vectors.\n",
    "    first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "    second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "    first_label = tf.cast(first_label, tf.float32)\n",
    "    second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels.\n",
    "    percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "    percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "    interpolation_labels = (\n",
    "        first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "    )\n",
    "\n",
    "    # Combine the noise and the labels and run inference with the generator.\n",
    "    noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "    fake = trained_gen.predict(noise_and_labels)\n",
    "    return fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directory for saving folder\n",
    "os.makedirs(path_save_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0de5e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve class name based on number\n",
    "classes_list = list(prelim_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79dfad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create images for every class and store in seperate folder\n",
    "for i in range(num_classes):\n",
    "    class_name = classes_list[i]\n",
    "    class_dir = f\"{path_save_imgs}/{class_name}\"\n",
    "    os.makedirs(class_dir)\n",
    "    start_class = i\n",
    "    end_class = i\n",
    "    fake_images = interpolate_class(start_class, end_class)\n",
    "    fake_images *= 255\n",
    "    converted_images = fake_images.astype(np.uint8)\n",
    "    converted_images = tf.image.resize(converted_images, (64, 64)).numpy().astype(np.uint8)\n",
    "    for j in range(num_interpolation):\n",
    "        np_array = np.squeeze(converted_images[j], axis=2)\n",
    "        im = Image.fromarray((np_array))\n",
    "        im.save(f\"{class_dir}/gen_imgs_{class_name}_{j}.png\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
