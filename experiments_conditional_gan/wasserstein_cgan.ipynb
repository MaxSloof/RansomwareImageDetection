{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ed78776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f7fbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 60000\n",
      "Shape of the images in the dataset: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (64, 64, 1)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "#fashion_mnist = keras.datasets.fashion_mnist\n",
    "#(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print(f\"Number of examples: {len(train_images)}\")\n",
    "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
    "\n",
    "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
    "#train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
    "#train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0918ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = \"C:/Users/Max/Documents/image_data/data_wo_benign\"\n",
    "path_save_imgs = f\"C:/Users/Max/Documents/image_data/WASSERSTEIN-cgan-local-v001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdb76e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12536 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator()\n",
    "\n",
    "prelim_dataset = datagen.flow_from_directory(\n",
    "    directory = path_root,\n",
    "    color_mode = 'grayscale',\n",
    "    target_size = (64,64),\n",
    "    interpolation = 'bicubic',\n",
    "    batch_size = 40000,\n",
    "    shuffle=True\n",
    ")\n",
    "imgs, labels = next(prelim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad4c32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = (imgs - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54ca2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_argmax = np.argmax(labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3339933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 68, 68, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 34, 34, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 17, 17, 128)       204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 9, 9, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 5, 5, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 12801     \n",
      "=================================================================\n",
      "Total params: 4,316,161\n",
      "Trainable params: 4,316,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f509217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 8, 8, 128)         294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 16, 16, 64)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_22 (UpSampling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 32, 32, 32)        18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_23 (UpSampling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 64, 64, 1)         288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 64, 64, 1)         4         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 928,932\n",
      "Trainable params: 920,290\n",
      "Non-trainable params: 8,642\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        32,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "   \n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a51ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "34b0e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, family_name, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.family_name = family_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 99 == 0:\n",
    "            random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "            generated_images = self.model.generator(random_latent_vectors)\n",
    "            generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "            for i in range(self.num_img):\n",
    "                img = generated_images[i].numpy()\n",
    "                img = keras.preprocessing.image.array_to_img(img)\n",
    "                img.save(\"C:/Users/Max/Documents/image_data/wasserstein_cgan_images/{family_name}/gen_img_{i}_{epoch}.png\".format(family_name = self.family_name, i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b958153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278b011",
   "metadata": {},
   "source": [
    "STILL NEEDS TO BE COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b58141bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_count(family):\n",
    "    count = len(os.listdir(f\"C:/Users/Max/Documents/image_data/cgan-ckpt-v009-thirdsize/{family}\"))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94427d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on BetterSurf\n",
      "Epoch 1/200\n",
      "20/20 [==============================] - 19s 688ms/step - d_loss: 95.2256 - g_loss: 622.2082\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - 14s 698ms/step - d_loss: 109.3151 - g_loss: 363.9856\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - 14s 697ms/step - d_loss: -135.5859 - g_loss: 535.0873\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - 14s 703ms/step - d_loss: -99.5740 - g_loss: 674.7309\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - 14s 699ms/step - d_loss: 12.1983 - g_loss: 791.8288\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - 14s 702ms/step - d_loss: -103.3924 - g_loss: 612.1865\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - 14s 702ms/step - d_loss: -355.6289 - g_loss: 872.6812\n",
      "Epoch 8/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -68.4045 - g_loss: 1041.0959\n",
      "Epoch 9/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 63.7075 - g_loss: 1146.3197\n",
      "Epoch 10/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -26.6777 - g_loss: 1010.6682\n",
      "Epoch 11/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 28.2763 - g_loss: 969.2454\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 86.1799 - g_loss: 935.0156\n",
      "Epoch 13/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -19.8699 - g_loss: 1082.5058\n",
      "Epoch 14/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -79.9054 - g_loss: 1209.8422\n",
      "Epoch 15/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 18.7410 - g_loss: 995.5101\n",
      "Epoch 16/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: 95.4784 - g_loss: 1192.5773\n",
      "Epoch 17/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: 86.1205 - g_loss: 1571.0371\n",
      "Epoch 18/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 19.9074 - g_loss: 1265.5889\n",
      "Epoch 19/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -31.2850 - g_loss: 1128.6050\n",
      "Epoch 20/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 147.2537 - g_loss: 1369.2307\n",
      "Epoch 21/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: 95.7678 - g_loss: 1459.7622\n",
      "Epoch 22/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 58.3376 - g_loss: 1946.1950\n",
      "Epoch 23/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -226.0888 - g_loss: 1692.3813\n",
      "Epoch 24/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -88.8078 - g_loss: 2290.4420\n",
      "Epoch 25/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -146.3219 - g_loss: 2754.1356\n",
      "Epoch 26/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -97.8968 - g_loss: 2151.4041\n",
      "Epoch 27/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 47.9173 - g_loss: 1799.5839\n",
      "Epoch 28/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 36.5376 - g_loss: 1508.3922\n",
      "Epoch 29/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -376.8074 - g_loss: 1509.6995\n",
      "Epoch 30/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 0.6490 - g_loss: 1653.8506\n",
      "Epoch 31/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -166.6045 - g_loss: 1671.8941\n",
      "Epoch 32/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 42.0089 - g_loss: 1584.9202\n",
      "Epoch 33/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 40.4167 - g_loss: 1498.5072\n",
      "Epoch 34/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -94.9702 - g_loss: 851.5182\n",
      "Epoch 35/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -590.7992 - g_loss: 373.8090\n",
      "Epoch 36/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -440.1288 - g_loss: 1237.9379\n",
      "Epoch 37/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -289.6381 - g_loss: 486.1069\n",
      "Epoch 38/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 466.1410 - g_loss: 500.6593\n",
      "Epoch 39/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -20.7854 - g_loss: 851.6745\n",
      "Epoch 40/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -312.4846 - g_loss: 452.2043\n",
      "Epoch 41/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -290.7923 - g_loss: 458.0347\n",
      "Epoch 42/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 143.1816 - g_loss: 869.2324\n",
      "Epoch 43/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 160.8482 - g_loss: 545.3814\n",
      "Epoch 44/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 147.8578 - g_loss: 313.3111\n",
      "Epoch 45/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -228.0929 - g_loss: 30.4347\n",
      "Epoch 46/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 329.9994 - g_loss: -551.4309\n",
      "Epoch 47/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 229.2441 - g_loss: -246.2643\n",
      "Epoch 48/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 2.2347 - g_loss: -356.7434\n",
      "Epoch 49/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -172.7152 - g_loss: -135.9221\n",
      "Epoch 50/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 111.6786 - g_loss: -187.5864\n",
      "Epoch 51/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 38.1147 - g_loss: -711.2272\n",
      "Epoch 52/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -240.0589 - g_loss: -679.9366\n",
      "Epoch 53/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 197.0762 - g_loss: -530.0767\n",
      "Epoch 54/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 36.0199 - g_loss: -428.4208\n",
      "Epoch 55/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 44.7279 - g_loss: -265.3343\n",
      "Epoch 56/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -174.3981 - g_loss: -120.7927\n",
      "Epoch 57/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 155.8491 - g_loss: -388.1213\n",
      "Epoch 58/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 36.2223 - g_loss: -72.8105\n",
      "Epoch 59/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -59.0906 - g_loss: -412.1480\n",
      "Epoch 60/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 124.6204 - g_loss: -396.0197\n",
      "Epoch 61/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -160.9240 - g_loss: 83.4641\n",
      "Epoch 62/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 17.1245 - g_loss: 540.6101\n",
      "Epoch 63/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 87.4705 - g_loss: 423.2279\n",
      "Epoch 64/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -95.0549 - g_loss: -31.0488\n",
      "Epoch 65/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 92.6248 - g_loss: -37.9252\n",
      "Epoch 66/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -113.8190 - g_loss: -181.2105\n",
      "Epoch 67/200\n",
      "20/20 [==============================] - 14s 704ms/step - d_loss: -59.0099 - g_loss: -59.6651\n",
      "Epoch 68/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 57.4597 - g_loss: -744.7611\n",
      "Epoch 69/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -196.0662 - g_loss: -598.5622\n",
      "Epoch 70/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -118.7884 - g_loss: -235.8290\n",
      "Epoch 71/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 99.7646 - g_loss: -25.4506\n",
      "Epoch 72/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 582.1899 - g_loss: -763.2558\n",
      "Epoch 73/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -88.4080 - g_loss: -276.7395\n",
      "Epoch 74/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -26.6310 - g_loss: 44.6769\n",
      "Epoch 75/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 374.5595 - g_loss: 169.3702\n",
      "Epoch 76/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -68.7091 - g_loss: 516.9145\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 14s 706ms/step - d_loss: -283.4211 - g_loss: -285.5864\n",
      "Epoch 78/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -75.7635 - g_loss: -487.6349\n",
      "Epoch 79/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 98.3387 - g_loss: -559.8998\n",
      "Epoch 80/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -474.2477 - g_loss: -914.7712\n",
      "Epoch 81/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -158.0249 - g_loss: -1174.0112\n",
      "Epoch 82/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -216.3916 - g_loss: -1030.1143\n",
      "Epoch 83/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -2.6156 - g_loss: -945.7177\n",
      "Epoch 84/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 548.2572 - g_loss: 144.9048\n",
      "Epoch 85/200\n",
      "20/20 [==============================] - 14s 704ms/step - d_loss: 33.4956 - g_loss: 429.8827\n",
      "Epoch 86/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 5.1081 - g_loss: -84.5657\n",
      "Epoch 87/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 458.9658 - g_loss: 134.3795\n",
      "Epoch 88/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 341.2528 - g_loss: 538.9387\n",
      "Epoch 89/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 80.2515 - g_loss: 660.6563\n",
      "Epoch 90/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -18.8545 - g_loss: 390.3576\n",
      "Epoch 91/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -437.2171 - g_loss: -28.4123\n",
      "Epoch 92/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 67.5030 - g_loss: -56.3315\n",
      "Epoch 93/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 185.2853 - g_loss: -44.2262\n",
      "Epoch 94/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 50.7001 - g_loss: 133.1721\n",
      "Epoch 95/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 15.2071 - g_loss: -27.2439\n",
      "Epoch 96/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: -74.8311 - g_loss: 329.5519\n",
      "Epoch 97/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 53.8955 - g_loss: 747.6251\n",
      "Epoch 98/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 2.8274 - g_loss: 238.1445\n",
      "Epoch 99/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 128.8807 - g_loss: 182.8352\n",
      "Epoch 100/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -63.7171 - g_loss: 41.4321\n",
      "Epoch 101/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 201.3575 - g_loss: 388.0401\n",
      "Epoch 102/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -36.4450 - g_loss: 225.4730\n",
      "Epoch 103/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -109.6620 - g_loss: 451.7347\n",
      "Epoch 104/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 47.1694 - g_loss: 476.4918\n",
      "Epoch 105/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -482.3527 - g_loss: 214.8135\n",
      "Epoch 106/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 76.4413 - g_loss: -66.1108\n",
      "Epoch 107/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 418.4189 - g_loss: -146.8477\n",
      "Epoch 108/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 135.2106 - g_loss: -113.2420\n",
      "Epoch 109/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 249.8238 - g_loss: 680.9845\n",
      "Epoch 110/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -37.1130 - g_loss: 255.8979\n",
      "Epoch 111/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 241.9348 - g_loss: 849.1207\n",
      "Epoch 112/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 78.2551 - g_loss: 577.0706\n",
      "Epoch 113/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -15.4364 - g_loss: 466.5528\n",
      "Epoch 114/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -72.1678 - g_loss: 373.7198\n",
      "Epoch 115/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -50.0302 - g_loss: 291.1115\n",
      "Epoch 116/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 41.5957 - g_loss: 344.1491\n",
      "Epoch 117/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 28.5680 - g_loss: 348.0449\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -24.9085 - g_loss: 375.7530\n",
      "Epoch 119/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: -23.5425 - g_loss: 426.4048\n",
      "Epoch 120/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -23.9439 - g_loss: 365.7864\n",
      "Epoch 121/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -15.8454 - g_loss: 257.4280\n",
      "Epoch 122/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -22.8688 - g_loss: 225.9612\n",
      "Epoch 123/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 20.7844 - g_loss: 332.3525\n",
      "Epoch 124/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 12.0860 - g_loss: 304.9298\n",
      "Epoch 125/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -22.6233 - g_loss: 316.9819\n",
      "Epoch 126/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 17.9029 - g_loss: 105.0801\n",
      "Epoch 127/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 41.0400 - g_loss: 110.9821\n",
      "Epoch 128/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 41.0861 - g_loss: 193.4653\n",
      "Epoch 129/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 7.9213 - g_loss: 145.5528\n",
      "Epoch 130/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 1.0465 - g_loss: 237.1456\n",
      "Epoch 131/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 4.2507 - g_loss: 74.5080\n",
      "Epoch 132/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -7.6950 - g_loss: 121.8801\n",
      "Epoch 133/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -4.6457 - g_loss: 122.9124\n",
      "Epoch 134/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -20.6403 - g_loss: 79.4444\n",
      "Epoch 135/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -34.2620 - g_loss: -48.4511\n",
      "Epoch 136/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: 16.2674 - g_loss: 83.4241\n",
      "Epoch 137/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 30.3409 - g_loss: 345.0557\n",
      "Epoch 138/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 49.0040 - g_loss: 426.4181\n",
      "Epoch 139/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 9.5681 - g_loss: 278.2940\n",
      "Epoch 140/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -0.6099 - g_loss: 228.5616\n",
      "Epoch 141/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 6.5120 - g_loss: 182.0406\n",
      "Epoch 142/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 5.3807 - g_loss: 165.6397\n",
      "Epoch 143/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -5.8509 - g_loss: 87.0134\n",
      "Epoch 144/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -16.0826 - g_loss: 67.9684\n",
      "Epoch 145/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 1.2052 - g_loss: 25.3408\n",
      "Epoch 146/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -1.1331 - g_loss: 66.2064\n",
      "Epoch 147/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 3.5106 - g_loss: 55.4376\n",
      "Epoch 148/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 2.6215 - g_loss: 90.3284\n",
      "Epoch 149/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -15.5743 - g_loss: 67.7462\n",
      "Epoch 150/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -16.3482 - g_loss: 157.1336\n",
      "Epoch 151/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 10.8211 - g_loss: 230.8164\n",
      "Epoch 152/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 20.0133 - g_loss: 342.7650\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 14s 706ms/step - d_loss: 10.5137 - g_loss: 212.7489\n",
      "Epoch 154/200\n",
      "20/20 [==============================] - 14s 704ms/step - d_loss: 8.5801 - g_loss: 251.8203\n",
      "Epoch 155/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 14.1044 - g_loss: 303.4627\n",
      "Epoch 156/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 15.7954 - g_loss: 325.7654\n",
      "Epoch 157/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 5.0598 - g_loss: 271.3721\n",
      "Epoch 158/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 4.1554 - g_loss: 205.7113\n",
      "Epoch 159/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -1.1005 - g_loss: 211.0484\n",
      "Epoch 160/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 1.2863 - g_loss: 241.9559\n",
      "Epoch 161/200\n",
      "20/20 [==============================] - 14s 708ms/step - d_loss: -10.7553 - g_loss: 164.4356\n",
      "Epoch 162/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 19.6820 - g_loss: 237.9028\n",
      "Epoch 163/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -50.1074 - g_loss: 172.0830\n",
      "Epoch 164/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 62.6979 - g_loss: 203.7593\n",
      "Epoch 165/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -29.2021 - g_loss: 206.3516\n",
      "Epoch 166/200\n",
      "20/20 [==============================] - 14s 712ms/step - d_loss: 17.1185 - g_loss: 240.2471\n",
      "Epoch 167/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 5.5962 - g_loss: 222.6409\n",
      "Epoch 168/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 1.5621 - g_loss: 200.3194\n",
      "Epoch 169/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -15.5513 - g_loss: 254.2224\n",
      "Epoch 170/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -46.4468 - g_loss: 145.5109\n",
      "Epoch 171/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -1.1986 - g_loss: 207.6071\n",
      "Epoch 172/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 11.0939 - g_loss: 211.4043\n",
      "Epoch 173/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -37.1564 - g_loss: 248.2613\n",
      "Epoch 174/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -24.8398 - g_loss: 280.0871\n",
      "Epoch 175/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 18.0635 - g_loss: 242.6592\n",
      "Epoch 176/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -36.2515 - g_loss: 165.3914\n",
      "Epoch 177/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -118.2711 - g_loss: 180.4978\n",
      "Epoch 178/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: -40.1987 - g_loss: 173.2058\n",
      "Epoch 179/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 112.3794 - g_loss: -13.0522\n",
      "Epoch 180/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 129.5110 - g_loss: -235.5462\n",
      "Epoch 181/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -142.1522 - g_loss: -110.4560\n",
      "Epoch 182/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 27.3307 - g_loss: -15.3985\n",
      "Epoch 183/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -294.2514 - g_loss: -135.9193\n",
      "Epoch 184/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 210.5275 - g_loss: -220.6719\n",
      "Epoch 185/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 53.9142 - g_loss: -293.0207\n",
      "Epoch 186/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 274.5319 - g_loss: -19.7554\n",
      "Epoch 187/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -74.5695 - g_loss: 220.5527\n",
      "Epoch 188/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -4.1436 - g_loss: 307.6462\n",
      "Epoch 189/200\n",
      "20/20 [==============================] - 14s 704ms/step - d_loss: -131.0995 - g_loss: 253.1236\n",
      "Epoch 190/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -56.0424 - g_loss: -125.6716\n",
      "Epoch 191/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: 10.7779 - g_loss: 63.2542\n",
      "Epoch 192/200\n",
      "20/20 [==============================] - 14s 707ms/step - d_loss: 56.3972 - g_loss: 321.6402\n",
      "Epoch 193/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 64.2757 - g_loss: 371.2390\n",
      "Epoch 194/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: -151.7150 - g_loss: 39.5895\n",
      "Epoch 195/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 179.3852 - g_loss: -340.6476\n",
      "Epoch 196/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -23.4210 - g_loss: -101.4320\n",
      "Epoch 197/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 184.4800 - g_loss: -401.7632\n",
      "Epoch 198/200\n",
      "20/20 [==============================] - 14s 705ms/step - d_loss: -19.5612 - g_loss: -914.7695\n",
      "Epoch 199/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 101.3571 - g_loss: -1129.1021\n",
      "Epoch 200/200\n",
      "20/20 [==============================] - 14s 706ms/step - d_loss: 290.7638 - g_loss: -1050.6397\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Eksor.A\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 18s 709ms/step - d_loss: -111.2523 - g_loss: -928.6062\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 16s 709ms/step - d_loss: -54.0784 - g_loss: -890.0328\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 16s 708ms/step - d_loss: -96.5612 - g_loss: -496.2113\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 16s 709ms/step - d_loss: -96.2046 - g_loss: -292.2138\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 16s 709ms/step - d_loss: -108.6834 - g_loss: -224.8846\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 16s 710ms/step - d_loss: -26.2864 - g_loss: -351.5680\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 16s 710ms/step - d_loss: -64.0781 - g_loss: -232.1195\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 65.6890 - g_loss: 30.9979\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -31.8601 - g_loss: 297.2799\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 38.6760 - g_loss: 744.2982\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -270.4455 - g_loss: 1049.0637\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -34.1383 - g_loss: 726.5617\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 6.0698 - g_loss: 855.1245\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 36.0824 - g_loss: 422.2110\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 24.8449 - g_loss: -33.9999\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 28.2666 - g_loss: 36.1911\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -45.8289 - g_loss: 146.1343\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -71.2443 - g_loss: -197.5253\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 18.7410 - g_loss: 90.6165\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -13.4831 - g_loss: -122.4563\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -22.4648 - g_loss: -422.7821\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -91.7529 - g_loss: -328.5242\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -57.6722 - g_loss: -96.1848\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -40.7647 - g_loss: -53.2714\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -40.7441 - g_loss: 97.3575\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -2.1778 - g_loss: 640.4206\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -3.9253 - g_loss: 552.9509\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 16s 713ms/step - d_loss: -11.8517 - g_loss: 256.3772\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 49.0460 - g_loss: 460.7945\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 7.7607 - g_loss: 338.7044\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: -45.4690 - g_loss: 181.4250\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 7.2728 - g_loss: 280.9931\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 34.0634 - g_loss: 208.5984\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -8.7842 - g_loss: 40.7895\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 22.0468 - g_loss: 24.8738\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -18.8646 - g_loss: 56.6340\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -21.8070 - g_loss: 101.4898\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -18.8502 - g_loss: 129.9671\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -22.6278 - g_loss: 166.6738\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -16.1448 - g_loss: 262.3513\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -8.9243 - g_loss: 255.2896\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -4.4626 - g_loss: 185.7826\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -31.1914 - g_loss: 230.3380\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -28.2983 - g_loss: 221.4473\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 0.2985 - g_loss: 307.7099\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -35.4444 - g_loss: 465.9118\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -1.2933 - g_loss: 408.0103\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -4.9920 - g_loss: 420.9514\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -19.0159 - g_loss: 329.8075\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 16s 715ms/step - d_loss: -31.2537 - g_loss: 433.2880\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: 25.7058 - g_loss: 568.3636\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -54.9925 - g_loss: 371.6790\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 45.1539 - g_loss: 281.2961\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -8.6633 - g_loss: 501.5162\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -91.7383 - g_loss: 513.9678\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 45.6306 - g_loss: 511.9501\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 71.8083 - g_loss: 29.6053\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 26.3910 - g_loss: 253.6745\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 6.3639 - g_loss: 450.8911\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 17.7574 - g_loss: -26.1401\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -68.0672 - g_loss: -19.2493\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -101.3902 - g_loss: 8.6040\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -82.6052 - g_loss: -281.0833\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -5.2815 - g_loss: -448.8169\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 114.8300 - g_loss: -98.1259\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 47.8608 - g_loss: -20.7774\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: -32.4577 - g_loss: -304.4474\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -61.1525 - g_loss: -370.8115\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 63.5140 - g_loss: 10.3649\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -3.4359 - g_loss: -117.2770\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 22.0112 - g_loss: 27.0098\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -35.1527 - g_loss: 400.9660\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 65.0003 - g_loss: -60.8616\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 55.8634 - g_loss: 50.8950\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 10.6432 - g_loss: -208.3019\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -54.2449 - g_loss: -546.9908\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: 50.1044 - g_loss: -631.8724\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -108.4596 - g_loss: -305.1465\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -48.2702 - g_loss: -179.6203\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -47.0051 - g_loss: -668.4907\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 0.7008 - g_loss: -700.4770\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 9.4084 - g_loss: -378.1527\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -114.6443 - g_loss: -630.0993\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 25.2036 - g_loss: -389.3680\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 97.5697 - g_loss: -645.4478\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 30.4955 - g_loss: -416.4682\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -86.0935 - g_loss: 27.1920\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -26.3747 - g_loss: 85.5122\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 216.4559 - g_loss: -769.0638\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -42.7864 - g_loss: -722.4663\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 47.2164 - g_loss: -872.9140\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -99.2343 - g_loss: -676.5953\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 59.9972 - g_loss: -449.7251\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -56.4824 - g_loss: -362.6815\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -108.0720 - g_loss: 253.3934\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -122.3279 - g_loss: 57.3425\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 134.0122 - g_loss: -60.4354\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 60.3686 - g_loss: 381.6683\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 145.0372 - g_loss: 313.7644\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 62.2861 - g_loss: 70.7882\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -71.6331 - g_loss: 110.0171\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 16s 710ms/step - d_loss: 43.4045 - g_loss: -297.1200\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 183.7735 - g_loss: -63.9605\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 16s 711ms/step - d_loss: 184.6485 - g_loss: 223.0984\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -194.6213 - g_loss: 342.5996\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 49.2657 - g_loss: 346.5342\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 55.0377 - g_loss: -323.8076\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 13.9194 - g_loss: -351.0661\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 115.3293 - g_loss: -13.4511\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 33.1130 - g_loss: 455.8194\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -121.8219 - g_loss: 370.0518\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -23.8636 - g_loss: 250.9043\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -4.6622 - g_loss: 38.7766\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 20.6163 - g_loss: 62.1800\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 127.8735 - g_loss: 332.0732\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -100.1576 - g_loss: 429.0050\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -235.6465 - g_loss: -113.2646\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -15.3844 - g_loss: -403.3896\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 55.5125 - g_loss: -426.8975\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 43.9012 - g_loss: -142.4182\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -43.7895 - g_loss: -30.0313\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -28.2894 - g_loss: -198.7033\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -54.0418 - g_loss: -752.0509\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -62.5466 - g_loss: -815.0130\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 132.9359 - g_loss: -665.2964\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -33.9172 - g_loss: -914.1236\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 94.3722 - g_loss: -1430.4417\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: -13.0109 - g_loss: -984.7064\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -1.1412 - g_loss: -1098.9800\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 110.6497 - g_loss: -735.6226\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -57.7095 - g_loss: -549.0918\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 94.6573 - g_loss: -657.7798\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 35.7606 - g_loss: -469.3867\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 23.8317 - g_loss: -132.8728\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 81.4425 - g_loss: 127.5926\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -107.0491 - g_loss: 572.8369\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -8.5305 - g_loss: 435.4562\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 45.2803 - g_loss: 459.6874\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -27.2084 - g_loss: 313.7429\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 45.3429 - g_loss: 0.1557\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -46.3273 - g_loss: -38.9553\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 205.6614 - g_loss: -150.8008\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 101.8220 - g_loss: 165.6029\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -107.2639 - g_loss: 322.6279\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 37.6037 - g_loss: 461.0308\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -90.5761 - g_loss: 1225.8458\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 43.0582 - g_loss: 908.8809\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -67.6816 - g_loss: 1092.4744\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -76.3616 - g_loss: 971.2901\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 72.0064 - g_loss: 676.3144\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -44.2759 - g_loss: 362.1812\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 0.3727 - g_loss: 477.0977\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: -0.6858 - g_loss: 515.8950\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -8.7663 - g_loss: 417.6324\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -25.2749 - g_loss: 454.5004\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -34.1470 - g_loss: 325.7241\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -26.4191 - g_loss: 407.6178\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -2.9778 - g_loss: 588.0555\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 17.6686 - g_loss: 384.5882\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -37.9480 - g_loss: 285.8583\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -14.3056 - g_loss: 428.6850\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 36.6906 - g_loss: 437.5083\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 43.9156 - g_loss: 250.6377\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -2.4592 - g_loss: 88.4567\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -27.4716 - g_loss: 259.0304\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -0.1649 - g_loss: 372.7117\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -0.9576 - g_loss: 464.9397\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -4.0306 - g_loss: 529.6665\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -26.0973 - g_loss: 832.4875\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 28.0902 - g_loss: 792.9853\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -14.4770 - g_loss: 435.8226\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 28.0755 - g_loss: 317.3778\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: 5.8308 - g_loss: 98.7241\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 13.0184 - g_loss: -6.9846\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 10.2081 - g_loss: 27.4605\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 36.0122 - g_loss: 254.6437\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -18.7877 - g_loss: 308.8010\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -4.5149 - g_loss: 584.9855\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 3.5170 - g_loss: 670.9381\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 16s 712ms/step - d_loss: 5.7147 - g_loss: 351.6019\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -1.7343 - g_loss: 206.9635\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -9.7171 - g_loss: 110.1549\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 8.4199 - g_loss: 196.5658\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -117.3294 - g_loss: 307.1931\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -93.3938 - g_loss: 23.5211\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 32.6164 - g_loss: 154.6477\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -22.0342 - g_loss: 95.7838\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: 1.1254 - g_loss: -70.7773\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -2.9716 - g_loss: -254.3716\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 35.0174 - g_loss: -375.8624\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: -2.6319 - g_loss: -335.9596\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 16s 714ms/step - d_loss: -17.1581 - g_loss: -563.6630\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 16s 712ms/step - d_loss: 30.8027 - g_loss: -492.9206\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: 23.4141 - g_loss: -285.4987\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -64.7882 - g_loss: -206.9443\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -9.9793 - g_loss: -173.7133\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 16s 713ms/step - d_loss: -63.0765 - g_loss: -141.5512\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 16s 718ms/step - d_loss: -100.7951 - g_loss: -400.6695\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -88.7286 - g_loss: -391.6753\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 16s 711ms/step - d_loss: -118.6316 - g_loss: -80.7972\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Obfuscator.AFQ\n",
      "Epoch 1/200\n",
      "7/7 [==============================] - 8s 686ms/step - d_loss: -16.5590 - g_loss: 21.9504\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 5s 684ms/step - d_loss: 55.0795 - g_loss: 126.4110\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: 28.4208 - g_loss: -57.6305\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -51.3323 - g_loss: 13.8484\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -142.4466 - g_loss: -337.8438\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -18.8261 - g_loss: -632.2548\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -127.1844 - g_loss: -617.2990\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 5s 684ms/step - d_loss: 151.6944 - g_loss: -86.1206\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 5s 683ms/step - d_loss: 25.9082 - g_loss: 119.5689\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -34.1614 - g_loss: 34.3553\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: 43.5325 - g_loss: -47.0014\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 5s 684ms/step - d_loss: -23.8495 - g_loss: -110.0393\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: 44.8538 - g_loss: -304.7638\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -25.6409 - g_loss: -372.7447\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 5s 684ms/step - d_loss: 3.3721 - g_loss: -572.4567\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 5s 684ms/step - d_loss: 13.6588 - g_loss: -547.5992\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -109.4858 - g_loss: -512.0286\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -21.0268 - g_loss: -695.7045\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 23.2512 - g_loss: -597.3903\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -44.4735 - g_loss: -545.2151\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -26.4208 - g_loss: -731.5283\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -100.7388 - g_loss: -541.5607\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 49.4548 - g_loss: -519.8327\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -17.0292 - g_loss: -533.2443\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -4.0428 - g_loss: -911.8568\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -94.4516 - g_loss: -1075.4657\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -309.9447 - g_loss: -1088.9983\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 125.7430 - g_loss: -709.2994\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 48.7031 - g_loss: -449.1916\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 14.7373 - g_loss: -355.7948\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 157.5408 - g_loss: -182.5852\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -162.5623 - g_loss: 10.5781\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 262.4935 - g_loss: 105.0874\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 3.7774 - g_loss: 15.9746\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 93.9761 - g_loss: 128.0553\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 6.9814 - g_loss: 46.9181\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -72.3203 - g_loss: -110.8123\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 68.1665 - g_loss: -139.0274\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -4.7989 - g_loss: -324.1723\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -103.3556 - g_loss: -546.9717\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -41.1337 - g_loss: -438.0988\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 217.3071 - g_loss: -223.1481\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 41.9919 - g_loss: -223.1505\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -183.9913 - g_loss: -191.2749\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 16.6210 - g_loss: -598.4963\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 29.6990 - g_loss: -739.2362\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -265.0686 - g_loss: -634.5255\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 120.2197 - g_loss: -555.6083\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -55.5022 - g_loss: -315.5916\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -88.3471 - g_loss: -454.3159\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -71.2695 - g_loss: -467.9680\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 33.2474 - g_loss: -548.7769\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 2.9334 - g_loss: -621.6086\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -156.0448 - g_loss: -880.2818\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -33.2314 - g_loss: -157.9246\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 75.9834 - g_loss: -254.5383\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 688ms/step - d_loss: -20.3624 - g_loss: -776.5311\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -26.4641 - g_loss: -1054.7405\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -4.1586 - g_loss: -795.5523\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 107.3841 - g_loss: -738.3218\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 8.3431 - g_loss: -465.9253\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -178.6553 - g_loss: -588.0865\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -216.3379 - g_loss: -599.4203\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -108.9810 - g_loss: -958.1701\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 81.0001 - g_loss: -423.1896\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 290.7884 - g_loss: -6.8097\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 44.6493 - g_loss: 470.9815\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -144.6748 - g_loss: -58.2785\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -381.1128 - g_loss: 244.2253\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 24.4847 - g_loss: 457.2270\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -365.5259 - g_loss: 442.9146\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -12.8349 - g_loss: 1541.9167\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 197.3524 - g_loss: 1295.0873\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -230.5480 - g_loss: 1231.5049\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 1.6985 - g_loss: 1206.0938\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 156.9969 - g_loss: 767.7149\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -164.5054 - g_loss: 786.4711\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -132.5873 - g_loss: 903.4238\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 202.0192 - g_loss: 467.9270\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -159.7611 - g_loss: 269.6548\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -313.3874 - g_loss: 160.6075\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: 109.7323 - g_loss: -343.0912\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -160.9888 - g_loss: -466.7351\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 199.3964 - g_loss: -970.8847\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -172.1478 - g_loss: -787.2346\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 232.4124 - g_loss: -1184.5724\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -100.3422 - g_loss: -777.9791\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 208.7784 - g_loss: -866.2792\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 203.2991 - g_loss: -661.5794\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 363.6205 - g_loss: -623.8460\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 94.8108 - g_loss: -627.1363\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 27.6560 - g_loss: -303.4322\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 37.7098 - g_loss: -72.1015\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 31.0196 - g_loss: -2.9999\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 122.4561 - g_loss: -95.5119\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -133.7615 - g_loss: 32.5353\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 5.8615 - g_loss: 420.7740\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -58.1116 - g_loss: 700.9290\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 62.8834 - g_loss: 916.0435\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 74.9551 - g_loss: 339.9176\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 37.3683 - g_loss: 146.0265\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -57.3159 - g_loss: -195.0340\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -30.1294 - g_loss: 42.5468\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -505.4001 - g_loss: 241.9949\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -71.8412 - g_loss: 755.5423\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 309.8502 - g_loss: 669.1179\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: 30.6207 - g_loss: 788.3695\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: 20.1274 - g_loss: 635.5968\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 332.7985 - g_loss: 553.4107\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -61.9226 - g_loss: 366.0707\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 136.7564 - g_loss: 442.2703\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -276.3759 - g_loss: 126.4637\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -362.3652 - g_loss: 35.3422\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 152.0220 - g_loss: -478.4935\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -160.9973 - g_loss: 137.0487\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -137.9221 - g_loss: -180.8844\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 236.7660 - g_loss: -134.0873\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: 62.9689 - g_loss: 233.6888\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 83.6199 - g_loss: 466.4563\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -368.5372 - g_loss: 425.5631\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 345.5277 - g_loss: 714.8732\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 6.2119 - g_loss: 461.4462\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 227.4351 - g_loss: 1699.7599\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 210.1448 - g_loss: 1372.1228\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 49.5723 - g_loss: 1015.5760\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 82.6986 - g_loss: 1252.9059\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -284.8088 - g_loss: 1633.1114\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 249.0347 - g_loss: 1554.7988\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -386.9722 - g_loss: 1438.8229\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -196.2025 - g_loss: 851.1511\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -85.4244 - g_loss: 945.8246\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 226.0997 - g_loss: 384.9997\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 162.1808 - g_loss: 439.4264\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 268.4526 - g_loss: 660.6391\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 687ms/step - d_loss: 153.8041 - g_loss: 274.3064\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -293.4552 - g_loss: -52.9888\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 100.8207 - g_loss: 770.9961\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 194.4418 - g_loss: 686.9293\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -4.5878 - g_loss: 502.7013\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 185.7894 - g_loss: 786.0781\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -20.2640 - g_loss: 228.4910\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 232.8740 - g_loss: -178.9128\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 309.0754 - g_loss: -460.7214\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -152.8685 - g_loss: -129.6855\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 394.0297 - g_loss: -478.5768\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 143.0094 - g_loss: 339.1726\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -84.1669 - g_loss: 483.4150\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 101.0261 - g_loss: 161.3049\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 323.1531 - g_loss: -51.2676\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 239.8618 - g_loss: 302.3457\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 165.7255 - g_loss: -467.9578\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 36.1311 - g_loss: 155.4248\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 44.5743 - g_loss: 513.8900\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -138.6230 - g_loss: 306.7224\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -193.5591 - g_loss: 635.0666\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 252.6088 - g_loss: 818.7690\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -42.6904 - g_loss: 255.4215\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 33.2038 - g_loss: 689.9930\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -144.8096 - g_loss: -17.0467\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 71.4325 - g_loss: -127.9155\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -406.6084 - g_loss: 155.6970\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -2.5161 - g_loss: -92.5262\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 152.0821 - g_loss: -321.0864\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 66.1489 - g_loss: -811.4806\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -72.4995 - g_loss: -713.7405\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 25.6451 - g_loss: -989.6706\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -259.2995 - g_loss: -952.0808\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -151.5911 - g_loss: -966.3640\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -241.3248 - g_loss: -685.7637\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -339.0602 - g_loss: -512.4956\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 95.5998 - g_loss: -323.6444\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -64.5291 - g_loss: -96.9210\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 141.1691 - g_loss: -188.9131\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 58.9868 - g_loss: 193.6940\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 323.0307 - g_loss: -58.5821\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 350.7263 - g_loss: 471.4834\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 5.6871 - g_loss: 285.6569\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 150.0303 - g_loss: 408.3772\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -53.7340 - g_loss: 615.4432\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 36.6864 - g_loss: 759.3273\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 351.6561 - g_loss: 948.1640\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 6.8517 - g_loss: 702.2946\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -48.3774 - g_loss: 1078.5829\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 51.8109 - g_loss: 1357.1915\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -180.4039 - g_loss: 1245.5580\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 287.1478 - g_loss: 1453.8217\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -269.7648 - g_loss: 1494.7410\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 552.6133 - g_loss: 1158.8606\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 405.9144 - g_loss: 868.9659\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 245.8987 - g_loss: 1455.7097\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -243.0490 - g_loss: 2099.3902\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -377.5700 - g_loss: 2157.5006\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 128.2960 - g_loss: 1762.1974\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -107.9467 - g_loss: 1804.7553\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 50.8455 - g_loss: 1449.5315\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -225.0353 - g_loss: 1734.1200\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 444.0076 - g_loss: 1144.4985\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -409.8999 - g_loss: 1029.8874\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 49.8601 - g_loss: 1528.4155\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -111.6535 - g_loss: 1742.4487\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Occamy.C\n",
      "Epoch 1/200\n",
      "7/7 [==============================] - 8s 687ms/step - d_loss: 197.9666 - g_loss: 2138.8992\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -72.2399 - g_loss: 1986.9655\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 54.3246 - g_loss: 1639.6661\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 144.3346 - g_loss: 1525.3401\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: 133.8821 - g_loss: 736.1202\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 5s 685ms/step - d_loss: -22.4592 - g_loss: 821.8415\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 63.7889 - g_loss: 723.6599\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 35.9010 - g_loss: 719.5076\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -43.9292 - g_loss: 1225.1866\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -75.8785 - g_loss: 1024.9150\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 5s 686ms/step - d_loss: -100.3871 - g_loss: 745.0813\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 688ms/step - d_loss: 285.2616 - g_loss: 480.1128\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -230.5195 - g_loss: 16.6620\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 140.3856 - g_loss: 5.3968\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 20.5333 - g_loss: 147.6749\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 57.8344 - g_loss: -95.8047\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -120.7183 - g_loss: -45.0067\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 262.0990 - g_loss: 22.1291\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -41.0918 - g_loss: -354.8808\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 0.2825 - g_loss: -569.1269\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -188.3252 - g_loss: -979.5130\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 261.5566 - g_loss: -724.4615\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -15.2216 - g_loss: -271.5851\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -103.3491 - g_loss: -698.9251\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 110.4226 - g_loss: -551.4116\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -26.8248 - g_loss: -423.2869\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -40.1056 - g_loss: -341.4972\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 21.7063 - g_loss: -484.3125\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -32.3816 - g_loss: -355.5346\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 1.3038 - g_loss: -285.6582\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 18.9715 - g_loss: -246.1742\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -18.3017 - g_loss: -317.2630\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -12.8298 - g_loss: -119.6585\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -94.0738 - g_loss: 88.0059\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 19.8426 - g_loss: 33.2226\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -57.4956 - g_loss: -62.2701\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -41.7227 - g_loss: 62.3028\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -61.2768 - g_loss: -159.5528\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -112.6124 - g_loss: 5.5292\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 58.7318 - g_loss: -114.4570\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 157.6781 - g_loss: -90.0669\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -78.5280 - g_loss: 30.5637\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -103.1955 - g_loss: -106.3911\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 54.7057 - g_loss: 127.8822\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -5.5457 - g_loss: 104.8222\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -34.0544 - g_loss: -42.4893\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -140.1608 - g_loss: 118.8981\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -68.1488 - g_loss: 335.2377\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -412.9237 - g_loss: -219.9561\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: -77.6691 - g_loss: -360.4583\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -258.3658 - g_loss: -41.0491\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -123.6344 - g_loss: -168.5294\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -176.0286 - g_loss: -507.3841\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: 78.4680 - g_loss: -402.6687\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 5s 695ms/step - d_loss: -29.4469 - g_loss: 43.4067\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 54.6330 - g_loss: 297.0923\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 255.1600 - g_loss: -141.9321\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 78.8692 - g_loss: 52.7553\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 102.6395 - g_loss: -188.7428\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: -7.1054 - g_loss: -165.2427\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: -55.1606 - g_loss: 288.0487\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -43.1626 - g_loss: 681.3981\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 92.2116 - g_loss: 829.7130\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 32.0125 - g_loss: 871.0764\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -224.6751 - g_loss: 898.3123\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -116.2877 - g_loss: 648.4524\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 129.7328 - g_loss: 828.0881\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 5s 712ms/step - d_loss: -34.1476 - g_loss: 660.4625\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: 239.1215 - g_loss: 471.4978\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 92.8552 - g_loss: 435.5462\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 5s 695ms/step - d_loss: 183.6249 - g_loss: 314.4900\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -5.0605 - g_loss: 428.9743\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 5s 696ms/step - d_loss: -26.6796 - g_loss: 515.7828\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 5s 695ms/step - d_loss: -118.0549 - g_loss: 497.1680\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: -13.8025 - g_loss: 471.0188\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 5s 696ms/step - d_loss: 114.4964 - g_loss: 603.1721\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -46.1152 - g_loss: 452.0260\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -44.8501 - g_loss: 607.3201\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: 78.7643 - g_loss: 742.1940\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 100.8793 - g_loss: 679.6246\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 5s 696ms/step - d_loss: -227.8241 - g_loss: 683.3154\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 5s 694ms/step - d_loss: -13.4445 - g_loss: 774.2430\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -96.6670 - g_loss: 936.0577\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -60.5947 - g_loss: 931.6741\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -115.9528 - g_loss: 988.4955\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -131.8598 - g_loss: 1137.1586\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -0.5349 - g_loss: 1703.0311\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -2.7614 - g_loss: 1353.3482\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -34.0737 - g_loss: 997.2747\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 693ms/step - d_loss: 79.9515 - g_loss: 866.3578\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -3.7932 - g_loss: 923.4600\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 167.5750 - g_loss: 820.1148\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -21.4031 - g_loss: 647.6093\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -80.4141 - g_loss: 588.5956\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -51.5405 - g_loss: 549.3436\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -37.4075 - g_loss: 952.7997\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 113.1366 - g_loss: 927.0216\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 145.6943 - g_loss: 767.6935\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 21.9831 - g_loss: 879.2282\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 20.7016 - g_loss: 1009.4552\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -82.8720 - g_loss: 1175.2076\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: 116.4901 - g_loss: 1077.0828\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 82.8482 - g_loss: 947.6081\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -59.8570 - g_loss: 1232.4988\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -148.9829 - g_loss: 997.4993\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 2.5703 - g_loss: 744.6342\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: 150.2444 - g_loss: 928.0259\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -205.3887 - g_loss: 865.5981\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 5.3829 - g_loss: 972.8474\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 80.8726 - g_loss: 894.5653\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 19.4729 - g_loss: 689.4821\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 32.2097 - g_loss: 691.4765\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 1.3004 - g_loss: 675.1298\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 33.4804 - g_loss: 477.1339\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 21.6387 - g_loss: 504.3564\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -37.9261 - g_loss: 532.6937\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -72.6084 - g_loss: 242.6796\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -42.6080 - g_loss: 417.9996\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: 129.2811 - g_loss: 341.6331\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -30.3860 - g_loss: 73.6677\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 74.2388 - g_loss: -97.2283\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -43.0600 - g_loss: -135.7495\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 47.1715 - g_loss: -247.3936\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -61.2657 - g_loss: -272.6898\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -69.4177 - g_loss: -233.5409\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 50.5301 - g_loss: 35.0880\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 116.9852 - g_loss: 108.6376\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 66.9698 - g_loss: -102.5346\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -32.6295 - g_loss: 14.1382\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -71.9330 - g_loss: 17.5752\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -37.5492 - g_loss: 152.8044\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 16.5854 - g_loss: 56.0592\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 85.3259 - g_loss: 131.1932\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -8.7620 - g_loss: 118.6051\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -35.1348 - g_loss: 62.1734\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -152.4645 - g_loss: -22.7641\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -145.4951 - g_loss: -79.1703\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -29.6126 - g_loss: -352.9886\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -62.0386 - g_loss: -587.7026\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 75.9378 - g_loss: -644.5110\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 223.6108 - g_loss: -726.8294\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 156.5209 - g_loss: -952.9162\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 211.1611 - g_loss: -862.5819\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -37.9496 - g_loss: -902.8163\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 56.6914 - g_loss: -704.3854\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -3.1581 - g_loss: -417.3452\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 66.5101 - g_loss: -251.1966\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 112.1415 - g_loss: -404.1632\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -3.8776 - g_loss: -381.7137\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -44.8993 - g_loss: -253.7214\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 55.7047 - g_loss: -381.2144\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 5s 695ms/step - d_loss: 15.8793 - g_loss: -340.6457\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -72.8662 - g_loss: -438.7692\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -11.8681 - g_loss: -526.6717\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 5s 695ms/step - d_loss: 109.8375 - g_loss: -571.6452\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 168.1492 - g_loss: -278.4761\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 38.7203 - g_loss: -255.0024\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -43.0995 - g_loss: -180.1076\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -43.7099 - g_loss: -62.5345\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 35.1026 - g_loss: 24.8321\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -36.0026 - g_loss: 77.4455\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 35.5890 - g_loss: 0.6615\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 37.9879 - g_loss: -123.1664\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -48.3488 - g_loss: 28.3922\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -25.9836 - g_loss: 82.9868\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -22.5968 - g_loss: 435.3438\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -41.4961 - g_loss: 541.7860\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 689ms/step - d_loss: -27.6924 - g_loss: 580.2789\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 62.4930 - g_loss: 531.3994\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -15.7146 - g_loss: 390.7922\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 17.1796 - g_loss: 229.7112\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -47.1192 - g_loss: 134.2555\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 12.1818 - g_loss: 243.2449\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -28.5601 - g_loss: 395.7647\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -73.2823 - g_loss: 315.3582\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 27.4588 - g_loss: 229.7370\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -14.3026 - g_loss: 169.5475\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -76.7509 - g_loss: 207.4454\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: -132.9312 - g_loss: 393.1188\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -56.5393 - g_loss: 644.6099\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 20.2220 - g_loss: 608.5574\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 252.9311 - g_loss: 481.7391\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: -47.1319 - g_loss: 572.1834\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -80.0569 - g_loss: 637.2678\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 5s 691ms/step - d_loss: 6.5539 - g_loss: 599.3289\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -142.1903 - g_loss: 588.2288\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -9.3921 - g_loss: 700.9988\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -32.1046 - g_loss: 686.2596\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -99.1250 - g_loss: 403.4701\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 105.6166 - g_loss: 185.9204\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: 48.7679 - g_loss: 247.1140\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 5s 688ms/step - d_loss: -65.2897 - g_loss: 241.7936\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -11.4100 - g_loss: 292.7292\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -78.0924 - g_loss: 521.5547\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: -22.7133 - g_loss: 340.5154\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 5s 693ms/step - d_loss: 50.3385 - g_loss: 393.1252\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 5s 690ms/step - d_loss: -43.5297 - g_loss: 194.0984\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 5s 692ms/step - d_loss: 98.9534 - g_loss: 190.0205\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 5s 689ms/step - d_loss: -18.8390 - g_loss: 94.0393\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 5s 687ms/step - d_loss: -78.1751 - g_loss: 22.5782\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on OnLineGames.CTB\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 7s 632ms/step - d_loss: -85.0758 - g_loss: 376.9961\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 4s 631ms/step - d_loss: -173.1855 - g_loss: 585.4637\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 4s 631ms/step - d_loss: -52.0161 - g_loss: 913.9886\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 4s 630ms/step - d_loss: -233.8004 - g_loss: 1141.1625\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: -65.1321 - g_loss: 1117.2568\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: -143.2147 - g_loss: 1145.8988\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 37.3682 - g_loss: 987.8581\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 18.2558 - g_loss: 811.7935\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 43.4305 - g_loss: 602.8837\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 12.8448 - g_loss: 671.1577\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 101.4480 - g_loss: 483.9551\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: -56.2183 - g_loss: 705.4223\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: 104.0925 - g_loss: 609.5786\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: 2.0806 - g_loss: 553.3679\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -19.0191 - g_loss: 446.2969\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 12.6023 - g_loss: 453.7557\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 57.5838 - g_loss: 304.8379\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: -19.8724 - g_loss: 284.3435\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -31.9391 - g_loss: 234.2811\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: 97.3622 - g_loss: 290.8583\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 54.0880 - g_loss: 227.1823\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 7.9904 - g_loss: 152.2078\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -44.1905 - g_loss: 106.6348\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 4.8256 - g_loss: 143.6712\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: 74.9031 - g_loss: 282.3718\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: -5.1802 - g_loss: 290.5467\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -50.3064 - g_loss: 240.2720\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -129.8525 - g_loss: 162.5359\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 40.1739 - g_loss: 169.8528\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -10.2909 - g_loss: 162.8738\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -100.8617 - g_loss: 179.0749\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -38.3389 - g_loss: 19.0792\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -34.8583 - g_loss: 92.5550\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -34.9033 - g_loss: 208.9865\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 94.4041 - g_loss: 306.0642\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -14.9861 - g_loss: 303.3315\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -27.1003 - g_loss: 341.9008\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -56.5944 - g_loss: 332.4117\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 1.3660 - g_loss: 315.3081\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 31.3583 - g_loss: 330.9557\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 63.2044 - g_loss: 447.9072\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -94.0945 - g_loss: 498.0005\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 38.8555 - g_loss: 598.3594\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 12.0200 - g_loss: 576.2841\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 128.2900 - g_loss: 477.1433\n",
      "Epoch 46/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 635ms/step - d_loss: 17.3876 - g_loss: 447.3648\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -6.3654 - g_loss: 533.9353\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 3.6074 - g_loss: 423.5923\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 3.6238 - g_loss: 363.3126\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 2.8952 - g_loss: 468.4255\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -12.6472 - g_loss: 403.5948\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 4s 639ms/step - d_loss: 69.3061 - g_loss: 341.4821\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -66.6283 - g_loss: 474.5092\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -70.7654 - g_loss: 551.8775\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 30.5394 - g_loss: 601.4350\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 11.2575 - g_loss: 570.9437\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 25.7336 - g_loss: 452.8992\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 51.3041 - g_loss: 247.3119\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -41.1225 - g_loss: 327.3383\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -75.5216 - g_loss: 338.5023\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 4s 640ms/step - d_loss: -52.8674 - g_loss: 313.3184\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 229.0456 - g_loss: 134.6948\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -4.6934 - g_loss: -85.7040\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 24.3290 - g_loss: -56.3092\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 35.0036 - g_loss: -78.0415\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 210.8710 - g_loss: -64.6812\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -149.5230 - g_loss: -0.3467\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 35.5195 - g_loss: -73.6528\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 4s 639ms/step - d_loss: 48.7122 - g_loss: -104.4761\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -27.0878 - g_loss: -110.4044\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -55.3340 - g_loss: -54.2309\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 4s 639ms/step - d_loss: 75.2331 - g_loss: -26.8453\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -42.8444 - g_loss: 1.0768\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 41.7416 - g_loss: -17.1223\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -29.5243 - g_loss: 144.5612\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 81.8286 - g_loss: 102.1370\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -105.9319 - g_loss: 107.9473\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 21.1107 - g_loss: 210.5012\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -41.4431 - g_loss: 239.9937\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 22.1522 - g_loss: 164.0809\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 90.8102 - g_loss: 235.6931\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 16.1759 - g_loss: 229.4766\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: -7.4077 - g_loss: 177.1248\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 0.0118 - g_loss: 198.1452\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -15.4719 - g_loss: 270.1263\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 31.6325 - g_loss: 270.6429\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 6.8755 - g_loss: 269.4575\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 13.2843 - g_loss: 202.2915\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -92.8032 - g_loss: 146.4825\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 4s 639ms/step - d_loss: 6.0678 - g_loss: 177.5209\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -88.4977 - g_loss: 151.6581\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 25.8440 - g_loss: 329.7713\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -27.8368 - g_loss: 217.6546\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 158.0265 - g_loss: 208.4860\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -31.3874 - g_loss: 208.4244\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 46.4374 - g_loss: 213.6974\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 60.9777 - g_loss: 132.6463\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -46.5841 - g_loss: 202.6204\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -56.8187 - g_loss: 212.8302\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: -18.8096 - g_loss: 215.3673\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 25.7424 - g_loss: 37.8071\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -36.8314 - g_loss: 109.6708\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -1.6934 - g_loss: 41.7206\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -21.4863 - g_loss: 108.3625\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 15.7363 - g_loss: 67.6635\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -19.0375 - g_loss: 60.2820\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -89.7892 - g_loss: -2.3303\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 4.4365 - g_loss: 45.9807\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 19.7157 - g_loss: -16.2805\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 130.2698 - g_loss: -85.9957\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 58.9278 - g_loss: 134.1307\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 4s 633ms/step - d_loss: 7.5584 - g_loss: 188.6723\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 59.7101 - g_loss: 158.3560\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -300.2197 - g_loss: 261.0091\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -59.8767 - g_loss: 203.6285\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -51.8068 - g_loss: 178.6811\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 112.2365 - g_loss: -85.6481\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 3.0529 - g_loss: -126.4957\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -85.1920 - g_loss: -141.7444\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -2.5547 - g_loss: -126.1741\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -75.2195 - g_loss: -246.1996\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -41.7720 - g_loss: -417.8100\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -80.7545 - g_loss: -551.4070\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: -120.0803 - g_loss: -649.3811\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 637ms/step - d_loss: -132.2956 - g_loss: -778.5806\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -29.9166 - g_loss: -567.7230\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 65.5771 - g_loss: -722.5237\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -159.1985 - g_loss: -710.1028\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -168.3612 - g_loss: -998.4388\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 112.8951 - g_loss: -816.4665\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 159.1698 - g_loss: -767.5593\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 151.7001 - g_loss: -790.2176\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -195.4784 - g_loss: -795.3972\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 25.6669 - g_loss: -474.7306\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -1.5179 - g_loss: -357.3103\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 90.6696 - g_loss: -286.1242\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 96.0063 - g_loss: -285.1534\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -112.1018 - g_loss: -467.0723\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -63.5971 - g_loss: -494.8979\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 29.3303 - g_loss: -778.7305\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -188.1304 - g_loss: -463.8904\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -160.6765 - g_loss: -464.4324\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -89.2141 - g_loss: -415.6630\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 183.4228 - g_loss: -437.2170\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 268.2274 - g_loss: -273.3113\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 61.5970 - g_loss: -393.0675\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -25.3526 - g_loss: 42.6753\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 265.9360 - g_loss: 113.5046\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -44.0036 - g_loss: -90.3796\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -253.7349 - g_loss: 15.7786\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -92.5392 - g_loss: -233.1435\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -14.4863 - g_loss: -266.3062\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -158.9690 - g_loss: -493.9005\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -105.9370 - g_loss: -673.0489\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: -152.9969 - g_loss: -143.7606\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 261.5000 - g_loss: -133.7720\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 309.7142 - g_loss: -167.0486\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 4s 634ms/step - d_loss: 164.2419 - g_loss: -128.1816\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 148.4759 - g_loss: 71.5547\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 28.3602 - g_loss: 93.3905\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 4s 632ms/step - d_loss: 106.7920 - g_loss: 14.7564\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -58.2974 - g_loss: 67.8344\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 80.8108 - g_loss: 23.0016\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -7.1083 - g_loss: -99.0712\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -147.0719 - g_loss: 29.5024\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 92.6301 - g_loss: 232.3621\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 13.0265 - g_loss: 85.4287\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 79.2222 - g_loss: 221.9774\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 17.7345 - g_loss: 215.0395\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -40.2968 - g_loss: 222.1722\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 79.6054 - g_loss: 228.3897\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -94.4238 - g_loss: 13.5828\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 157.4177 - g_loss: 12.0085\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -42.7057 - g_loss: -126.0169\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -120.8647 - g_loss: -38.3413\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: -124.7366 - g_loss: -26.0381\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 46.9561 - g_loss: 104.2192\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -132.4852 - g_loss: -81.8299\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 6.3478 - g_loss: -315.8163\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 35.6463 - g_loss: -137.0418\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 192.6704 - g_loss: -36.1145\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 60.0036 - g_loss: 170.6526\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: 39.4346 - g_loss: 66.7199\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 4s 638ms/step - d_loss: -12.4260 - g_loss: 124.4204\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 16.4283 - g_loss: 45.4525\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 12.1955 - g_loss: -160.8836\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -23.7594 - g_loss: -308.7774\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -52.5037 - g_loss: -310.9559\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -61.2973 - g_loss: -369.7565\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -115.0435 - g_loss: -405.9734\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -148.9000 - g_loss: -534.9396\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 241.5834 - g_loss: -299.2924\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 56.7010 - g_loss: -340.0236\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: 121.7922 - g_loss: -329.1533\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -109.3796 - g_loss: -359.5351\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 63.7144 - g_loss: -392.9305\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: 74.7322 - g_loss: -579.8140\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 4s 637ms/step - d_loss: 6.2659 - g_loss: -291.5303\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 4s 636ms/step - d_loss: -49.3674 - g_loss: -523.3442\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 4s 635ms/step - d_loss: -0.1311 - g_loss: -589.6651\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Reveton.A\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 11s 694ms/step - d_loss: -91.3117 - g_loss: -242.6685\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 691ms/step - d_loss: 85.4776 - g_loss: -321.5281\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 8s 692ms/step - d_loss: -106.5306 - g_loss: -103.0367\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 8s 694ms/step - d_loss: 55.9768 - g_loss: 112.5284\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 8s 693ms/step - d_loss: 108.3110 - g_loss: 121.7075\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: 31.9956 - g_loss: 372.5331\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 8s 693ms/step - d_loss: -32.3032 - g_loss: 439.9243\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 8s 693ms/step - d_loss: 28.6871 - g_loss: 437.6877\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 8s 694ms/step - d_loss: 17.4142 - g_loss: 322.0770\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 8s 694ms/step - d_loss: -120.6150 - g_loss: 168.5990\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -18.4655 - g_loss: 34.0088\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -37.2113 - g_loss: 111.8406\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 3.1301 - g_loss: 79.9499\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 29.5541 - g_loss: -89.3509\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 45.8680 - g_loss: -134.0370\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -8.5151 - g_loss: -113.5169\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 18.9129 - g_loss: -53.9834\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -16.5227 - g_loss: -60.3901\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -23.0552 - g_loss: -271.2085\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -43.9153 - g_loss: -327.1137\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -68.3726 - g_loss: -536.4671\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -81.7067 - g_loss: -530.8360\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -14.7991 - g_loss: -296.0776\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -35.2846 - g_loss: -230.1336\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -39.5341 - g_loss: -401.2314\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: 7.6874 - g_loss: -329.6256\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -67.1001 - g_loss: -164.1914\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 60.8111 - g_loss: 12.0083\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -58.8063 - g_loss: -280.7377\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -20.2222 - g_loss: -180.5788\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -28.3653 - g_loss: -122.9099\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -14.5354 - g_loss: 11.7662\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 51.1501 - g_loss: 10.8699\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 46.1034 - g_loss: -39.9960\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 41.8045 - g_loss: -47.7969\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -33.7564 - g_loss: -65.4970\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -67.0469 - g_loss: -178.3303\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -0.4448 - g_loss: -260.4922\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 0.1390 - g_loss: -189.4146\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 35.8731 - g_loss: -45.6361\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -0.9967 - g_loss: 54.3435\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 43.9517 - g_loss: 168.6456\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -15.8084 - g_loss: 336.3655\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -76.5697 - g_loss: 284.7708\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 68.1993 - g_loss: 217.8324\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -19.0999 - g_loss: 448.1912\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 29.2567 - g_loss: 375.2091\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: -21.5120 - g_loss: 228.3059\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -60.0162 - g_loss: 286.8133\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -11.3313 - g_loss: 347.0748\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 44.2034 - g_loss: 266.6232\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 45.5150 - g_loss: 191.3709\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 27.1937 - g_loss: 275.1830\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -62.7177 - g_loss: 390.5044\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -93.0575 - g_loss: 710.6951\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 63.3524 - g_loss: 814.6954\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 59.9991 - g_loss: 759.9654\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -23.8028 - g_loss: 751.3179\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 2.2870 - g_loss: 934.8721\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -12.2635 - g_loss: 729.2315\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -2.4945 - g_loss: 460.6727\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 21.6770 - g_loss: 461.4660\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -90.9768 - g_loss: 343.4620\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 125.1349 - g_loss: 396.6653\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 38.8818 - g_loss: 373.6579\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -2.4489 - g_loss: 368.3956\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 19.4796 - g_loss: 679.8948\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -134.2784 - g_loss: 507.5346\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 54.6888 - g_loss: 551.1961\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 136.2128 - g_loss: 926.4242\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -79.4519 - g_loss: 714.4102\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -108.9689 - g_loss: 576.4950\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -56.3648 - g_loss: 710.6013\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 41.0252 - g_loss: 187.4592\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -52.2941 - g_loss: 143.9647\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 162.5629 - g_loss: 50.5863\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -24.4494 - g_loss: 110.7240\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -17.6907 - g_loss: -76.2184\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 695ms/step - d_loss: -40.9062 - g_loss: 58.8574\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 127.5628 - g_loss: -346.2466\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -71.0307 - g_loss: -171.5751\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -20.3566 - g_loss: -284.2780\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -100.1292 - g_loss: -239.4108\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 26.1381 - g_loss: -136.6610\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -63.9300 - g_loss: -150.2890\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -141.2596 - g_loss: 11.1648\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: -35.8175 - g_loss: 126.1279\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -345.5401 - g_loss: 297.9384\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 44.2745 - g_loss: 521.7682\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -79.9526 - g_loss: 905.5709\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 8s 702ms/step - d_loss: -137.5416 - g_loss: 1327.6778\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 9s 701ms/step - d_loss: -261.4423 - g_loss: 1241.3759\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 32.9563 - g_loss: 967.1347\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 39.0553 - g_loss: 909.0122\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -24.7500 - g_loss: 845.6435\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 269.0337 - g_loss: 250.3194\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 66.5693 - g_loss: 263.9592\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -4.2024 - g_loss: 222.1434\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -31.6474 - g_loss: -95.7995\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 74.6132 - g_loss: -138.0454\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 8s 693ms/step - d_loss: 2.6972 - g_loss: -34.4530\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: 1.0030 - g_loss: -296.7303\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 114.1821 - g_loss: -236.7565\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 49.9163 - g_loss: -237.3230\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -32.2252 - g_loss: -364.6880\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -189.5153 - g_loss: -493.4671\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 119.9577 - g_loss: -317.9476\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -162.9227 - g_loss: -535.0899\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 19.2247 - g_loss: -741.6057\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -185.9313 - g_loss: -405.3866\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 8s 694ms/step - d_loss: -102.3562 - g_loss: -394.2298\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 146.6901 - g_loss: -258.1932\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -6.0903 - g_loss: -175.9064\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -74.0368 - g_loss: -341.4672\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: 84.0062 - g_loss: 141.3688\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -62.9533 - g_loss: 69.4882\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 93.4580 - g_loss: 34.0424\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -21.6375 - g_loss: 11.1788\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 13.4758 - g_loss: 4.8112\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -89.6964 - g_loss: -109.1056\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 19.6202 - g_loss: -309.4102\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 39.6384 - g_loss: -297.9101\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 5.4229 - g_loss: -309.1683\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -96.2686 - g_loss: -328.1314\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -47.3540 - g_loss: -70.9437\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: 121.5125 - g_loss: 87.8798\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -52.5569 - g_loss: 204.5381\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 25.4611 - g_loss: 563.0612\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 54.8899 - g_loss: 454.3096\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 36.3624 - g_loss: 413.8736\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 61.2202 - g_loss: 245.4190\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -42.1012 - g_loss: 418.8851\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -67.1059 - g_loss: 508.1131\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -218.5768 - g_loss: 568.4955\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 102.5999 - g_loss: 497.6047\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 80.5290 - g_loss: 223.2158\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 71.8679 - g_loss: 241.2364\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -59.7974 - g_loss: 193.0238\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 47.0633 - g_loss: 219.9994\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 26.2910 - g_loss: 280.0124\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -93.2510 - g_loss: 16.9848\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -11.5402 - g_loss: -101.9073\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -128.8260 - g_loss: 73.6715\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -18.5863 - g_loss: 462.2243\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -182.3370 - g_loss: 607.4729\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 48.3541 - g_loss: 74.4235\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -155.2624 - g_loss: -38.5746\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -89.5054 - g_loss: -51.8665\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 111.3437 - g_loss: 192.3285\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 56.3166 - g_loss: 224.2297\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 20.4287 - g_loss: 159.8647\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 38.4003 - g_loss: 478.6265\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 9.9533 - g_loss: 495.3923\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: -183.1426 - g_loss: 381.9583\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 696ms/step - d_loss: 7.3436 - g_loss: 245.3495\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -151.0597 - g_loss: 481.7469\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 151.7287 - g_loss: 560.1880\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 110.3235 - g_loss: 514.4089\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 165.8289 - g_loss: 566.9361\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 155.8173 - g_loss: 121.7205\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 168.5958 - g_loss: 148.5009\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 25.0204 - g_loss: 313.1940\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -35.2262 - g_loss: 419.3580\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 3.6710 - g_loss: 643.5735\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 213.5208 - g_loss: 734.1719\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -8.8866 - g_loss: 557.7186\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -40.3487 - g_loss: 375.8844\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 0.1972 - g_loss: -54.3414\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -77.4715 - g_loss: -315.3910\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 8s 694ms/step - d_loss: 77.3423 - g_loss: -721.9784\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 8s 699ms/step - d_loss: -107.3886 - g_loss: -556.6793\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 3.8511 - g_loss: -294.4412\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 126.9205 - g_loss: -231.6093\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: -91.1598 - g_loss: -382.5559\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 80.5407 - g_loss: -501.0029\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: -246.7166 - g_loss: -888.2956\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 67.4289 - g_loss: -914.7162\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -103.9105 - g_loss: -815.4281\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 84.0341 - g_loss: -821.5862\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 26.9889 - g_loss: -655.1542\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: -88.9364 - g_loss: -681.2576\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 60.6216 - g_loss: -420.9758\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 46.0225 - g_loss: -468.5129\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 3.5654 - g_loss: -328.3867\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 6.0393 - g_loss: -451.7661\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 57.2542 - g_loss: -325.8801\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 19.4144 - g_loss: -564.7825\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 39.0945 - g_loss: -669.8605\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 1.5787 - g_loss: -657.1748\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 37.5005 - g_loss: -745.7545\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 59.6367 - g_loss: -857.5531\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 42.5222 - g_loss: -439.7865\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 8s 698ms/step - d_loss: 72.4542 - g_loss: -369.2978\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -156.3301 - g_loss: -175.6460\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 32.4036 - g_loss: -387.3394\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 8s 696ms/step - d_loss: 83.2625 - g_loss: -398.2646\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 36.2371 - g_loss: -267.6401\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: -62.0680 - g_loss: -100.7226\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 8s 697ms/step - d_loss: 2.4721 - g_loss: -66.4799\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 8s 695ms/step - d_loss: 28.2330 - g_loss: -174.8821\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Sfone\n",
      "Epoch 1/200\n",
      "8/8 [==============================] - 9s 755ms/step - d_loss: -25.9259 - g_loss: -228.1345\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 6s 683ms/step - d_loss: -7.7425 - g_loss: -240.9210\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 6s 684ms/step - d_loss: -46.8571 - g_loss: -279.5640\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -42.6466 - g_loss: -330.0124\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: 47.0297 - g_loss: -354.1316\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 6s 684ms/step - d_loss: -6.2384 - g_loss: -313.8544\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 6s 683ms/step - d_loss: -200.6342 - g_loss: -430.7150\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: 61.8445 - g_loss: -589.8670\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -92.9623 - g_loss: -381.5571\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 6s 684ms/step - d_loss: 7.1766 - g_loss: -332.0841\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -15.2272 - g_loss: -240.8938\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 52.4012 - g_loss: -277.3207\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 7.1151 - g_loss: -286.1600\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -16.0286 - g_loss: -230.6557\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -11.1626 - g_loss: -259.8524\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -79.6229 - g_loss: -191.2866\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 10.8922 - g_loss: -114.2462\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -86.0281 - g_loss: -141.4876\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 46.2250 - g_loss: -11.4624\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -2.1611 - g_loss: -77.7147\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -35.5855 - g_loss: 28.5558\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -15.6215 - g_loss: -9.8693\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 10.7961 - g_loss: 72.0552\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 77.6188 - g_loss: 28.1177\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -46.9346 - g_loss: 53.0948\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 55.8992 - g_loss: 57.2132\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: 52.4352 - g_loss: 283.5877\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -23.4832 - g_loss: 98.0744\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 40.6686 - g_loss: 22.3089\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -40.0002 - g_loss: 79.5570\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -44.1531 - g_loss: 89.3048\n",
      "Epoch 32/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 691ms/step - d_loss: 49.9125 - g_loss: 131.5912\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -6.9402 - g_loss: 126.8485\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -31.3032 - g_loss: 202.5639\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 6s 692ms/step - d_loss: 84.2662 - g_loss: 43.5400\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 19.5347 - g_loss: 104.1676\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: 13.7563 - g_loss: 96.0384\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -58.6477 - g_loss: 115.1775\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -63.1240 - g_loss: 200.3715\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 43.7858 - g_loss: 82.6099\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 98.4940 - g_loss: 53.3155\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -4.9863 - g_loss: -47.6170\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -31.2006 - g_loss: -17.6694\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -33.6107 - g_loss: -125.3897\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -17.7409 - g_loss: -89.0820\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -11.3516 - g_loss: -137.4088\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -43.1719 - g_loss: -77.9543\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 65.4319 - g_loss: -21.8995\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -33.4813 - g_loss: -42.7151\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -19.9753 - g_loss: -82.9949\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: 1.2761 - g_loss: -77.6331\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -7.9028 - g_loss: -235.7253\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 18.5127 - g_loss: -125.8535\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 29.3886 - g_loss: -91.7977\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -42.1184 - g_loss: -139.5679\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 27.2270 - g_loss: -281.9212\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -51.1806 - g_loss: -260.3328\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 24.9847 - g_loss: -224.4230\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 19.2664 - g_loss: -71.4577\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 50.3026 - g_loss: -3.7048\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -60.5289 - g_loss: -29.2692\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 64.0042 - g_loss: -129.1449\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -63.5457 - g_loss: 20.8688\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -40.7728 - g_loss: -126.5108\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 63.4347 - g_loss: -102.2227\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 23.5059 - g_loss: -130.6320\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 2.0746 - g_loss: -132.1509\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -40.7016 - g_loss: -79.3071\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: -16.2349 - g_loss: 71.8742\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -85.4946 - g_loss: -79.9554\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 14.9549 - g_loss: -71.6468\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -86.4837 - g_loss: -5.0590\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -54.0121 - g_loss: -79.5736\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -80.2232 - g_loss: -284.9899\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 36.4075 - g_loss: -409.9652\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 2.1914 - g_loss: -376.3767\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -6.8115 - g_loss: -417.1109\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -41.6244 - g_loss: -460.6380\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 6s 693ms/step - d_loss: 30.7349 - g_loss: -422.5583\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 6s 693ms/step - d_loss: 32.7412 - g_loss: -434.4335\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 6.4433 - g_loss: -535.4812\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 23.0803 - g_loss: -612.6142\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 6s 693ms/step - d_loss: -25.2870 - g_loss: -553.3844\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: 23.8230 - g_loss: -632.3766\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 6s 692ms/step - d_loss: -9.9915 - g_loss: -557.1788\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 106.7310 - g_loss: -624.1890\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 19.7315 - g_loss: -722.2676\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -35.4029 - g_loss: -904.2804\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -97.1013 - g_loss: -1251.7023\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 42.7180 - g_loss: -1381.3330\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -67.7694 - g_loss: -1766.6734\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -1.4308 - g_loss: -1726.8620\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -3.3711 - g_loss: -1799.2858\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -15.6684 - g_loss: -1717.4912\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 2.7684 - g_loss: -1334.2732\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 109.2265 - g_loss: -1392.6789\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -188.5133 - g_loss: -1352.5888\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -40.2116 - g_loss: -1215.0425\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -209.3224 - g_loss: -1190.2504\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 105.6955 - g_loss: -835.7233\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -84.8344 - g_loss: -907.0532\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -120.8540 - g_loss: -1005.1188\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: 33.9294 - g_loss: -828.5039\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 109.2225 - g_loss: -556.3289\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 13.2231 - g_loss: -234.0996\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 94.0267 - g_loss: -403.5499\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 69.0865 - g_loss: -693.1667\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -27.2462 - g_loss: -585.8650\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -16.8621 - g_loss: -712.4125\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 684ms/step - d_loss: -65.9171 - g_loss: -601.0572\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -46.3060 - g_loss: -426.2896\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 12.2189 - g_loss: -665.8499\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 164.5496 - g_loss: -578.2225\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 124.9262 - g_loss: -347.5304\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 36.3980 - g_loss: -371.6876\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -205.8416 - g_loss: -95.9040\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 16.7752 - g_loss: -109.8200\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 26.5250 - g_loss: -147.5144\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -121.9373 - g_loss: -94.3861\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 92.0395 - g_loss: -5.4973\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -115.1636 - g_loss: 47.8841\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 92.8981 - g_loss: 28.4850\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 9.6021 - g_loss: -48.2876\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -1.6856 - g_loss: -410.9819\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 49.8892 - g_loss: -374.4669\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -26.3106 - g_loss: -415.2669\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -208.3374 - g_loss: -743.9344\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 21.3821 - g_loss: -637.2914\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 268.8946 - g_loss: -707.0083\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -25.3600 - g_loss: -817.0686\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -126.6958 - g_loss: -813.8234\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -240.8667 - g_loss: -983.0707\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -25.7806 - g_loss: -882.3708\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 337.0385 - g_loss: -747.7605\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -185.6543 - g_loss: -1020.5151\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -63.6311 - g_loss: -569.6887\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 33.4337 - g_loss: -270.1271\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -57.0730 - g_loss: -782.3614\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -258.7406 - g_loss: -815.3694\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -95.3819 - g_loss: -932.5156\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -32.7843 - g_loss: -1338.9822\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -350.7541 - g_loss: -1003.9962\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 220.2850 - g_loss: -473.1810\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 204.5435 - g_loss: -252.5497\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -284.5880 - g_loss: 89.5681\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -105.4185 - g_loss: -44.3042\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 175.9446 - g_loss: -65.8723\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -204.4627 - g_loss: -38.6603\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -44.9854 - g_loss: 233.1574\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -124.7589 - g_loss: 213.6054\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 215.1725 - g_loss: -44.4513\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -163.3447 - g_loss: -544.2088\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 40.3827 - g_loss: -816.7195\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -145.6308 - g_loss: -807.2505\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -133.2790 - g_loss: -937.2922\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -71.6051 - g_loss: -839.1829\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -11.3474 - g_loss: -809.9067\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 69.4847 - g_loss: -959.7926\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 22.3554 - g_loss: -918.1203\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -38.6176 - g_loss: -894.9293\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 176.7865 - g_loss: -874.5173\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -131.0828 - g_loss: -743.0452\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -17.6823 - g_loss: -435.5530\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -130.6149 - g_loss: -352.8529\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -75.0418 - g_loss: -609.9864\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 148.3508 - g_loss: -404.1462\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -395.5959 - g_loss: -391.5152\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -68.1189 - g_loss: -939.1100\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -162.6691 - g_loss: -873.8188\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: -89.0338 - g_loss: -971.6144\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 31.7404 - g_loss: -773.3661\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -10.9440 - g_loss: -497.4383\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -133.8717 - g_loss: -435.7086\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 157.0775 - g_loss: -726.4589\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -173.4560 - g_loss: -327.7662\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -196.1947 - g_loss: -475.3325\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: 10.6579 - g_loss: -255.3705\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -168.5003 - g_loss: -355.4388\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -191.0377 - g_loss: -893.3295\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: 247.7711 - g_loss: -276.4862\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 102.9405 - g_loss: -701.2937\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 7.4222 - g_loss: -883.9417\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -163.4235 - g_loss: -886.0373\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: 186.3717 - g_loss: -775.0848\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -25.1451 - g_loss: -846.5645\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 163.4208 - g_loss: -293.9953\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 6s 687ms/step - d_loss: -48.7911 - g_loss: -530.6445\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 6s 689ms/step - d_loss: -142.6598 - g_loss: -1199.5026\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: 158.7738 - g_loss: -1297.4515\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -320.9559 - g_loss: -1688.5127\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -93.3543 - g_loss: -1393.1891\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 6s 690ms/step - d_loss: -375.4877 - g_loss: -1653.1736\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 6s 691ms/step - d_loss: -154.0944 - g_loss: -1030.5490\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 397.4344 - g_loss: -712.1690\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 6s 687ms/step - d_loss: -306.4360 - g_loss: -1044.8998\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -247.8170 - g_loss: -2056.0520\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 6s 686ms/step - d_loss: 159.2680 - g_loss: -1069.9022\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -25.6069 - g_loss: -1385.2794\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 6s 688ms/step - d_loss: -251.4288 - g_loss: -1460.0104\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 6s 685ms/step - d_loss: -74.4154 - g_loss: -1251.5188\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on VB.IL\n",
      "Epoch 1/200\n",
      "12/12 [==============================] - 12s 757ms/step - d_loss: 39.8505 - g_loss: -975.7621\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 9s 707ms/step - d_loss: 91.8183 - g_loss: -871.4358\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 9s 708ms/step - d_loss: -154.1557 - g_loss: -1143.8459\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 9s 708ms/step - d_loss: -15.4950 - g_loss: -1335.0648\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 9s 706ms/step - d_loss: -72.4860 - g_loss: -758.8638\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 9s 709ms/step - d_loss: 239.4481 - g_loss: -994.6179\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 9s 708ms/step - d_loss: -7.2793 - g_loss: -1381.7845\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: -46.2846 - g_loss: -949.0497\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 9s 709ms/step - d_loss: -71.5106 - g_loss: -1112.0735\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: -366.2002 - g_loss: -760.6966\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -168.5641 - g_loss: -144.5232\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 87.8227 - g_loss: -49.7081\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 109.7088 - g_loss: 709.2592\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -6.6169 - g_loss: 744.3649\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 9s 709ms/step - d_loss: -82.9694 - g_loss: 401.2983\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 136.2881 - g_loss: 246.0002\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 56.2676 - g_loss: -475.0404\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 113.5043 - g_loss: -395.8798\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 190.1614 - g_loss: -586.0195\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 11.8875 - g_loss: -705.4132\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -281.7083 - g_loss: -726.6662\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -81.9052 - g_loss: -896.2731\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -478.8414 - g_loss: -449.9715\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -365.0061 - g_loss: -442.4728\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: 9.4004 - g_loss: -837.0438\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -86.4329 - g_loss: -809.6692\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 252.1460 - g_loss: -576.2859\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 302.8661 - g_loss: -215.8631\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 62.8841 - g_loss: 438.7982\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -57.2792 - g_loss: 1114.3319\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 343.7273 - g_loss: 1094.1862\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 142.5336 - g_loss: 650.2791\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -13.9980 - g_loss: 891.7493\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 26.4577 - g_loss: 755.7418\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 144.2807 - g_loss: 686.0594\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 127.6972 - g_loss: 476.7283\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -88.2577 - g_loss: 637.7862\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 11.6378 - g_loss: 589.2925\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -124.1645 - g_loss: 406.6226\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 26.6852 - g_loss: 299.5553\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 26.6179 - g_loss: 314.5447\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -136.7489 - g_loss: 360.5013\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -88.1599 - g_loss: 552.1567\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -123.7193 - g_loss: 432.7262\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 127.7554 - g_loss: 221.6187\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -183.6221 - g_loss: -53.4478\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -133.8314 - g_loss: 37.9764\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 76.1731 - g_loss: -464.0992\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -3.9553 - g_loss: -309.2862\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -19.3121 - g_loss: -403.3020\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -308.4536 - g_loss: -634.1312\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -251.8427 - g_loss: 306.6526\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -183.1503 - g_loss: 673.2630\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 35.2564 - g_loss: 700.1706\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -66.1238 - g_loss: 817.4135\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -434.2730 - g_loss: 499.4718\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 225.6398 - g_loss: 341.1864\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 19.7823 - g_loss: 89.5155\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -71.0366 - g_loss: -164.8883\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -63.7855 - g_loss: 152.9195\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -16.5367 - g_loss: -479.0479\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 151.1368 - g_loss: -788.3912\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 710ms/step - d_loss: 232.6919 - g_loss: -950.6757\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 291.9122 - g_loss: -474.2474\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -120.9550 - g_loss: -280.9984\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -205.3367 - g_loss: -37.2330\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -367.9471 - g_loss: -73.5438\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 260.5000 - g_loss: 719.7149\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -161.1329 - g_loss: 935.1169\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -64.3233 - g_loss: 1097.6350\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -203.2131 - g_loss: 1207.7368\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -42.2008 - g_loss: 1398.6246\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 91.9401 - g_loss: 1379.9145\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -242.0040 - g_loss: 938.3353\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 45.8768 - g_loss: 321.5255\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 260.7313 - g_loss: 479.7128\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -220.0040 - g_loss: -542.2873\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -34.9658 - g_loss: -276.7890\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -133.5286 - g_loss: -239.2200\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -127.0869 - g_loss: -463.8504\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: -47.7565 - g_loss: -47.1119\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -13.7965 - g_loss: 134.7530\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 300.0445 - g_loss: 57.3932\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 235.8771 - g_loss: 74.1331\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 137.9139 - g_loss: -252.4173\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -0.8444 - g_loss: -387.1411\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 502.5825 - g_loss: -307.8338\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -115.6776 - g_loss: 201.5834\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 106.5105 - g_loss: 697.5670\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 146.1919 - g_loss: 675.9373\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 5.4764 - g_loss: 706.5929\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: 312.0534 - g_loss: 856.3858\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 69.0583 - g_loss: 1095.9747\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 48.6886 - g_loss: 789.1452\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -221.7188 - g_loss: -227.6891\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -36.3240 - g_loss: -1576.8471\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 180.9546 - g_loss: -1211.3847\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -152.7091 - g_loss: -568.6127\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 115.5066 - g_loss: -242.0517\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 233.3970 - g_loss: -330.5156\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -134.8695 - g_loss: -103.6944\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -189.3828 - g_loss: -1270.0670\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 105.0393 - g_loss: -938.3917\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 119.8105 - g_loss: -1309.4647\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 268.8510 - g_loss: -1822.1842\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 98.2141 - g_loss: -864.9267\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 48.7611 - g_loss: -819.5433\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 19.2279 - g_loss: -2082.2984\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 64.3608 - g_loss: -2532.8645\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 166.1440 - g_loss: -2108.0779\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -114.3348 - g_loss: -1624.1353\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -298.3004 - g_loss: -1068.3922\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -69.8207 - g_loss: -112.3929\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 42.7616 - g_loss: -88.9984\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -2.5567 - g_loss: 407.9018\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -122.5224 - g_loss: 736.8657\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 76.4282 - g_loss: 709.1285\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -30.5817 - g_loss: 740.0548\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 9s 715ms/step - d_loss: 53.5405 - g_loss: 120.5267\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -53.9812 - g_loss: -321.3978\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -51.7596 - g_loss: -679.2891\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 118.9759 - g_loss: -652.4246\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -65.9016 - g_loss: -580.2760\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: 38.3690 - g_loss: -823.7104\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -41.7954 - g_loss: -582.4905\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 559.5880 - g_loss: -904.9431\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -244.2831 - g_loss: -224.3229\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -336.1212 - g_loss: 45.4376\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 194.6097 - g_loss: 97.7471\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -336.4147 - g_loss: 143.4821\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -34.2466 - g_loss: 1019.6152\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -205.5549 - g_loss: 714.7823\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -317.8522 - g_loss: 531.5611\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -67.1017 - g_loss: 529.9400\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -198.0292 - g_loss: 1303.5841\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -45.8037 - g_loss: 718.1534\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -132.5111 - g_loss: 827.3399\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 258.9185 - g_loss: 1038.5961\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 711ms/step - d_loss: -158.5703 - g_loss: 1034.6454\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -229.9910 - g_loss: 1135.5496\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -278.5215 - g_loss: 1309.1473\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -87.0767 - g_loss: 1578.9908\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -46.8760 - g_loss: 1808.9753\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 9s 709ms/step - d_loss: 256.3484 - g_loss: 2450.2982\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 1.0576 - g_loss: 1106.9365\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -119.6135 - g_loss: 911.1228\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 199.0157 - g_loss: 81.2025\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -393.4203 - g_loss: 21.2932\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 96.3608 - g_loss: 754.3026\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -96.9207 - g_loss: 402.5740\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -14.4408 - g_loss: 329.6409\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: -204.2898 - g_loss: 689.0649\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 394.5055 - g_loss: 1039.6111\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 430.0048 - g_loss: 166.3853\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 328.2775 - g_loss: 72.3950\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 13.9536 - g_loss: 460.3501\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: -148.1009 - g_loss: 132.6477\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -157.7474 - g_loss: -216.0069\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -443.6249 - g_loss: -744.8119\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -44.2138 - g_loss: 50.5758\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -161.5692 - g_loss: -295.8281\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -155.3890 - g_loss: -128.8417\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 135.5892 - g_loss: 629.3681\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 25.9174 - g_loss: 487.7204\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 41.8158 - g_loss: 315.2248\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 222.0533 - g_loss: -873.8235\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 276.0749 - g_loss: -689.9086\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 237.9354 - g_loss: -516.0876\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -11.4968 - g_loss: -329.1944\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -313.8346 - g_loss: -66.1665\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -266.4676 - g_loss: 444.9332\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 170.4422 - g_loss: -709.7194\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -248.7528 - g_loss: -1094.6095\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: -200.4149 - g_loss: -1444.7927\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 9s 714ms/step - d_loss: -369.1655 - g_loss: -1445.9233\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 246.3004 - g_loss: -1513.1848\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 437.8945 - g_loss: -1979.5598\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 123.9627 - g_loss: -3697.3442\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 9s 724ms/step - d_loss: -157.1126 - g_loss: -3259.0911\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -38.8409 - g_loss: -1597.3020\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 580.0141 - g_loss: -365.0138\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 9s 710ms/step - d_loss: 209.2668 - g_loss: 102.8641\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 61.5126 - g_loss: -504.0029\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -377.0536 - g_loss: -224.3637\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 318.9924 - g_loss: -889.1048\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 317.0978 - g_loss: -477.3197\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -27.2590 - g_loss: -631.1368\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 24.2947 - g_loss: -966.5153\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 21.2412 - g_loss: -939.3228\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 50.9783 - g_loss: -271.4865\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 227.0362 - g_loss: -1055.5454\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 266.1477 - g_loss: -247.5551\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -9.1308 - g_loss: -157.6045\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 123.3897 - g_loss: -410.1785\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: -305.2569 - g_loss: 875.7280\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 9s 713ms/step - d_loss: 14.1140 - g_loss: 909.1931\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 270.8195 - g_loss: 406.6167\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: -217.1972 - g_loss: 100.5441\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 9s 712ms/step - d_loss: 317.3286 - g_loss: 228.3019\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 9s 711ms/step - d_loss: 100.2515 - g_loss: 242.4609\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Zbot\n",
      "Epoch 1/200\n",
      "6/6 [==============================] - 7s 651ms/step - d_loss: -671.5710 - g_loss: -639.9410\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: 661.3233 - g_loss: -372.4060\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 4s 610ms/step - d_loss: 1397.3084 - g_loss: 3.0833\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: 679.2681 - g_loss: 590.3905\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -107.6755 - g_loss: 600.8208\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: 404.0324 - g_loss: -185.6028\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: 1079.0199 - g_loss: -2057.9097\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: -1176.5344 - g_loss: 401.0609\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: -561.4259 - g_loss: -829.6776\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: -82.7647 - g_loss: 187.9282\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 4s 610ms/step - d_loss: 148.5270 - g_loss: -1332.5202\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: -1392.8758 - g_loss: -803.9121\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: 355.5062 - g_loss: 694.5502\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -576.7035 - g_loss: -495.4547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -325.5507 - g_loss: -241.9480\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 4s 610ms/step - d_loss: 1990.0543 - g_loss: -860.0959\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1182.5476 - g_loss: -975.3683\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 1505.5855 - g_loss: -1898.5584\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 640.3982 - g_loss: -969.0049\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 839.1719 - g_loss: 111.6220\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 1013.8841 - g_loss: -414.4098\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: -562.2638 - g_loss: -1326.8793\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1283.5081 - g_loss: -695.7254\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 550.7732 - g_loss: -847.2275\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1142.9273 - g_loss: -213.7211\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 167.0949 - g_loss: 512.5078\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -776.9809 - g_loss: -577.6765\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 463.2392 - g_loss: 115.0027\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -107.5904 - g_loss: -1356.7776\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -962.2826 - g_loss: 543.3642\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 336.5426 - g_loss: -622.5279\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 284.9592 - g_loss: -124.8885\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -869.9235 - g_loss: -226.6521\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1364.0340 - g_loss: -42.2096\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 1375.0081 - g_loss: -1292.0837\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 607.8873 - g_loss: -660.1997\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 202.7252 - g_loss: 76.5559\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 753.6117 - g_loss: 834.5008\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 337.2393 - g_loss: -149.4821\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -590.0350 - g_loss: -1283.3099\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 529.3341 - g_loss: -1007.2237\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -671.2910 - g_loss: -36.8356\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -277.1439 - g_loss: -1031.3583\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 283.9811 - g_loss: -742.8788\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -897.8351 - g_loss: 403.6892\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -749.0255 - g_loss: -291.5862\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: 1463.5803 - g_loss: -199.3529\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -405.8217 - g_loss: 23.7533\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -485.6526 - g_loss: 1430.9525\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -23.9108 - g_loss: 283.7844\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -355.2527 - g_loss: 873.7174\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 1315.4278 - g_loss: -831.8856\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -953.8461 - g_loss: -1157.0493\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 796.0250 - g_loss: -853.9896\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 58.5456 - g_loss: -739.5251\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -145.7707 - g_loss: -40.8503\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -10.7913 - g_loss: -974.3409\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 945.6265 - g_loss: -1127.7554\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -1024.2872 - g_loss: -1370.6357\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 587.7623 - g_loss: 267.3931\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 0.0862 - g_loss: -1351.5714\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -577.3875 - g_loss: -1772.3871\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: -46.9250 - g_loss: -1949.4760\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 641.4761 - g_loss: -2004.5910\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 532.5773 - g_loss: -83.5889\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 4s 620ms/step - d_loss: -59.0216 - g_loss: -1094.8603\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 6.2324 - g_loss: -554.5418\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -606.4242 - g_loss: -1259.3405\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 332.4525 - g_loss: -2278.1668\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -248.6200 - g_loss: -1141.4606\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -492.8590 - g_loss: -1393.1064\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -26.3274 - g_loss: -1718.6603\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 162.4331 - g_loss: -1979.6831\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -853.0959 - g_loss: -2357.7015\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 1500.0193 - g_loss: -2252.0769\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -155.0556 - g_loss: -1981.4153\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -605.6838 - g_loss: -1665.9084\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 653.8133 - g_loss: -1714.9546\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 687.1710 - g_loss: -1274.4276\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -552.9987 - g_loss: -879.0349\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -486.8626 - g_loss: -137.5363\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -646.8122 - g_loss: -1322.1180\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -1045.7875 - g_loss: -1938.8602\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -1065.5933 - g_loss: -1481.8691\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -451.4425 - g_loss: -1422.0979\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -257.7233 - g_loss: -2086.2126\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 89.3430 - g_loss: -989.3312\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -1089.5811 - g_loss: -645.0018\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 622.5244 - g_loss: -375.3846\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: 191.8568 - g_loss: -543.7718\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1522.9769 - g_loss: -1953.2866\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 615ms/step - d_loss: 112.8114 - g_loss: -2241.4605\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -1333.1180 - g_loss: -1831.0836\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 477.6869 - g_loss: -1251.1459\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: -1452.1436 - g_loss: -1480.4708\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 375.3626 - g_loss: -2147.8025\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -1592.5148 - g_loss: -2506.6001\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: 717.0411 - g_loss: -1501.3147\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -487.7191 - g_loss: -1085.8173\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 1331.9881 - g_loss: -1113.6420\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -317.4973 - g_loss: -857.7213\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -524.1025 - g_loss: -1544.9173\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 664.1315 - g_loss: -689.6160\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -54.2448 - g_loss: -638.3484\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 558.6358 - g_loss: 197.8009\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -43.1108 - g_loss: -1109.9283\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 2011.4637 - g_loss: -843.3502\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 4s 612ms/step - d_loss: 258.7053 - g_loss: -1906.2546\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -676.8772 - g_loss: 173.4082\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 1175.2308 - g_loss: -717.9323\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: -987.8165 - g_loss: -1065.2092\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 824.8866 - g_loss: -1251.7950\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -822.8267 - g_loss: -1750.5658\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -480.4868 - g_loss: -1329.9414\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1046.0197 - g_loss: -1330.6742\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 154.1996 - g_loss: 160.7003\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1152.1416 - g_loss: -2219.3827\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 782.8132 - g_loss: -2494.8925\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 167.2186 - g_loss: -2410.5668\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 4s 613ms/step - d_loss: -444.3172 - g_loss: -2876.8357\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -50.4466 - g_loss: -3157.5127\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 888.0207 - g_loss: -3331.5778\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 991.7513 - g_loss: -1728.8895\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 859.6412 - g_loss: -2898.8883\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 1926.1612 - g_loss: -2439.7548\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 626.9490 - g_loss: -1440.0658\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 115.0796 - g_loss: -1360.7962\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -325.6254 - g_loss: -1259.8797\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -386.1111 - g_loss: -2008.6113\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -113.5600 - g_loss: -2539.7249\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -260.6686 - g_loss: -1232.5559\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: -215.3051 - g_loss: -291.5315\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 92.3437 - g_loss: -44.5885\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 232.5452 - g_loss: 528.7643\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -11.7829 - g_loss: 579.1867\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 1586.2902 - g_loss: -52.6516\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 128.9820 - g_loss: -36.4893\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: -997.2275 - g_loss: -241.5003\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -1121.2323 - g_loss: -1227.1969\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: 1598.5535 - g_loss: 487.0735\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -503.5927 - g_loss: 122.2939\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -930.5947 - g_loss: -194.4949\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 86.8242 - g_loss: 1586.8581\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 816.5596 - g_loss: 1175.7981\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -370.2886 - g_loss: 1397.3137\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 115.9086 - g_loss: 576.8891\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -1056.7275 - g_loss: 1490.2032\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 2914.5182 - g_loss: 1154.8491\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 380.8816 - g_loss: 1517.3231\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 1143.9766 - g_loss: 1420.8662\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -1457.7491 - g_loss: 87.5044\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 715.2817 - g_loss: 1903.6542\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 1029.8062 - g_loss: 961.4308\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 948.8754 - g_loss: 491.0873\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 2135.2945 - g_loss: 165.3185\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -307.0533 - g_loss: 367.0367\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: -1560.3254 - g_loss: 659.4927\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 526.6534 - g_loss: 1050.2744\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 816.6954 - g_loss: 737.4214\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 975.5991 - g_loss: 640.0583\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -1672.8964 - g_loss: 677.1133\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -188.3353 - g_loss: 1553.0340\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 562.6034 - g_loss: -94.4817\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 907.9655 - g_loss: -1244.7759\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 945.6082 - g_loss: -533.2989\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -845.5987 - g_loss: -611.3470\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 22.5637 - g_loss: -741.6726\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 617ms/step - d_loss: -704.6191 - g_loss: -1161.6428\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 1833.1430 - g_loss: -40.0532\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: 1985.2500 - g_loss: -743.2424\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 1162.2284 - g_loss: 528.4037\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 60.4449 - g_loss: -239.7020\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 143.3009 - g_loss: -959.2235\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -785.4459 - g_loss: -601.4851\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 361.6752 - g_loss: -550.7372\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: -1030.4173 - g_loss: -1254.0897\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 566.0298 - g_loss: -432.6246\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -352.9135 - g_loss: -776.5564\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 1048.4649 - g_loss: -243.0801\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -557.4675 - g_loss: -190.4396\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -287.5122 - g_loss: -1050.8032\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 4s 620ms/step - d_loss: 787.7930 - g_loss: -2338.5095\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 44.8508 - g_loss: -1604.1620\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: -140.4114 - g_loss: -766.8466\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -464.7861 - g_loss: 25.1728\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 4s 619ms/step - d_loss: -766.2607 - g_loss: -817.5497\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 636.2381 - g_loss: -239.6084\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 236.2051 - g_loss: 143.9534\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 4s 618ms/step - d_loss: 272.3641 - g_loss: -541.8601\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: -316.0506 - g_loss: -11.3860\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 860.5346 - g_loss: -9.7290\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: 97.1102 - g_loss: -368.4791\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 90.5490 - g_loss: 566.7640\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 695.8047 - g_loss: -438.1483\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 4s 614ms/step - d_loss: -381.4458 - g_loss: 519.8380\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 4s 616ms/step - d_loss: 240.8077 - g_loss: -41.9168\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: -342.5766 - g_loss: -361.2141\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 4s 617ms/step - d_loss: 1196.9268 - g_loss: -132.9712\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 792.1057 - g_loss: 216.3156\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 4s 615ms/step - d_loss: 681.7439 - g_loss: -450.7937\n",
      "TRAINING DONE\n",
      "-----------------------\n",
      "Training model on Zbot!CI\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 7s 657ms/step - d_loss: -595.7615 - g_loss: -221.1741\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -205.8967 - g_loss: -68.9997\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 30.7974 - g_loss: 821.2507\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 3s 594ms/step - d_loss: -367.8147 - g_loss: 419.4877\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -113.5708 - g_loss: 774.8692\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -154.5491 - g_loss: 679.4512\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 3s 594ms/step - d_loss: 691.9096 - g_loss: 1591.5480\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 3s 593ms/step - d_loss: -678.2436 - g_loss: 908.9552\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 159.2816 - g_loss: 1035.3543\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 242.6664 - g_loss: 1318.9568\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 3s 594ms/step - d_loss: -1032.6309 - g_loss: 791.8933\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 1102.7609 - g_loss: 279.2789\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 110.6430 - g_loss: 451.7887\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 3s 593ms/step - d_loss: 34.2823 - g_loss: 1063.4062\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 3s 593ms/step - d_loss: 485.4237 - g_loss: 435.5178\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 3s 594ms/step - d_loss: 882.3747 - g_loss: 478.5165\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 365.4830 - g_loss: 119.4021\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 521.6212 - g_loss: -145.2500\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -165.2144 - g_loss: -223.0561\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -267.4983 - g_loss: 675.4921\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 362.6729 - g_loss: 258.3592\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 484.8630 - g_loss: 651.9290\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 651.0293 - g_loss: 297.4097\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 415.0170 - g_loss: 596.5808\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -576.4220 - g_loss: 169.8889\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -419.7238 - g_loss: 226.0171\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -565.7536 - g_loss: 1074.9849\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 967.7583 - g_loss: 684.8453\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 231.2083 - g_loss: 951.2186\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 216.6410 - g_loss: 839.6327\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 598.2924 - g_loss: 1738.5420\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 376.4732 - g_loss: 1396.6549\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -1116.9374 - g_loss: 966.9531\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 573.8886 - g_loss: 580.3445\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -664.6635 - g_loss: 1043.8911\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 0.2269 - g_loss: 1376.8618\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -106.7208 - g_loss: 1514.3169\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -225.6120 - g_loss: 877.0805\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -267.7333 - g_loss: 1764.4048\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 697.8869 - g_loss: 505.8504\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 64.2861 - g_loss: 1579.9111\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -109.4533 - g_loss: 953.3463\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -479.8395 - g_loss: 1374.1834\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 466.2426 - g_loss: 578.4131\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 600ms/step - d_loss: 119.8455 - g_loss: 493.7742\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -64.7476 - g_loss: 896.3230\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 179.6395 - g_loss: 1163.2289\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 155.9968 - g_loss: 738.5210\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -530.5953 - g_loss: 313.1246\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -365.4314 - g_loss: -613.3481\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 823.0402 - g_loss: 401.2333\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 852.2247 - g_loss: -827.8766\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -18.6002 - g_loss: -104.4813\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -300.9971 - g_loss: -331.8923\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -73.2776 - g_loss: -516.0102\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 327.5736 - g_loss: -191.6974\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -294.8920 - g_loss: -270.8446\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 1533.5112 - g_loss: 82.8117\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 131.9246 - g_loss: 493.7978\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -383.9067 - g_loss: 445.3298\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 207.0994 - g_loss: 1060.9178\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -30.1217 - g_loss: 820.4689\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -701.3240 - g_loss: 1370.9565\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -768.5987 - g_loss: 1028.8985\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -176.1963 - g_loss: 1423.3846\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -948.1172 - g_loss: 1218.1080\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -540.3119 - g_loss: 1183.6128\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -630.3364 - g_loss: 1580.1714\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -331.8403 - g_loss: 2398.1762\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 694.3538 - g_loss: 2725.3122\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -422.7860 - g_loss: 1086.0238\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -119.5109 - g_loss: 1136.6420\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -1115.1910 - g_loss: 685.6912\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -393.7970 - g_loss: -72.5523\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 420.5515 - g_loss: 170.0949\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 1731.1349 - g_loss: 54.6342\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -1048.8010 - g_loss: -786.9734\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 412.4070 - g_loss: -583.0543\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 41.8424 - g_loss: -143.2518\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 427.3163 - g_loss: -604.3520\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -251.8664 - g_loss: 512.2653\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 294.4974 - g_loss: -1419.5394\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -815.6787 - g_loss: 80.0172\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -46.1707 - g_loss: -549.0043\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 571.8911 - g_loss: -1544.4453\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -19.6075 - g_loss: -212.7363\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -424.1495 - g_loss: -152.3205\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -118.7086 - g_loss: 258.1826\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 232.5364 - g_loss: 1429.0518\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -123.7014 - g_loss: 1302.9869\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 1363.7922 - g_loss: 1111.2701\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -847.2879 - g_loss: 1383.6723\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -1122.0212 - g_loss: 6.8967\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: -476.1622 - g_loss: -170.3862\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -282.8127 - g_loss: 683.7724\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -783.4843 - g_loss: -741.3783\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 108.6480 - g_loss: 182.5027\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -445.4629 - g_loss: 544.8157\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -546.6191 - g_loss: 806.5636\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -444.0162 - g_loss: 355.7296\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -241.6688 - g_loss: 1453.9912\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 157.3008 - g_loss: 697.7721\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -398.8972 - g_loss: 1079.5995\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -72.3964 - g_loss: 787.1173\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -1403.9607 - g_loss: 244.0074\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 85.1306 - g_loss: 907.9882\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -444.3012 - g_loss: 906.8053\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 612.5827 - g_loss: 434.5265\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -77.6572 - g_loss: 501.6195\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 764.0915 - g_loss: -773.9403\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 363.3401 - g_loss: 816.2986\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -283.1899 - g_loss: 422.2292\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -143.3230 - g_loss: 445.6346\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 177.3249 - g_loss: 260.5863\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: 411.2323 - g_loss: 404.5666\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -759.4817 - g_loss: -395.8464\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 294.6893 - g_loss: -912.2016\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -510.2576 - g_loss: -123.2867\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 267.6862 - g_loss: -809.6916\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -771.4120 - g_loss: 247.2765\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -935.6127 - g_loss: 193.1022\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 596ms/step - d_loss: -498.1847 - g_loss: -346.4515\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: -185.7657 - g_loss: 160.4171\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 1207.0321 - g_loss: 14.0944\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 612.3311 - g_loss: -1234.4952\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 482.2810 - g_loss: -1438.1450\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 695.0890 - g_loss: -609.2902\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -863.7555 - g_loss: -304.7951\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 702.3941 - g_loss: -1185.0669\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 3s 594ms/step - d_loss: -287.6313 - g_loss: -680.4318\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 431.3870 - g_loss: -1608.7309\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -662.2235 - g_loss: -1563.7672\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 242.1905 - g_loss: -1625.5570\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 332.6384 - g_loss: -1842.6300\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -420.5349 - g_loss: -1861.4181\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 108.4979 - g_loss: -1640.5495\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -868.1071 - g_loss: -271.3744\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -156.6612 - g_loss: -1679.8098\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 264.4856 - g_loss: -1918.6663\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -410.8000 - g_loss: -933.7050\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 10.4614 - g_loss: -1477.1308\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -683.0265 - g_loss: -1680.8215\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 544.1318 - g_loss: -611.8507\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 3s 603ms/step - d_loss: -31.0733 - g_loss: -1112.2663\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: -1007.7378 - g_loss: -2308.5485\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 3s 603ms/step - d_loss: 965.7555 - g_loss: -825.0400\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -282.9132 - g_loss: -1013.8248\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -85.8258 - g_loss: -1059.1143\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 124.3103 - g_loss: -969.1218\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: -1.4011 - g_loss: -1151.7897\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 3s 604ms/step - d_loss: -274.1757 - g_loss: -610.7245\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -404.1888 - g_loss: 332.6307\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: -576.8661 - g_loss: 73.2630\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 3s 603ms/step - d_loss: -146.9862 - g_loss: 676.8334\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 3s 602ms/step - d_loss: 347.7707 - g_loss: 539.3188\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 225.0891 - g_loss: 972.3671\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 3s 603ms/step - d_loss: -515.7458 - g_loss: 93.0786\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 347.4134 - g_loss: 1275.5125\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 209.5426 - g_loss: 1082.4058\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -375.6184 - g_loss: 1642.8014\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -4.5075 - g_loss: 565.2016\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 121.2381 - g_loss: -63.7203\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 318.5556 - g_loss: 521.4179\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 641.4971 - g_loss: 633.6367\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 471.3554 - g_loss: 752.5698\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -599.3378 - g_loss: 663.4117\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 364.4036 - g_loss: 691.6496\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -59.5845 - g_loss: 1173.0716\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -1032.8097 - g_loss: 229.4177\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 3s 595ms/step - d_loss: -902.8055 - g_loss: 383.1986\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 10.8666 - g_loss: -362.7155\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -241.9598 - g_loss: 950.7072\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 350.4394 - g_loss: 762.2890\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 635.5848 - g_loss: -70.1330\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -182.1294 - g_loss: -571.7478\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -915.8031 - g_loss: 661.8356\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 1072.3660 - g_loss: 133.5263\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -325.2754 - g_loss: 1030.4107\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 3s 601ms/step - d_loss: 1036.3497 - g_loss: 104.9085\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -108.0871 - g_loss: 1029.1475\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 1005.4343 - g_loss: 714.2566\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 138.7349 - g_loss: 648.5035\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: -20.8625 - g_loss: 635.4929\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 265.2748 - g_loss: 500.4826\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -86.4576 - g_loss: 361.5266\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: 59.8039 - g_loss: 1122.6605\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: -437.3581 - g_loss: 329.7984\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: -322.7881 - g_loss: 1862.6460\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 377.9151 - g_loss: 2270.0367\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 813.3553 - g_loss: 1791.2326\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 781.2719 - g_loss: 1937.0064\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 1145.2480 - g_loss: 1174.6171\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -179.5062 - g_loss: 2156.8879\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 3s 598ms/step - d_loss: 127.2174 - g_loss: 2483.4653\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 3s 599ms/step - d_loss: 67.7030 - g_loss: 2470.7062\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 3s 600ms/step - d_loss: 588.7789 - g_loss: 2312.8193\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: 498.5882 - g_loss: 2387.1546\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 3s 597ms/step - d_loss: -238.9635 - g_loss: 2470.6811\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 597ms/step - d_loss: 880.1950 - g_loss: 2219.3822\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 3s 596ms/step - d_loss: -247.8770 - g_loss: 2619.5597\n",
      "TRAINING DONE\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(prelim_dataset.class_indices))):\n",
    "    index = np.where(labels_argmax == i)\n",
    "    family = list(prelim_dataset.class_indices.keys())[i]\n",
    "    \n",
    "    os.makedirs(f\"C:/Users/Max/Documents/image_data/wasserstein_cgan_images/{family}\")\n",
    "    \n",
    "    print(\"Training model on {family}\".format(family = family))\n",
    "    \n",
    "    # Instantiate the customer `GANMonitor` Keras callback.\n",
    "    cbk = GANMonitor(num_img=dir_count(family), latent_dim=noise_dim, family_name = family)\n",
    "    \n",
    "    # Set the number of epochs for trainining.\n",
    "    t_epochs = 200\n",
    "\n",
    "\n",
    "    # Instantiate the WGAN model.\n",
    "    wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    "    )\n",
    "\n",
    "    # Compile the WGAN model.\n",
    "    wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    "    )\n",
    "\n",
    "    \n",
    "    wgan.fit(imgs[index], batch_size=BATCH_SIZE, \n",
    "             epochs=t_epochs, callbacks=[cbk])\n",
    "\n",
    "    print(\"TRAINING DONE\")\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1623d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
